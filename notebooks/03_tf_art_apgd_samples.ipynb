{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bae8e9f",
   "metadata": {},
   "source": [
    "# ART APGD sample generation (TF2.10)\n",
    "\n",
    "Generate APGD-CE adversarial samples using ART against:\n",
    "- $\\mathcal{U}^{(0)}$: baseline ensemble (4 ResNet + 4 VGG probability models)\n",
    "- $\\mathcal{U}^{(1)}$: first immunized generation ensemble\n",
    "\n",
    "Outputs are saved under `data/adversarial_samples/art/` as `.npz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, TypedDict\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import source.custom_specialization as custom_specialization\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "from art.attacks.evasion import AutoProjectedGradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9109ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(path: str):\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "PATHS = load_yaml(\"./configs/paths.yaml\")\n",
    "EXP   = load_yaml(\"./configs/exp.yaml\")\n",
    "\n",
    "data_root   = PATHS[\"data_root\"]\n",
    "tf_model_dir = PATHS[\"tf_model_dir\"]\n",
    "apgd_out    = PATHS[\"apgd_out\"]\n",
    "\n",
    "seed = int(EXP[\"seed\"])\n",
    "apgd_cfg = EXP[\"art_apgd\"]\n",
    "\n",
    "NORM = int(apgd_cfg[\"norm\"])              # 2\n",
    "EPS  = float(apgd_cfg[\"eps\"])             # 0.5 (override below)\n",
    "EPS_STEP = float(apgd_cfg[\"eps_step\"])    # 0.2\n",
    "MAX_ITER = int(apgd_cfg[\"max_iter\"])      # 2\n",
    "RINIT = int(apgd_cfg[\"nb_random_init\"])   # 4\n",
    "APGD_SETTINGS = [\n",
    "    (EPS, EPS_STEP, MAX_ITER, RINIT),\n",
    "]\n",
    "APGD_OUT = Path(apgd_out)\n",
    "APGD_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tf_model_dir = Path(tf_model_dir)\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "(x_all, y_all), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "y_all  = y_all.reshape(-1).astype(np.int64)\n",
    "y_test = y_test.reshape(-1).astype(np.int64)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_all, y_all,\n",
    "    test_size=0.2,\n",
    "    random_state=seed,\n",
    "    stratify=y_all\n",
    ")\n",
    "\n",
    "# normalize to [0,1]\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_val   = x_val.astype(np.float32) / 255.0\n",
    "x_test  = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_val   = y_val.astype(np.int64)\n",
    "\n",
    "def make_train_chunks(x: np.ndarray, y: np.ndarray, chunk_size: int = 10000):\n",
    "    out = []\n",
    "    n = x.shape[0]\n",
    "    for s in range(0, n, chunk_size):\n",
    "        e = min(s + chunk_size, n)\n",
    "        name = f\"train_all_{s:04d}_{e:04d}\"\n",
    "        out.append((name, x[s:e], y[s:e]))\n",
    "    return out\n",
    "\n",
    "splits: List[Tuple[str, np.ndarray, np.ndarray]] = []\n",
    "splits.extend(make_train_chunks(x_train, y_train, chunk_size=10000))\n",
    "splits.append((\"val\",  x_val,  y_val))\n",
    "splits.append((\"test\", x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5a1ef",
   "metadata": {},
   "source": [
    "## Load ensembles for $\\mathcal{U}^{(0)}$\n",
    "\n",
    "We load probability-output Keras models (`.keras`) and build an average-probability ensemble for ART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f23d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "U0_GLOB = [\n",
    "    str(tf_model_dir / \"resnet_*.keras\"),\n",
    "    str(tf_model_dir / \"vgg*_raw.keras\"),\n",
    "]\n",
    "U1_GLOB = [\n",
    "    \"./data/specialized_models/*.keras\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_model_paths(globs: List[str]) -> List[str]:\n",
    "    paths = []\n",
    "    for g in globs:\n",
    "        paths.extend(sorted(glob.glob(g)))\n",
    "    # de-dup while preserving order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for p in paths:\n",
    "        if p not in seen:\n",
    "            seen.add(p)\n",
    "            out.append(p)\n",
    "    return out\n",
    "\n",
    "def load_prob_models(paths: List[str]) -> List[tf.keras.Model]:\n",
    "    models = []\n",
    "    for p in paths:\n",
    "        m = load_model(p)\n",
    "        models.append(m)\n",
    "    return models\n",
    "\n",
    "\n",
    "class ProbAverageEnsemble(tf.keras.Model):\n",
    "    def __init__(self, prob_models: List[tf.keras.Model], eps: float = 1e-12):\n",
    "        super().__init__()\n",
    "        self.prob_models = prob_models\n",
    "        self.eps = eps\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        ps = [m(x, training=training) for m in self.prob_models]  # probs\n",
    "        p = tf.add_n(ps) / float(len(ps))\n",
    "        return tf.clip_by_value(p, self.eps, 1.0)\n",
    "def build_art_classifier(prob_models: List[tf.keras.Model]) -> TensorFlowV2Classifier:\n",
    "    ens = ProbAverageEnsemble(prob_models)\n",
    "    clf = TensorFlowV2Classifier(\n",
    "        model=ens,\n",
    "        nb_classes=10,\n",
    "        input_shape=(32, 32, 3),\n",
    "        clip_values=(0.0, 1.0),\n",
    "        loss_object=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    )\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb3a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_npz(path: Path, **kwargs):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    np.savez_compressed(path, **kwargs)\n",
    "    print(\"Saved:\", path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a379c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_apgdce_and_save(\n",
    "    clf: TensorFlowV2Classifier,\n",
    "    X: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    split : str,\n",
    "    out_path: Path,\n",
    "    eps: float,\n",
    "    eps_step: float,\n",
    "    max_iter: int,\n",
    "    nb_random_init: int,\n",
    "    batch_size: int = 64,\n",
    "    tag: str = \"\"\n",
    "    \n",
    "):\n",
    "    attack = AutoProjectedGradientDescent(\n",
    "        estimator=clf,\n",
    "        norm=2,\n",
    "        eps=float(eps),\n",
    "        eps_step=float(eps_step),\n",
    "        max_iter=int(max_iter),\n",
    "        nb_random_init=int(nb_random_init),\n",
    "        targeted=False,\n",
    "        batch_size=int(batch_size),\n",
    "        loss_type=\"cross_entropy\",\n",
    "        verbose=False,\n",
    "    )\n",
    "    x_adv = attack.generate(x=X, y=None).astype(np.float32)\n",
    "\n",
    "    save_npz(\n",
    "        out_path,\n",
    "        x_adv=x_adv,\n",
    "        y=Y.astype(np.int64),\n",
    "        split = split,\n",
    "        norm=\"L2\",\n",
    "        eps=np.float32(eps),\n",
    "        eps_step=np.float32(eps_step),\n",
    "        max_iter=np.int32(max_iter),\n",
    "        nb_random_init=np.int32(nb_random_init),\n",
    "        tag = tag\n",
    "    )\n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625155e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_for_ensemble(tag: str, model_paths: List[str]):\n",
    "    if len(model_paths) == 0:\n",
    "        print(f\"[{tag}] no models found -> skipping\")\n",
    "        return\n",
    "\n",
    "    prob_models = load_prob_models(model_paths)\n",
    "    clf = build_art_classifier(prob_models)\n",
    "\n",
    "    for eps, eps_step, max_iter, rinit in APGD_SETTINGS:\n",
    "        eps_tag = str(eps).replace(\".\", \"p\")\n",
    "        step_tag = str(eps_step).replace(\".\", \"p\")\n",
    "\n",
    "        for split_name, Xs, Ys in splits:\n",
    "            out_name = f\"{split_name}_apgdce_l2_eps{eps_tag}_step{step_tag}_it{max_iter}_rinit{rinit}_against_{tag}.npz\"\n",
    "            out_path = APGD_OUT / out_name\n",
    "\n",
    "            if out_path.exists():\n",
    "                continue  # avoid re-generation\n",
    "\n",
    "            _ = run_apgdce_and_save(\n",
    "                clf,\n",
    "                Xs.astype(np.float32),\n",
    "                Ys.astype(np.int64),\n",
    "                split = split_name,\n",
    "                out_path=out_path,\n",
    "                eps=eps,\n",
    "                eps_step=eps_step,\n",
    "                max_iter=max_iter,\n",
    "                nb_random_init=rinit,\n",
    "                batch_size=64,\n",
    "                tag = tag\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01df4719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_train_chunks(tag: str, eps: float, eps_step: float, max_iter: int, rinit: int):\n",
    "    eps_tag = str(eps).replace(\".\", \"p\")\n",
    "    step_tag = str(eps_step).replace(\".\", \"p\")\n",
    "\n",
    "    pattern = str(APGD_OUT / f\"train_all_*_apgdce_l2_eps{eps_tag}_step{step_tag}_it{max_iter}_rinit{rinit}_against_{tag}.npz\")\n",
    "    paths = sorted(glob.glob(pattern))\n",
    "    if len(paths) == 0:\n",
    "        print(\"No chunk files:\", pattern)\n",
    "        return\n",
    "\n",
    "    x_list, y_list = [], []\n",
    "    for p in paths:\n",
    "        d = np.load(p)\n",
    "        x_list.append(d[\"x_adv\"])\n",
    "        y_list.append(d[\"y\"])\n",
    "\n",
    "    x_adv = np.concatenate(x_list, axis=0).astype(np.float32)\n",
    "    y = np.concatenate(y_list, axis=0).astype(np.int64)\n",
    "\n",
    "    out_name = f\"train_all_apgdce_l2_eps{eps_tag}_step{step_tag}_it{max_iter}_rinit{rinit}_against_{tag}.npz\"\n",
    "    out_path = APGD_OUT / out_name\n",
    "    save_npz(\n",
    "        out_path,\n",
    "        x_adv=x_adv,\n",
    "        y=y,\n",
    "        split = 'train',\n",
    "        norm=\"L2\",\n",
    "        eps=np.float32(eps),\n",
    "        eps_step=np.float32(eps_step),\n",
    "        max_iter=np.int32(max_iter),\n",
    "        nb_random_init=np.int32(rinit),\n",
    "        tag = tag\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eaceef",
   "metadata": {},
   "source": [
    "## Generate APGD-CE samples for $\\mathcal{U}^{(0)}$\n",
    "\n",
    "We generate and save `.npz` for each split chunk to avoid memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234da600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current config is the \"weak\" perturbation.\n",
    "u0_paths = collect_model_paths(U0_GLOB)\n",
    "\n",
    "gen_for_ensemble(\"U0\", u0_paths)\n",
    "merge_train_chunks(\"U0\", EPS, EPS_STEP, MAX_ITER, RINIT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d776e5a",
   "metadata": {},
   "source": [
    "# Get $\\mathcal{U}^{(1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74419530",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_DIR = Path(\"./data/specialized_models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData = Tuple[np.ndarray, np.ndarray]\n",
    "ValData = Tuple[np.ndarray, np.ndarray]\n",
    "\n",
    "class HistoryDict(TypedDict, total=False):\n",
    "    history: Dict[str, List[float]]\n",
    "    params: Dict[str, Any]\n",
    "    epoch: List[int]\n",
    "\n",
    "\n",
    "class SpecializeResult(TypedDict):\n",
    "    model_path: str   \n",
    "    history: HistoryDict  \n",
    "\n",
    "        \n",
    "def specialize(\n",
    "    new_train : TrainData,\n",
    "    new_val : ValData,\n",
    "    original_model : tf.keras.Model,\n",
    "    new_model_path : str = 'specialized_model.keras',\n",
    "    path : Path = Path('./data/specialized_models/'),\n",
    "    verbose = 1,\n",
    "    name : str = ''\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Returns (baseline_adv_model, tuned_baseline_adv_model, tuned_history_dict_or_None)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    x_tr, y_tr = new_train\n",
    "    x_v, y_v = new_val\n",
    "    if y_tr.ndim == 1 or y_tr.shape[1] != 10:\n",
    "        y_tr = to_categorical(y_tr, 10)\n",
    "    if y_v.ndim == 1 or y_v.shape[1] != 10:\n",
    "        y_v = to_categorical(y_v, 10)\n",
    "    if not new_model_path.lower().endswith('.keras'):\n",
    "        new_model_path += '.keras'\n",
    "        \n",
    "    model_path  = path /  Path(new_model_path)    \n",
    "\n",
    "    if model_path.exists():\n",
    "        specialized_model = load_model(model_path)\n",
    "        print(f'{model_path} already exists.')\n",
    "        hist = custom_specialization.load_history(model_path) \n",
    "        \n",
    "        if hist is None:\n",
    "            print(f\"[WARN] No history found for {model_path}. History is empty dictionary.\")\n",
    "            hist = {}\n",
    "    else:\n",
    "        print(f'{model_path} training...')\n",
    "        specialized_model,hist = custom_specialization.turn_specialist(original_model, path = model_path,\n",
    "                                                x_tr=x_tr, y_tr=y_tr,\n",
    "                                                  x_v=x_v,   y_v=y_v,\n",
    "                                                  epochs=21, learning_rate=1e-3, batch_size=128, verbose=verbose, name=f\"tuned_once{name}\")\n",
    "        hist = {\"history\": hist.history, \"params\": hist.params, \"epoch\": hist.epoch}\n",
    "        specialized_model.save(model_path)\n",
    "    return {\n",
    "    \"model_path\": str(model_path),\n",
    "    \"history\": hist,\n",
    "}, specialized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {}\n",
    "y_true_dict = {}\n",
    "for p in os.listdir(APGD_OUT):\n",
    "    d = np.load(APGD_OUT/Path(p))\n",
    "    adv_name = 'APGD_weak' if d['max_iter'] == 2 else 'APGD_strong'\n",
    "    split = str(d['split'])\n",
    "    sample_name = adv_name + '_' + split\n",
    "    samples[sample_name] = d['x_adv']\n",
    "    y_true_dict[sample_name] = d['y']\n",
    "\n",
    "x_temp = np.concatenate([samples['APGD_weak_train'],samples['APGD_weak_val']],axis=0)\n",
    "y_temp = np.concatenate([y_true_dict['APGD_weak_train'],y_true_dict['APGD_weak_val']],axis=0)\n",
    "\n",
    "new_stratified_x_train, new_stratified_x_val, new_stratified_y_train, new_stratified_y_val = train_test_split(\n",
    "        x_temp,\n",
    "        y_temp,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "\n",
    "specialized_model_dict={}\n",
    "\n",
    "model_dict = {}\n",
    "for f_name in sorted(os.listdir(\"./data/models/\")):\n",
    "    if 'original' not in f_name and f_name.endswith('keras'):\n",
    "        print(f_name)\n",
    "        m = load_model(f\"./data/models/{f_name}\")\n",
    "        model_dict[f_name] = m\n",
    "adv_sample_name = 'APGD_weak'\n",
    "for model_name, m in model_dict.items():\n",
    "    model_name = model_name[:-6]\n",
    "    model_name = model_name.split('_')[0] if model_name[0] == 'v' else model_name.split('_')[0] + model_name.split('_')[-1] + 'v'+model_name.split('_')[1][-1]\n",
    "    info = specialize(\n",
    "        (new_stratified_x_train, to_categorical(new_stratified_y_train,10)),\n",
    "        (new_stratified_x_val, to_categorical(new_stratified_y_val,10)),\n",
    "        m,\n",
    "        new_model_path=f\"{model_name}_{adv_sample_name}-SP\")\n",
    "model_dict = {}\n",
    "for f_name in sorted(os.listdir(\"./data/models/\")):\n",
    "    if 'original' not in f_name and f_name.endswith('keras'):\n",
    "        print(f_name)\n",
    "        m = load_model(f\"./data/models/{f_name}\")\n",
    "        model_dict[f_name] = m\n",
    "        \n",
    "SPs_GLOB = [\n",
    "    \"./data/specialized_models/*.keras\"\n",
    "]\n",
    "U1_GLOB = U0_GLOB + SPs_GLOB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6f3239",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPs_paths = collect_model_paths(SPs_GLOB)\n",
    "\n",
    "gen_for_ensemble(\"SPs\", SPs_paths)\n",
    "merge_train_chunks(\"SPs\", EPS, EPS_STEP, MAX_ITER, RINIT)\n",
    "\n",
    "u1_paths = collect_model_paths(U1_GLOB)\n",
    "\n",
    "gen_for_ensemble(\"U1\", u1_paths)\n",
    "merge_train_chunks(\"U1\", EPS, EPS_STEP, MAX_ITER, RINIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80506977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strong configuration.\n",
    "EPS, EPS_STEP, MAX_ITER, RINIT = (0.7, 0.2, 10, 4)\n",
    "\n",
    "\n",
    "gen_for_ensemble(\"U0\", u0_paths)\n",
    "merge_train_chunks(\"U0\", EPS, EPS_STEP, MAX_ITER, RINIT)\n",
    "\n",
    "\n",
    "gen_for_ensemble(\"SPs\", SPs_paths)\n",
    "merge_train_chunks(\"SPs\", EPS, EPS_STEP, MAX_ITER, RINIT)\n",
    "\n",
    "\n",
    "gen_for_ensemble(\"U1\", u1_paths)\n",
    "merge_train_chunks(\"U1\", EPS, EPS_STEP, MAX_ITER, RINIT)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
