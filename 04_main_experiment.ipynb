{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea35ca26",
   "metadata": {},
   "source": [
    "# Main experiment (TF2.10)\n",
    "\n",
    "This notebook produces the main experimental pipeline:\n",
    "- Load CIFAR-10 and adversarial samples produced in `01` (Torch AutoAttack) and `03` (TF2 + ART APGD).\n",
    "- Cache per-model predictions and compute ensemble entropy.\n",
    "- Sweep entropy thresholds (core vs out-of-core) and report accuracy/coverage.\n",
    "- Specialize **second-generation $\\mathcal{U}^{(2)^\\prime}$** on the out-of-core region and evaluate the full IMM pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ae187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os, sys, glob, yaml, csv, time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Any, TypedDict\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('./source')\n",
    "\n",
    "import source.cache_store as cache_store\n",
    "import source.custom_specialization as custom_specialization \n",
    "import source.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac54425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(path: str):\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "PATHS = load_yaml(\"./configs/paths.yaml\")\n",
    "EXP   = load_yaml(\"./configs/exp.yaml\")\n",
    "\n",
    "data_root    = Path(PATHS[\"data_root\"])\n",
    "results_root = Path(PATHS[\"results_root\"])\n",
    "tf_model_dir = Path(PATHS[\"tf_model_dir\"])\n",
    "SPs_model_dir = Path(PATHS[\"SPs_model_dir\"])\n",
    "autoattack_out = Path(PATHS[\"autoattack_out\"])\n",
    "apgd_out     = Path(PATHS[\"apgd_out\"])\n",
    "cache_root   = Path(PATHS[\"cache_root\"])\n",
    "\n",
    "seed = int(EXP[\"seed\"])\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Ensure dirs\n",
    "results_root.mkdir(parents=True, exist_ok=True)\n",
    "cache_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"tf_model_dir:\", tf_model_dir)\n",
    "print(\"autoattack_out:\", autoattack_out)\n",
    "print(\"apgd_out:\", apgd_out)\n",
    "print(\"cache_root:\", cache_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5041757f",
   "metadata": {},
   "source": [
    "## Load CIFAR-10 (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f92f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_all, y_all), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "y_all  = y_all.reshape(-1).astype(np.int64)\n",
    "y_test = y_test.reshape(-1).astype(np.int64)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=seed, stratify=y_all\n",
    ")\n",
    "\n",
    "x_train = x_train.astype(np.float32)/255.0\n",
    "x_val   = x_val.astype(np.float32)/255.0\n",
    "x_test  = x_test.astype(np.float32)/255.0\n",
    "\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_val   = y_val.astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb51b6b",
   "metadata": {},
   "source": [
    "## Load adversarial samples from 01 (Torch AutoAttack) and 03 (ART APGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a11ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples: Dict[str, np.ndarray] = {}\n",
    "y_true: Dict[str, np.ndarray] = {}\n",
    "\n",
    "# Original\n",
    "samples[\"original_train\"] = x_train.copy()\n",
    "samples[\"original_val\"]   = x_val.copy()\n",
    "samples[\"original_test\"]  = x_test.copy()\n",
    "y_true[\"original_train\"] = y_train.copy()\n",
    "y_true[\"original_val\"]   = y_val.copy()\n",
    "y_true[\"original_test\"]  = y_test.copy()\n",
    "\n",
    "# Torch AutoAttack outputs (saved as .npy in NCHW; convert to NHWC)\n",
    "aa_path = autoattack_out / \"x_adv_test_std_l2_eps5.npy\"  # adjust if needed\n",
    "x_adv = np.load(aa_path).astype(np.float32)\n",
    "\n",
    "if x_adv.ndim == 4:\n",
    "    if x_adv.shape[-1] == 3 and x_adv.shape[1] != 3:\n",
    "        # already NHWC: (N, H, W, C)\n",
    "        pass\n",
    "    elif x_adv.shape[1] == 3 and x_adv.shape[-1] != 3:\n",
    "        # NCHW: (N, C, H, W) -> NHWC\n",
    "        x_adv = np.transpose(x_adv, (0, 2, 3, 1))\n",
    "samples[\"AA_std_test\"] = x_adv\n",
    "y_true[\"AA_std_test\"] = y_test.copy()\n",
    "\n",
    "# ART APGD outputs -> key = \"{tag}_{split}_{weak/strong}\"\n",
    "for p in sorted(apgd_out.glob(\"*.npz\")):\n",
    "    d = np.load(p)\n",
    "\n",
    "    split = str(d[\"split\"]) if \"split\" in d else \"unknown\"\n",
    "    tag   = str(d[\"tag\"])   if \"tag\"   in d else \"unknown\"\n",
    "    it    = int(d[\"max_iter\"]) if \"max_iter\" in d else None\n",
    "\n",
    "    strength = \"weak\" if it == 2 else (\"strong\" if it == 10 else f\"it{it}\")\n",
    "    sample_name = f\"{tag}_{strength}_{split}\"\n",
    "\n",
    "    samples[sample_name] = d[\"x_adv\"].astype(np.float32)\n",
    "    y_true[sample_name]  = d[\"y\"].astype(np.int64).reshape(-1)\n",
    "    \n",
    "samples[\"U0_union_train\"] = np.concatenate(\n",
    "    [samples[\"original_train\"], samples[\"U0_weak_train\"], samples[\"U0_strong_train\"]],\n",
    "    axis=0\n",
    ").astype(np.float32)\n",
    "y_true[\"U0_union_train\"] = np.concatenate(\n",
    "    [y_true[\"original_train\"], y_true[\"U0_weak_train\"], y_true[\"U0_strong_train\"]],\n",
    "    axis=0\n",
    ").astype(np.int64)\n",
    "\n",
    "samples[\"U0_union_val\"] = np.concatenate(\n",
    "    [samples[\"original_val\"], samples[\"U0_weak_val\"], samples[\"U0_strong_val\"]],\n",
    "    axis=0\n",
    ").astype(np.float32)\n",
    "y_true[\"U0_union_val\"] = np.concatenate(\n",
    "    [y_true[\"original_val\"], y_true[\"U0_weak_val\"], y_true[\"U0_strong_val\"]],\n",
    "    axis=0\n",
    ").astype(np.int64)\n",
    "\n",
    "samples[\"U0_union_test\"] = np.concatenate(\n",
    "    [samples[\"original_test\"], samples[\"U0_weak_test\"], samples[\"U0_strong_test\"]],\n",
    "    axis=0\n",
    ").astype(np.float32)\n",
    "y_true[\"U0_union_test\"] = np.concatenate(\n",
    "    [y_true[\"original_test\"], y_true[\"U0_weak_test\"], y_true[\"U0_strong_test\"]],\n",
    "    axis=0\n",
    ").astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d4a0d2",
   "metadata": {},
   "source": [
    "## Load models and build first-layer committee (U0 âˆª U1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e828549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models_from_dir(model_dir: Path, pattern: str=\"*.keras\", exclude_substr: str=\"original\") -> Dict[str, tf.keras.Model]:\n",
    "    md = {}\n",
    "    for p in sorted(model_dir.glob(pattern)):\n",
    "        if exclude_substr and exclude_substr in p.name:\n",
    "            continue\n",
    "        md[p.name] = load_model(str(p))\n",
    "    return md\n",
    "\n",
    "# U0 baseline models\n",
    "u0 = load_models_from_dir(tf_model_dir, pattern=\"*.keras\")\n",
    "sps = load_models_from_dir(SPs_model_dir, pattern=\"*.keras\")\n",
    "u1 = {}\n",
    "u1.update(u0)\n",
    "u1.update(sps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abcfe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.clear_folder(str(cache_root), dry_run=False)\n",
    "caching = cache_store.ResultStore(root=str(cache_root))\n",
    "\n",
    "# committees\n",
    "u0_keys = sorted(u0.keys())\n",
    "u1_keys = sorted(u1.keys())\n",
    "sps_keys = sorted(sps.keys())\n",
    "\n",
    "# predict-cache: U1 (== First_layer) only (contains U0 + SPs)\n",
    "participating_models = u1\n",
    "\n",
    "for sample_name, X in samples.items():\n",
    "    Y = y_true[sample_name]\n",
    "    print(f\"{len(participating_models)} models caching preds on {sample_name}...\", end=\"---\")\n",
    "\n",
    "    for model_name, model in participating_models.items():\n",
    "        pred = model.predict(X, verbose=0)\n",
    "        caching.set_pred(model_name, sample_name, pred)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # entropy for U0\n",
    "    print(f\"caching entropy (U0) on {sample_name}...\", end=\"---\")\n",
    "    arr_u0 = np.array([caching.get_pred(mn, sample_name) for mn in u0_keys])  # (M,N,C)\n",
    "    ent_u0 = np.array([utils.cross_entropy(arr_u0[:, i, :]) for i in range(arr_u0.shape[1])])\n",
    "    caching.set_entropy(u0_keys, sample_name, ent_u0)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # entropy for U1 (= U0 + SPs)\n",
    "    print(f\"caching entropy (U1) on {sample_name}...\", end=\"---\")\n",
    "    arr_u1 = np.array([caching.get_pred(mn, sample_name) for mn in u1_keys])\n",
    "    ent_u1 = np.array([utils.cross_entropy(arr_u1[:, i, :]) for i in range(arr_u1.shape[1])])\n",
    "    caching.set_entropy(u1_keys, sample_name, ent_u1)\n",
    "    print(\"Done.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7081a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbAverageEnsembleFromProbModels(tf.keras.Model):\n",
    "    def __init__(self, prob_models : dict, eps=1e-12, cache_root : str = './data/cache/'):\n",
    "        super().__init__()\n",
    "        self.prob_models = prob_models\n",
    "        self.eps = eps\n",
    "        self.caching = cache_store.ResultStore(root = cache_root)\n",
    "        \n",
    "    def probs(self, x, sample_name : str = ''):\n",
    "        '''\n",
    "        return average probability (average softmax value)\n",
    "        '''\n",
    "        if sample_name:\n",
    "            self.caching_pred(x, sample_name)\n",
    "            preds = [self.caching.get_pred(model_name, sample_name) for model_name in sorted(self.prob_models)]\n",
    "        else:\n",
    "            preds = [self.prob_models[k].predict(x,verbose = 0, batch_size = 64) for k in sorted(self.prob_models)]\n",
    "        \n",
    "        return np.mean(preds, axis=0)\n",
    "    def core_preds(self, x, sample_name : str = '', ent_th : float = 0.1):\n",
    "        '''\n",
    "        return average probability (average softmax value)\n",
    "        '''\n",
    "        if sample_name:\n",
    "            self.caching_pred(x, sample_name)\n",
    "            preds = [self.caching.get_pred(model_name, sample_name) for model_name in sorted(self.prob_models)]\n",
    "        else:\n",
    "            preds = [self.prob_models[k].predict(x,verbose = 0, batch_size = 64) for k in sorted(self.prob_models)]\n",
    "        preds = np.array(preds)\n",
    "        \n",
    "        if sample_name:\n",
    "            self.caching_ent(sample_name)\n",
    "            ent = self.caching.get_entropy(sorted(self.prob_models), sample_name)\n",
    "        else:\n",
    "            ent = []\n",
    "            for i in range(preds.shape[1]):\n",
    "                ent.append(self.cross_entropy(preds[:,i,: ]))\n",
    "            ent = np.array(ent)\n",
    "        \n",
    "        core = ent<ent_th\n",
    "        \n",
    "        preds = preds[:, core, :]\n",
    "        core_preds = np.mean(preds, axis=0)\n",
    "        \n",
    "        return core_preds, core, ent\n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "    def caching_pred(self, x, sample_name):\n",
    "        for model_name, model in self.prob_models.items():\n",
    "            key = (self.caching.int_from_model_path(model_name), sample_name)\n",
    "            fp = self.caching._preds_idx.get(key)\n",
    "\n",
    "            if fp and Path(fp).exists():\n",
    "                # Already cached and file exists: nothing to do.\n",
    "#                 print(f'{model_name} already made predictions on {sample_name} and it was cached.')\n",
    "                \n",
    "                continue\n",
    "\n",
    "            elif fp and not Path(fp).exists():\n",
    "                # Index says there is a file, but it doesn't exist anymore.\n",
    "                # Recompute and overwrite.\n",
    "#                 print(f\"Index says there is a file, but it doesn't exist anymore. {model_name}  on {sample_name}. Recompute and Overwrite.\")\n",
    "\n",
    "                pred = model.predict(x, verbose=0)\n",
    "                self.caching.set_pred(model_name, sample_name, pred)\n",
    "\n",
    "            else:\n",
    "                # No index entry; check if file exists under the naming convention.\n",
    "#                 print(f'{model_name} make predictions on {sample_name} and it is cached.')\n",
    "                outdir = self.caching.root / \"preds\" / sample_name\n",
    "                fp = outdir / f\"{self.caching._stem(model_name)}.npy\"\n",
    "                if Path(fp).exists():\n",
    "#                     print('File exists but is not in the index: add it.')\n",
    "                    # File exists but is not in the index: add it.\n",
    "                    pred = np.load(fp)\n",
    "                    key = (self.caching.int_from_model_path(model_name), sample_name)\n",
    "                    self.caching._preds_idx[key] = str(fp)\n",
    "                    self.caching._dump_idx(self.caching._preds_idx_path, self.caching._preds_idx)\n",
    "                else:\n",
    "#                     print('processing..', end = ' ')\n",
    "                    # Compute and cache.\n",
    "                    pred = model.predict(x, verbose=0)\n",
    "                    self.caching.set_pred(model_name, sample_name, pred)\n",
    "#                     print('Done.')\n",
    "                    \n",
    "    def caching_ent(self, sample_name):\n",
    "        # Get predictions for each model.\n",
    "        sig = self.caching._committee_sig(list(self.prob_models.keys()))\n",
    "        key = (\"entropy\", sig, sample_name)\n",
    "        fp = self.caching._metrics_idx.get(key)\n",
    "\n",
    "        # Caching entropy\n",
    "        if fp and Path(fp).exists():\n",
    "#             print('sample', sample_name, 'entropy cached already')\n",
    "            # Already cached and file exists: fine.\n",
    "            pass\n",
    "\n",
    "        elif fp and not Path(fp).exists():\n",
    "            # Index points to missing file: recompute and overwrite.\n",
    "            arrays = np.array([self.caching.get_pred(model_name, sample_name) for model_name in sorted(self.prob_models.keys())])\n",
    "\n",
    "            ent = []\n",
    "            for i in range(arrays.shape[1]):\n",
    "                ent.append(self.cross_entropy(arrays[:,i,: ]))\n",
    "            ent = np.array(ent)\n",
    "            self.caching.set_entropy(sorted(self.prob_models.keys()), sample_name, ent)\n",
    "\n",
    "        else:\n",
    "            # No index entry; check if entropy file exists under naming convention.\n",
    "            outdir = self.caching.root / \"metrics\" / sample_name\n",
    "            fp = outdir / f\"entropy__{sig}.npy\"\n",
    "            if Path(fp).exists():\n",
    "                key = (\"entropy\", sig, sample_name)\n",
    "                self.caching._metrics_idx[key] = str(fp)\n",
    "                self.caching._dump_idx(self.caching._metrics_idx_path, self.caching._metrics_idx)\n",
    "            else:\n",
    "                # Compute and cache.\n",
    "                arrays = np.array([self.caching.get_pred(model_name, sample_name) for model_name in sorted(self.prob_models.keys())])\n",
    "\n",
    "                ent = []\n",
    "                for i in range(arrays.shape[1]):\n",
    "                    ent.append(self.cross_entropy(arrays[:,i,: ]))\n",
    "                ent = np.array(ent)\n",
    "                self.caching.set_entropy(sorted(self.prob_models.keys()), sample_name, ent)\n",
    "    \n",
    "    def cross_entropy(self, arrays):\n",
    "        n = len(arrays)\n",
    "        if n == 0:\n",
    "            raise ValueError(\"At least one array is required.\")\n",
    "\n",
    "        # Stack to shape (m, n)\n",
    "        cols = [np.asarray(a).reshape(-1) for a in arrays]\n",
    "        m = cols[0].shape[0]\n",
    "        if any(c.shape[0] != m for c in cols):\n",
    "            raise ValueError(\"All arrays must have the same first dimension m.\")\n",
    "        A = np.column_stack(cols)  # shape (m, n)\n",
    "\n",
    "        log_base = m  \n",
    "        L = np.log(A + 1e-12) / np.log(log_base)\n",
    "\n",
    "        M = A.T @ L  # shape (n, n), M[k, l] = sum_i a_{i,k} * log(a_{i,l})\n",
    "        total = -np.sum(M) # sum over all pair\n",
    "        \n",
    "        return float(total)/(n**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a00be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_ACC = 0.95\n",
    "STEP = 0.01\n",
    "MAX_TH = 2.0\n",
    "th_grid = np.arange(STEP, MAX_TH + 1e-9, STEP)\n",
    "\n",
    "def find_th_for_core_acc(ens, sample_name, X, y, th_grid):\n",
    "    for th in th_grid:\n",
    "        core_preds, core, _ = ens.core_preds(X, sample_name=sample_name, ent_th=float(th))\n",
    "        cnt = int(core.sum())\n",
    "        if cnt < 10:\n",
    "            continue\n",
    "        core_acc = float(np.mean(np.argmax(core_preds, axis=-1) == y[core]))\n",
    "        if core_acc >= TARGET_ACC:\n",
    "            core_cov = float(core.mean())\n",
    "            return float(th), core_acc, core_cov, cnt\n",
    "    return None\n",
    "\n",
    "ens_u1 = ProbAverageEnsembleFromProbModels(u1, cache_root=str(cache_root))\n",
    "\n",
    "ent_th_dict = {}\n",
    "\n",
    "for sample_name, X in samples.items():\n",
    "    if not sample_name.endswith(\"_val\"):\n",
    "        continue\n",
    "\n",
    "    y = y_true[sample_name]\n",
    "    head = \"_\".join(sample_name.split(\"_\")[:-1])  # drop \"_val\"\n",
    "\n",
    "    out = find_th_for_core_acc(ens_u1, sample_name, X, y, th_grid)\n",
    "    if out is None:\n",
    "        ent_th_dict[head] = None\n",
    "        print(head, None, None, None)\n",
    "    else:\n",
    "        th, core_acc, core_cov, core_cnt = out\n",
    "        ent_th_dict[head] = th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd80749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_entropy_fuzzy(\n",
    "    participating_models: dict[str, tf.keras.Model],\n",
    "    caching: cache_store.CacheStore,\n",
    "    samples: dict[str, np.ndarray],\n",
    "    y_true_dict: dict[str, np.ndarray],\n",
    "    ent_th: float,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    sample_names = list(samples.keys())\n",
    "    results = {}\n",
    "\n",
    "    keys_sorted = sorted(participating_models.keys())\n",
    "\n",
    "    for sample_name in sample_names:\n",
    "        y_true = y_true_dict[sample_name].reshape(-1)\n",
    "\n",
    "        ent = caching.get_entropy(keys_sorted, sample_name)\n",
    "        low_idx = ent <= ent_th\n",
    "        high_idx = ~low_idx\n",
    "\n",
    "        low_cnt = int(np.sum(low_idx))\n",
    "        high_cnt = int(np.sum(high_idx))\n",
    "\n",
    "        # Per-model stats\n",
    "        per_model_rows = []\n",
    "        for model_name in keys_sorted:\n",
    "            pred = caching.get_pred(model_name, sample_name)\n",
    "            pred_label = np.argmax(pred, axis=1)\n",
    "\n",
    "            global_acc = float(np.mean(pred_label == y_true))\n",
    "            low_acc = float(np.mean(pred_label[low_idx] == y_true[low_idx])) if low_cnt > 0 else 0.0\n",
    "            high_acc = float(np.mean(pred_label[high_idx] == y_true[high_idx])) if high_cnt > 0 else 0.0\n",
    "\n",
    "            per_model_rows.append({\n",
    "                \"model\": str(Path(model_name).stem),\n",
    "                \"global_acc\": global_acc,\n",
    "                \"low_ent_acc\": low_acc,\n",
    "                \"high_ent_acc\": high_acc,\n",
    "            })\n",
    "\n",
    "        per_model_df = pd.DataFrame(per_model_rows)\n",
    "\n",
    "        # Ensemble stats\n",
    "        preds_all = np.array([caching.get_pred(mn, sample_name) for mn in keys_sorted])  # (M,N,C)\n",
    "        ens_raw = np.mean(preds_all, axis=0)  # (N,C)\n",
    "        overall_acc = float(np.mean(np.argmax(ens_raw, axis=1) == y_true))\n",
    "\n",
    "        # Low entropy: zero out high region before averaging\n",
    "        preds_low = preds_all.copy()\n",
    "        preds_low[:, ~low_idx, :] = 0\n",
    "        ens_low = np.mean(preds_low, axis=0)\n",
    "\n",
    "        # High entropy: zero out low region before averaging\n",
    "        preds_high = preds_all.copy()\n",
    "        preds_high[:, low_idx, :] = 0\n",
    "        ens_high = np.mean(preds_high, axis=0)\n",
    "\n",
    "        low_acc_ens = float(np.mean(np.argmax(ens_low[low_idx], axis=1) == y_true[low_idx])) if low_cnt > 0 else 0.0\n",
    "        high_acc_ens = float(np.mean(np.argmax(ens_high[high_idx], axis=1) == y_true[high_idx])) if high_cnt > 0 else 0.0\n",
    "\n",
    "        ensemble_summary = {\n",
    "            \"overall_acc\": overall_acc,\n",
    "            \"low_entropy_count\": low_cnt,\n",
    "            \"low_entropy_acc\": low_acc_ens,\n",
    "            \"high_entropy_count\": high_cnt,\n",
    "            \"high_entropy_acc\": high_acc_ens,\n",
    "        }\n",
    "\n",
    "        results[sample_name] = {\n",
    "            \"entropy_threshold\": float(ent_th),\n",
    "            \"per_model_df\": per_model_df,\n",
    "            \"ensemble\": ensemble_summary,\n",
    "        }\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(f\"Sample: {sample_name}\")\n",
    "            print(\"-\" * 70)\n",
    "            print(f\"Entropy threshold: {ent_th:.6f}\")\n",
    "\n",
    "            print(\"\\nPer-model performance:\")\n",
    "            print(per_model_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "            print(\"\\nEnsemble performance:\")\n",
    "            print(\n",
    "                f\"  Overall accuracy:     {overall_acc:.4f}\\n\"\n",
    "                f\"  Low-entropy count:    {low_cnt}\\n\"\n",
    "                f\"  Low-entropy accuracy: {low_acc_ens:.4f}\\n\"\n",
    "                f\"  High-entropy count:   {high_cnt}\\n\"\n",
    "                f\"  High-entropy accuracy:{high_acc_ens:.4f}\\n\"\n",
    "            )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_ent_th_sweep(\n",
    "    df,\n",
    "    overall_acc,\n",
    "    sample_name=\"AA_RBST_test\",\n",
    "    outdir=None,\n",
    "    vline_ent_ths: list[float] = [0.0414, 1.0],\n",
    "    show_legend: bool = False,\n",
    "    legend_outside: bool = True,\n",
    "):\n",
    "    assert len(vline_ent_ths) == 2, \"vline_ent_ths must have two thresholds (low and high).\"\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"ent_th\"] = df[\"ent_th\"].astype(float)\n",
    "    df = df.sort_values(\"ent_th\").reset_index(drop=True)\n",
    "\n",
    "    x = df[\"ent_th\"].to_numpy()\n",
    "    core_cnt = df[\"core count\"].to_numpy(dtype=float)\n",
    "    high_cnt = df[\"high count\"].to_numpy(dtype=float)\n",
    "    acc1 = df[\"acc1\"].to_numpy(dtype=float)\n",
    "    acc2 = df[\"acc2\"].to_numpy(dtype=float)\n",
    "\n",
    "    total_n = core_cnt + high_cnt\n",
    "    N = float(total_n[0])\n",
    "\n",
    "    def _save(fig, name):\n",
    "        if outdir is None:\n",
    "            return\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        fig.savefig(os.path.join(outdir, name), dpi=300)\n",
    "\n",
    "    def nearest_idx(arr, val):\n",
    "        arr = np.asarray(arr, dtype=float)\n",
    "        return int(np.argmin(np.abs(arr - float(val))))\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.plot(x, core_cnt / N, label=\"core coverage\", linestyle=\"--\", color=\"orange\")\n",
    "    ax1.plot(x, high_cnt / N, label=\"high coverage\", linestyle=\"-.\", color=\"blue\")\n",
    "\n",
    "    ax2.plot(x, acc1, linestyle=\"-\", label=\"acc (core)\", color=\"black\")\n",
    "    ax2.plot(x, acc2, linestyle=\"-\", label=\"acc (high)\", color=\"red\")\n",
    "\n",
    "    ax2.axhline(float(overall_acc), linestyle=\":\")\n",
    "    ax2.annotate(\n",
    "        f\"overall acc\\n{overall_acc:.4f}\",\n",
    "        (x[-1], float(overall_acc)),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(18, 8),\n",
    "        ha=\"left\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "    ax1.set_xlabel(\"Entropy Threshold\")\n",
    "    ax1.set_ylabel(\"Coverage\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax1.set_ylim(0, 1.1)\n",
    "    ax2.set_ylim(0, 1.1)\n",
    "\n",
    "    ax1.grid(False)\n",
    "    ax2.grid(False)\n",
    "    plt.yticks([])\n",
    "\n",
    "    core_loc = nearest_idx(x, vline_ent_ths[0])\n",
    "\n",
    "    ax1.axvline(x[core_loc], linestyle=\"--\", alpha=0.7)\n",
    "    ax1.annotate(\n",
    "        f\"{vline_ent_ths[0]:.1f}\",\n",
    "        (x[core_loc], 0),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(6, -6),\n",
    "        ha=\"left\",\n",
    "        va=\"top\",\n",
    "        fontsize=11,\n",
    "    )\n",
    "\n",
    "    ax1.plot([x[core_loc]], [core_cnt[core_loc] / N], marker=\"o\", linestyle=\"None\", color=\"black\")\n",
    "    ax1.annotate(\n",
    "        f\"{core_cnt[core_loc] / N:.4f}\",\n",
    "        (x[core_loc], core_cnt[core_loc] / N),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(-6, 4),\n",
    "        ha=\"right\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=11,\n",
    "    )\n",
    "\n",
    "    ax1.set_xscale(\"log\")\n",
    "\n",
    "    fig.subplots_adjust(right=0.85, top=0.82)\n",
    "\n",
    "    if show_legend:\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        lines = lines1 + lines2\n",
    "        labels = labels1 + labels2\n",
    "\n",
    "        if legend_outside:\n",
    "            ax1.legend(\n",
    "                lines,\n",
    "                labels,\n",
    "                loc=\"lower center\",\n",
    "                bbox_to_anchor=(0.5, 1.02),\n",
    "                ncol=len(labels),\n",
    "                frameon=True,\n",
    "                borderaxespad=0.0,\n",
    "                handlelength=2.0,\n",
    "                columnspacing=1.2,\n",
    "            )\n",
    "        else:\n",
    "            ax1.legend(lines, labels, loc=\"best\")\n",
    "\n",
    "    _save(fig, f\"{sample_name}_counts_and_acc_dualaxis.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05479ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust this for each figures.\n",
    "high_ents = np.linspace(1.05, 0.1, 8)\n",
    "low_ents = np.linspace(0.15, np.log(1.1)/np.log(10), 6)\n",
    "very_low_ents = np.linspace(np.log(1.1)/np.log(10), 0.001, 8)\n",
    "\n",
    "base_ent_ths = np.unique(np.concatenate([high_ents, low_ents, very_low_ents]))\n",
    "base_ent_ths = np.sort(base_ent_ths)[::-1]\n",
    "\n",
    "legend_outside = False\n",
    "show_legend = False\n",
    "per_sample_ent_th_results = {}\n",
    "\n",
    "for sample in sorted(samples):\n",
    "    if not sample.endswith(\"_val\"):\n",
    "        continue\n",
    "\n",
    "    head = \"_\".join(sample.split(\"_\")[:-1])  # drop \"_val\"\n",
    "    vth = ent_th_dict.get(head, None)\n",
    "\n",
    "    # add per-sample vth into sweep thresholds\n",
    "    if vth is None:\n",
    "        ent_ths = base_ent_ths\n",
    "        low_and_high = [0.1, 1.0]\n",
    "    else:\n",
    "        ent_ths = np.unique(np.concatenate([base_ent_ths, np.array([vth], dtype=float)]))\n",
    "        ent_ths = np.sort(ent_ths)[::-1]\n",
    "        low_and_high = [float(round(vth, 4)), 1.0]\n",
    "\n",
    "    rows = []\n",
    "    for ent_th in ent_ths:\n",
    "        ent_th = float(round(float(ent_th), 4))\n",
    "        result = eval_entropy_fuzzy(\n",
    "            participating_models=u1,\n",
    "            caching=caching,\n",
    "            samples={sample: samples[sample]},\n",
    "            y_true_dict={sample: y_true[sample]},\n",
    "            ent_th=ent_th,\n",
    "            verbose=False,\n",
    "        )\n",
    "        per_sample_ent_th_results[(sample, ent_th)] = result\n",
    "        ens = result[sample][\"ensemble\"]\n",
    "        rows.append({\n",
    "            \"ent_th\": ent_th,\n",
    "            \"core count\": ens[\"low_entropy_count\"],\n",
    "            \"acc1\": ens[\"low_entropy_acc\"],\n",
    "            \"high count\": ens[\"high_entropy_count\"],\n",
    "            \"acc2\": ens[\"high_entropy_acc\"],\n",
    "        })\n",
    "\n",
    "    df_sweep = pd.DataFrame(rows)\n",
    "    overall_acc = per_sample_ent_th_results[(sample, float(round(float(ent_ths[0]), 4)))][sample][\"ensemble\"][\"overall_acc\"]\n",
    "\n",
    "    outdir = f'./analysis/figures/{sample}_u1_val'\n",
    "    plot_ent_th_sweep(\n",
    "        df_sweep,\n",
    "        overall_acc=overall_acc,\n",
    "        sample_name=sample,\n",
    "        outdir=outdir,\n",
    "        vline_ent_ths=low_and_high,\n",
    "        show_legend=show_legend,\n",
    "        legend_outside=legend_outside\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031fa6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single model entropy stats (clean vs adv) aggregated by family (resnet/vgg)\n",
    "truth = y_true[\"original_test\"].reshape(-1)\n",
    "\n",
    "ADV_DIR = apgd_out  # use your configured path\n",
    "STRONG = False\n",
    "ent_th = 0.95\n",
    "\n",
    "resnet_res, vgg_res = [], []\n",
    "\n",
    "for mn, m in u0.items():  # u0 baseline models\n",
    "    stem = Path(mn).stem\n",
    "\n",
    "    if STRONG:\n",
    "        p = ADV_DIR / f\"test_apgdce_l2_eps0p7_step0p2_it10_rinit4_from_{stem}.npz\"\n",
    "    else:\n",
    "        p = ADV_DIR / f\"test_apgdce_l2_eps0p5_step0p2_it2_rinit4_from_{stem}.npz\"\n",
    "\n",
    "    X_adv = np.load(p)[\"x_adv\"].astype(np.float32)\n",
    "\n",
    "    # clean\n",
    "    clean_pred = m.predict(samples[\"original_test\"], verbose=0)\n",
    "    clean_lab = np.argmax(clean_pred, axis=1)\n",
    "    clean_ent = -np.sum(clean_pred * np.log(clean_pred + 1e-12) / np.log(10), axis=1)\n",
    "\n",
    "    # adv\n",
    "    adv_pred = m.predict(X_adv, verbose=0)\n",
    "    adv_lab = np.argmax(adv_pred, axis=1)\n",
    "    adv_ent = -np.sum(adv_pred * np.log(adv_pred + 1e-12) / np.log(10), axis=1)\n",
    "\n",
    "    adv_core = adv_ent <= ent_th\n",
    "    cln_core = clean_ent <= ent_th\n",
    "\n",
    "    adv_cnt = int(adv_core.sum())\n",
    "    cln_cnt = int(cln_core.sum())\n",
    "\n",
    "    adv_cov = adv_cnt / X_adv.shape[0]\n",
    "    cln_cov = cln_cnt / samples[\"original_test\"].shape[0]\n",
    "\n",
    "    adv_acc = float(np.mean(adv_lab[adv_core] == truth[adv_core])) if adv_cnt > 0 else 0.0\n",
    "    cln_acc = float(np.mean(clean_lab[cln_core] == truth[cln_core])) if cln_cnt > 0 else 0.0\n",
    "\n",
    "    row = [adv_cov, cln_cov, adv_acc, cln_acc]\n",
    "    (resnet_res if \"resnet\" in mn else vgg_res).append(row)\n",
    "\n",
    "print(\"adv_cov, clean_cov, adv_acc, clean_acc\")\n",
    "\n",
    "print(\"resnet\")\n",
    "R = np.array(resnet_res, dtype=float)\n",
    "print(R.mean(axis=0))\n",
    "print(R.std(axis=0))\n",
    "\n",
    "print(\"vgg\")\n",
    "V = np.array(vgg_res, dtype=float)\n",
    "print(V.mean(axis=0))\n",
    "print(V.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0424ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_stats(ens, X, y, ent_th, sample_name=\"\"):\n",
    "    core_preds, core, _ = ens.core_preds(X, sample_name=sample_name, ent_th=float(ent_th))\n",
    "    cnt = int(core.sum())\n",
    "    acc = float(np.mean(np.argmax(core_preds, axis=-1) == y[core])) if cnt > 0 else 0.0\n",
    "    cov = float(core.mean())\n",
    "    return acc, cov, cnt\n",
    "\n",
    "truth = y_true[\"original_test\"].reshape(-1)\n",
    "ent_th = 0.95\n",
    "\n",
    "U0 = ProbAverageEnsembleFromProbModels(u0)\n",
    "SPs = ProbAverageEnsembleFromProbModels(sps)\n",
    "U1 = ProbAverageEnsembleFromProbModels(u1)\n",
    "cases = [\n",
    "    (\"U0_clean_test\",   U0,  samples[\"original_test\"]),\n",
    "    (\"U0_weak_test\",    U0,  samples['U0_weak_test']),\n",
    "    (\"U0_strong_test\",  U0,  samples['U0_strong_test']),\n",
    "    (\"SPs_weak_test\",   SPs, samples['SPs_weak_test']),\n",
    "    (\"SPs_strong_test\", SPs, samples['SPs_strong_test']),\n",
    "    (\"U1_weak_test\",    U1,  samples['U1_weak_test']),\n",
    "    (\"U1_strong_test\",  U1,  samples['U1_strong_test']),\n",
    "]\n",
    "\n",
    "for name, ens, X in cases:\n",
    "    acc, cov, cnt = core_stats(ens, X, truth, ent_th, sample_name=name)\n",
    "    print(f\"{name:10s} | core_acc {acc:.4f} | core_cov {cov:.4f} | core_cnt {cnt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e749c2dc",
   "metadata": {},
   "source": [
    "## Evaluate Logifold / IMM (if second layer exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f003dc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData = Tuple[np.ndarray, np.ndarray]\n",
    "ValData = Tuple[np.ndarray, np.ndarray]\n",
    "\n",
    "class HistoryDict(TypedDict, total=False):\n",
    "    history: Dict[str, List[float]]\n",
    "    params: Dict[str, Any]\n",
    "    epoch: List[int]\n",
    "\n",
    "\n",
    "class SpecializeResult(TypedDict):\n",
    "    model_path: str   \n",
    "    history: HistoryDict  \n",
    "\n",
    "        \n",
    "def specialize(\n",
    "    new_train : TrainData,\n",
    "    new_val : ValData,\n",
    "    original_model : tf.keras.Model,\n",
    "    new_model_path : str = 'specialized_model.keras',\n",
    "    path : Path = Path('./data/specialized_models/'),\n",
    "    verbose = 1,\n",
    "    epochs = 21\n",
    "                      ) -> dict:\n",
    "    \"\"\"\n",
    "    Returns (baseline_adv_model, tuned_baseline_adv_model, tuned_history_dict_or_None)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    x_tr, y_tr = new_train\n",
    "    x_v, y_v = new_val\n",
    "    if y_tr.ndim == 1 or y_tr.shape[1] != 10:\n",
    "        y_tr = to_categorical(y_tr, 10)\n",
    "    if y_v.ndim == 1 or y_v.shape[1] != 10:\n",
    "        y_v = to_categorical(y_v, 10)\n",
    "    if not new_model_path.lower().endswith('.keras'):\n",
    "        new_model_path += '.keras'\n",
    "        \n",
    "    model_path  = path /  Path(new_model_path)    \n",
    "\n",
    "    if model_path.exists():\n",
    "        specialized_model = load_model(model_path)\n",
    "        print(f'{model_path} already exists.')\n",
    "        hist = custom_specialization.load_history(model_path) \n",
    "        \n",
    "        if hist is None:\n",
    "            print(f\"[WARN] No history found for {model_path}. History is empty dictionary.\")\n",
    "            hist = {}\n",
    "    else:\n",
    "        print(f'{model_path} training...')\n",
    "        specialized_model,hist = custom_specialization.turn_specialist(original_model, path = model_path,\n",
    "                                                x_tr=x_tr, y_tr=y_tr,\n",
    "                                                  x_v=x_v,   y_v=y_v,\n",
    "                                                  epochs=epochs, learning_rate=1e-3, batch_size=128, verbose=verbose, name=f\"tuned_once\")\n",
    "        hist = {\"history\": hist.history, \"params\": hist.params, \"epoch\": hist.epoch}\n",
    "        specialized_model.save(model_path)\n",
    "    return {\n",
    "    \"model_path\": str(model_path),\n",
    "    \"history\": hist,\n",
    "}, specialized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b948402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "First_layer = u1\n",
    "baseline = u0\n",
    "model_dict = u0\n",
    "specialized_model_dict = sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e72f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv\n",
    "from datetime import datetime\n",
    "experiment_description = '''\n",
    "Baseline : Res + VGG\n",
    "first layer : Res\n",
    "Second layer : Res\n",
    "union : clean + weak + strong\n",
    "'''\n",
    "def log_metrics_csv(out_csv, row: dict):\n",
    "    os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
    "    write_header = not os.path.exists(out_csv)\n",
    "    with open(out_csv, \"a\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=list(row.keys()))\n",
    "        if write_header:\n",
    "            w.writeheader()\n",
    "        w.writerow(row)\n",
    "        \n",
    "def log_text(out_txt, text: str):\n",
    "    os.makedirs(os.path.dirname(out_txt), exist_ok=True)\n",
    "    with open(out_txt, \"a\") as f:\n",
    "        f.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6fd941",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_name, sample in samples.items():\n",
    "    tokens = sample_name.split('_')\n",
    "    split = tokens[-1]\n",
    "#     # union only\n",
    "    if 'U0' not in sample_name:\n",
    "        continue\n",
    "    if split != 'test':\n",
    "        continue\n",
    "    else:\n",
    "        sample_name_head = '_'.join(tokens[:-1])\n",
    "        sample_val = sample_name_head + '_val'\n",
    "        sample_train = sample_name_head + '_train'\n",
    "        sample_test = sample_name_head + '_test'\n",
    "    \n",
    "    if sample_val not in samples or sample_train not in samples:\n",
    "        print(f'[WARN] {sample_name} does not have train/val samples. Skip.')\n",
    "        continue\n",
    "    \n",
    "    print(sample_name, end = ' | ')\n",
    "    truth = y_true[sample_name]\n",
    "    \n",
    "    ent_first = caching.get_entropy(sorted(First_layer.keys()), sample_name)\n",
    "    \n",
    "    \n",
    "    ent_th = round(ent_th_dict[sample_name_head],3)\n",
    "    print('ent_th:' , ent_th, end = ' | ')\n",
    "    print('----------------------------------------')\n",
    "    print('-----------------------------------------------------------------------')\n",
    "    str_ent_th = str(ent_th).replace('.', 'p')\n",
    "    out_of_core_th = ent_th\n",
    "\n",
    "\n",
    "    ## Get Specialized 2nd layer experts\n",
    "    ## Prepare for OoC samples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    s = np.concatenate([samples[sample_train], samples[sample_val]], axis=0)\n",
    "    t = np.concatenate([y_true[sample_train], y_true[sample_val]], axis=0)\n",
    "    new_ent = np.concatenate([caching.get_entropy(sorted(First_layer.keys()), sample_train), caching.get_entropy(sorted(First_layer.keys()), sample_val)], axis = 0)\n",
    "    OoC_idx = new_ent>out_of_core_th\n",
    "    OoC_s = s.copy()[OoC_idx]\n",
    "    OoC_t = t.copy()[OoC_idx]\n",
    "    if OoC_t.shape[0] < 0.1*t.shape[0]:\n",
    "            print(f'[WARN] out_of_core_th={out_of_core_th:.3e} -> OoC size too small:', OoC_t.shape[0])\n",
    "            print('Not preceed further.')\n",
    "            continue\n",
    "    OoC_x_train, OoC_x_val, OoC_y_train, OoC_y_val = train_test_split(\n",
    "        OoC_s, OoC_t,\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "        stratify=OoC_t\n",
    "    )\n",
    "    print(f'For 2nd layer experts, we prepared {OoC_y_train.shape} training samples and {OoC_y_val.shape} validation samples')\n",
    "    Second_layer={}\n",
    "    adv_sample_name = sample_name_head + '_OoC_E1E2'+'_'+str_ent_th\n",
    "    for model_name, m in model_dict.items():\n",
    "        # ResNet only\n",
    "        if 'resnet' not in model_name:\n",
    "            continue\n",
    "        model_name = model_name[:-6]\n",
    "        model_name = model_name.split('_')[0] if model_name[0] == 'v' else model_name.split('_')[0] + model_name.split('_')[-1] + 'v'+model_name.split('_')[1][-1]\n",
    "        model_path = Path('./data/specialized_models/') / Path(f\"temp/{model_name}_{adv_sample_name}-SP.keras\")\n",
    "        if model_path.exists():\n",
    "            Second_layer[f\"{model_name}_{adv_sample_name}\"] = load_model(model_path)\n",
    "        else:\n",
    "            info = specialize(\n",
    "                    (OoC_x_train, to_categorical(OoC_y_train,10)),\n",
    "                    (OoC_x_val, to_categorical(OoC_y_val,10)),\n",
    "                    m,\n",
    "                    new_model_path=f\"temp/{model_name}_{adv_sample_name}-SP\",\n",
    "                verbose = 0)\n",
    "\n",
    "            Second_layer[f\"{model_name}_{adv_sample_name}\"] = info[1]\n",
    "\n",
    "    ## Reload baseline models\n",
    "    model_dict = {}\n",
    "    for f_name in sorted(tf_model_dir.glob(\"*.keras\")):\n",
    "        if f_name.endswith('keras'):\n",
    "            m = load_model(f\"./data/models/{f_name}\")\n",
    "            model_dict[f_name] = m\n",
    "    First_layer = {}\n",
    "    First_layer.update(model_dict)\n",
    "    First_layer.update(specialized_model_dict)\n",
    "\n",
    "    ## baseline results\n",
    "    baseline = ProbAverageEnsembleFromProbModels(model_dict)\n",
    "    p_baseline = baseline.probs(sample, sample_name)\n",
    "    acc_baseline = np.mean(np.argmax(p_baseline, axis=-1) == truth)\n",
    "\n",
    "    ## First layer results\n",
    "    first_layer = ProbAverageEnsembleFromProbModels(First_layer)\n",
    "    p_first = first_layer.probs(sample, sample_name)\n",
    "    acc_first = np.mean(np.argmax(p_first, axis=-1) == truth)\n",
    "\n",
    "    first_core_loc = ent_first<ent_th\n",
    "    first_core_ent = ent_first[first_core_loc]\n",
    "\n",
    "    core_p_first = p_first[first_core_loc]\n",
    "    core_truth = truth[first_core_loc]\n",
    "    core_acc_first = np.mean(np.argmax(core_p_first, axis=-1) == core_truth)\n",
    "    OutCore_acc_first = np.mean(np.argmax(p_first[~first_core_loc], axis=-1) == truth[~first_core_loc])\n",
    "\n",
    "    ## Wrap all_ens and second_layer_ens\n",
    "    All_layer = {}\n",
    "    All_layer.update(First_layer)\n",
    "    All_layer.update(Second_layer)\n",
    "    all_ens = ProbAverageEnsembleFromProbModels(All_layer)\n",
    "    second_layer_ens = ProbAverageEnsembleFromProbModels(Second_layer)\n",
    "\n",
    "    ## All ensemble results\n",
    "    p_all = all_ens.probs(sample, sample_name)\n",
    "    acc_all = float(np.mean(np.argmax(p_all, axis=-1) == truth))\n",
    "\n",
    "    ## Second results\n",
    "    p_second = second_layer_ens.probs(sample, sample_name)\n",
    "    acc_second = float(np.mean(np.argmax(p_second, axis=-1) == truth))\n",
    "\n",
    "    p_second_OutCore = p_second[~first_core_loc]\n",
    "    acc_second_OutCore = float(np.mean(np.argmax(p_second_OutCore, axis=-1) == truth[~first_core_loc]))\n",
    "\n",
    "    preds = []\n",
    "    for model_name in Second_layer.keys():\n",
    "        pred = caching.get_pred(model_name, sample_name)\n",
    "        preds.append(pred)\n",
    "    preds = np.array(preds)\n",
    "    ent_second = []\n",
    "    for i in range(preds.shape[1]):\n",
    "        ent_second.append(utils.cross_entropy(preds[:,i,: ]))\n",
    "    ent_second = np.array(ent_second)\n",
    "    ent_second = ent_second[~first_core_loc]\n",
    "    second_core = ent_second<out_of_core_th\n",
    "    p_second_core = p_second_OutCore[second_core]\n",
    "    acc_second_core_of_OutCore = np.mean(np.argmax(p_second_core, axis=-1) == truth[~first_core_loc][second_core])\n",
    "\n",
    "    ## Gated results (IMM)\n",
    "    acc_gated = np.sum(np.argmax(p_first[first_core_loc], axis=-1) == truth[first_core_loc]) + np.sum(np.argmax(p_second_OutCore, axis=-1) == truth[~first_core_loc]) \n",
    "    acc_gated /= sample.shape[0]\n",
    "\n",
    "    # See behaviour on AutoAttack samples\n",
    "    sample_name_aa = 'AA_std_test'\n",
    "    sample_aa = samples[sample_name_aa]\n",
    "    truth_aa = y_true[sample_name_aa]\n",
    "\n",
    "    aa_acc_baseline = float(np.mean(np.argmax(baseline.probs(sample_aa, sample_name_aa), axis=-1) == truth_aa))\n",
    "    aa_p_first_layer = first_layer.probs(sample_aa, sample_name_aa)\n",
    "    aa_acc_first = float(np.mean(np.argmax(aa_p_first_layer, axis=-1) == truth_aa))\n",
    "    aa_first_ent = caching.get_entropy(sorted(First_layer.keys()), sample_name_aa)\n",
    "    aa_first_core_loc = aa_first_ent<out_of_core_th\n",
    "\n",
    "    aa_core_p_first = aa_p_first_layer[aa_first_core_loc]\n",
    "    aa_core_truth = truth_aa[aa_first_core_loc]\n",
    "    aa_core_acc_first = np.mean(np.argmax(aa_core_p_first, axis=-1) == aa_core_truth)\n",
    "\n",
    "    aa_p_second = second_layer_ens.probs(sample_aa, sample_name_aa)\n",
    "    aa_acc_second = float(np.mean(np.argmax(aa_p_second, axis=-1) == truth_aa))\n",
    "\n",
    "    aa_p_second_OutCore = aa_p_second[~aa_first_core_loc]\n",
    "    aa_acc_second_OutCore = float(np.mean(np.argmax(aa_p_second_OutCore, axis=-1) == truth_aa[~aa_first_core_loc])) \n",
    "    preds = []\n",
    "    for model_name in Second_layer.keys():\n",
    "        pred = caching.get_pred(model_name, sample_name_aa)\n",
    "        preds.append(pred)\n",
    "    preds = np.array(preds)\n",
    "    aa_ent_second = []\n",
    "    for i in range(preds.shape[1]):\n",
    "        aa_ent_second.append(utils.cross_entropy(preds[:,i,: ]))\n",
    "    aa_ent_second = np.array(aa_ent_second)\n",
    "    aa_ent_second = aa_ent_second[~aa_first_core_loc]\n",
    "    aa_second_core = aa_ent_second<out_of_core_th\n",
    "    aa_p_second_core = aa_p_second_OutCore[aa_second_core]\n",
    "    aa_acc_second_core_of_OutCore = np.mean(np.argmax(aa_p_second_core, axis=-1) == truth_aa[~aa_first_core_loc][aa_second_core])\n",
    "\n",
    "    aa_acc_gated = np.sum(np.argmax(aa_p_first_layer[aa_first_core_loc], axis=-1) == truth_aa[aa_first_core_loc]) + np.sum(np.argmax(aa_p_second_OutCore, axis=-1) == truth_aa[~aa_first_core_loc]) \n",
    "    aa_acc_gated /= sample_aa.shape[0]\n",
    "\n",
    "    aa_acc_all = float(np.mean(np.argmax(all_ens.probs(sample_aa, sample_name_aa), axis=-1) == truth_aa))\n",
    "    print('----------------------------------------')\n",
    "    print('----------------------------------------')\n",
    "    print(f\"Sample name : {sample_name} (size : {sample.shape[0]}), out_of_core_th : {out_of_core_th}\")\n",
    "    print(f\"First layer acc : {acc_first}\")\n",
    "    print(f\"Baseline acc : {acc_baseline}\")\n",
    "    print(f\"Second layer acc : {acc_second}\")\n",
    "    print(f\"All members ensemble acc : {acc_all}\")\n",
    "    print(f\"Entropy threshold for first layer : {out_of_core_th}\")\n",
    "    print(f\"First layer core acc : {core_acc_first} (size : {np.sum(first_core_loc)})\")\n",
    "    print(f\"First layer OutCore acc : {OutCore_acc_first} (size : {np.sum(~first_core_loc)})\")\n",
    "    print(f\"First layer core entropy : {np.sum(first_core_ent)}\")\n",
    "    print(f\"First layer entropy : {np.sum(ent_first)}\")\n",
    "    print(f\"First layer OutCore entropy : {np.sum(ent_first[~first_core_loc])}\")\n",
    "    print(f\"First layer core coverage : {np.sum(first_core_loc) / sample.shape[0]}\")\n",
    "    print(f\"Second layer acc on OutCore from first layer  : {acc_second_OutCore} (size : {np.sum(~first_core_loc)})\")\n",
    "    print(f\"Second layer core acc on OutCore from first layer : {acc_second_core_of_OutCore} (size : {np.sum(second_core)})\")\n",
    "    print(f\"Second layer core entropy on OutCore from first layer : {np.sum(ent_second[second_core])}\")\n",
    "    print(f\"Second layer entropy on OutCore from first layer : {np.sum(ent_second)}\")\n",
    "    print(f\"Second layer OutCore entropy on OutCore from first layer : {np.sum(ent_second[~second_core])}\")\n",
    "    print(f\"Second layer core coverage on OutCore from first layer : {np.sum(second_core) / ent_second.shape[0]}\")\n",
    "    print(f\"out core of second layer count : {np.sum(~second_core)}\")\n",
    "    print(f\"Gated ensemble acc : {acc_gated}\")\n",
    "    print(f\"total entropy = first layer core ent + second layer ent on OutCore from first layer\\n{np.sum(first_core_ent)} + {np.sum(ent_second)} = {np.sum(first_core_ent)+np.sum(ent_second)}\")\n",
    "    print(\"total core entropy (first layer) + core entropy (second layer) : \", np.sum(first_core_ent) + np.sum(ent_second[second_core]))\n",
    "    print(f\"maximum acc among baseline, first layer, second layer : {max(acc_baseline, acc_first, acc_second)}\")\n",
    "    print(f\"difference between gated ensemble and maximum acc : {acc_gated - max(acc_baseline, acc_first, acc_second)}\")\n",
    "\n",
    "    print()\n",
    "    print('AutoAttack result')\n",
    "    print(f\"AA Sample name : {sample_name_aa} (size : {sample_aa.shape[0]}), out_of_core_th : {out_of_core_th}\")\n",
    "    print(f\"First layer acc : {aa_acc_first}\")\n",
    "    print(f\"Baseline acc : {aa_acc_baseline}\")\n",
    "    print(f\"Second layer acc : {aa_acc_second}\")\n",
    "    print(f\"All members ensemble acc : {aa_acc_all}\")\n",
    "    print(f\"Entropy threshold for first layer : {out_of_core_th}\")\n",
    "    print(f\"First layer core acc : {aa_core_acc_first} (size : {np.sum(aa_first_core_loc)})\")\n",
    "    print(f\"Second layer acc on OutCore from first layer  : {aa_acc_second_OutCore} (size : {np.sum(~aa_first_core_loc)})\")\n",
    "    print(f\"Second layer core acc on OutCore from first layer : {aa_acc_second_core_of_OutCore} (size : {np.sum(aa_second_core)})\")\n",
    "    print(f\"Gated ensemble  acc : {aa_acc_gated}\")\n",
    "    print()\n",
    "    txt = \"\\n\".join([\n",
    "        \"----------------------------------------\",\n",
    "        f\"Sample name : {sample_name} (size : {sample.shape[0]}), out_of_core_th : {out_of_core_th}\",\n",
    "        f\"First layer acc : {acc_first}\",\n",
    "        f\"Baseline acc : {acc_baseline}\",\n",
    "        f\"Second layer acc : {acc_second}\",\n",
    "        f\"All members ensemble acc : {acc_all}\",\n",
    "        f\"Entropy threshold for first layer : {out_of_core_th}\",\n",
    "        f\"First layer core acc : {core_acc_first} (size : {np.sum(first_core_loc)})\",\n",
    "        f\"First layer OutCore acc : {OutCore_acc_first} (size : {np.sum(~first_core_loc)})\",\n",
    "        f\"First layer core entropy : {np.sum(first_core_ent)}\",\n",
    "        f\"First layer entropy : {np.sum(ent_first)}\",\n",
    "        f\"First layer OutCore entropy : {np.sum(ent_first[~first_core_loc])}\",\n",
    "        f\"First layer core coverage : {np.sum(first_core_loc) / sample.shape[0]}\",\n",
    "        f\"Second layer acc on OutCore from first layer  : {acc_second_OutCore} (size : {np.sum(~first_core_loc)})\",\n",
    "        f\"Second layer core acc on OutCore from first layer : {acc_second_core_of_OutCore} (size : {np.sum(second_core)})\",\n",
    "        f\"Second layer core entropy on OutCore from first layer : {np.sum(ent_second[second_core])}\",\n",
    "        f\"Second layer entropy on OutCore from first layer : {np.sum(ent_second)}\",\n",
    "        f\"out core of second layer count : {np.sum(~second_core)}\",\n",
    "        f\"Second layer OutCore entropy on OutCore from first layer : {np.sum(ent_second[~second_core])}\",\n",
    "        f\"Second layer core coverage on OutCore from first layer : {np.sum(second_core) / ent_second.shape[0]}\",\n",
    "        f\"Gated ensemble acc : {acc_gated}\",\n",
    "        f\"total entropy = first layer core ent + second layer ent on OutCore from first layer\\n{np.sum(first_core_ent)} + {np.sum(ent_second)} = {np.sum(first_core_ent)+np.sum(ent_second)}\",\n",
    "        f\"total core entropy (first layer) + core entropy (second layer) : {np.sum(first_core_ent) + np.sum(ent_second[second_core])}\",\n",
    "        f\"maximum acc among baseline, first layer, second layer : {max(acc_baseline, acc_first, acc_second)}\",\n",
    "        f\"difference between gated ensemble and maximum acc : {acc_gated - max(acc_baseline, acc_first, acc_second)}\",\n",
    "    ])\n",
    "\n",
    "    log_text(\"./analysis/logs/run.txt\", txt)\n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
