{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea35ca26",
   "metadata": {},
   "source": [
    "# Main experiment (TF2.11)\n",
    "\n",
    "This notebook produces the main experimental pipeline:\n",
    "- Load CIFAR-10 and adversarial samples produced in `01` (Torch AutoAttack) and `03` (TF2 + ART APGD).\n",
    "- Cache per-model predictions and compute ensemble entropy.\n",
    "- Sweep entropy thresholds (core vs out-of-core) and report accuracy/coverage.\n",
    "- Specialize **second-generation $\\mathcal{U}^{(2)^\\prime}$** on the out-of-core region and evaluate the full IMM pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba0cb8a",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    "### import libraries and set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ae187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import source.cache_store as cache_store\n",
    "import source.custom_specialization as custom_specialization\n",
    "import source.utils as utils\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from source.utils import load_yaml\n",
    "\n",
    "\n",
    "PATHS = load_yaml(\"./configs/paths.yaml\")\n",
    "EXP   = load_yaml(\"./configs/exp.yaml\")\n",
    "\n",
    "data_root    = Path(PATHS[\"data_root\"])\n",
    "results_root = Path(PATHS[\"results_root\"])\n",
    "tf_model_dir = Path(PATHS[\"tf_model_dir\"])\n",
    "SPs_model_dir = Path(PATHS[\"SPs_model_dir\"])\n",
    "autoattack_out = Path(PATHS[\"autoattack_out\"])\n",
    "apgd_out     = Path(PATHS[\"apgd_out\"])\n",
    "cache_root   = Path(PATHS[\"cache_root\"])\n",
    "\n",
    "seed = int(EXP[\"seed\"])\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Ensure dirs\n",
    "results_root.mkdir(parents=True, exist_ok=True)\n",
    "cache_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"tf_model_dir:\", tf_model_dir)\n",
    "print(\"autoattack_out:\", autoattack_out)\n",
    "print(\"apgd_out:\", apgd_out)\n",
    "print(\"cache_root:\", cache_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de472964",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5041757f",
   "metadata": {},
   "source": [
    "### Load CIFAR-10 (Keras) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f92f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_all, y_all), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "y_all  = y_all.reshape(-1).astype(np.int64)\n",
    "y_test = y_test.reshape(-1).astype(np.int64)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=seed, stratify=y_all\n",
    ")\n",
    "\n",
    "x_train = x_train.astype(np.float32)/255.0\n",
    "x_val   = x_val.astype(np.float32)/255.0\n",
    "x_test  = x_test.astype(np.float32)/255.0\n",
    "\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_val   = y_val.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb51b6b",
   "metadata": {},
   "source": [
    "### Load adversarial samples from 01 (Torch AutoAttack) and 03 (ART APGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a11ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples: Dict[str, np.ndarray] = {}\n",
    "y_true: Dict[str, np.ndarray] = {}\n",
    "\n",
    "# Original\n",
    "samples[\"original_train\"] = x_train.copy()\n",
    "samples[\"original_val\"]   = x_val.copy()\n",
    "samples[\"original_test\"]  = x_test.copy()\n",
    "y_true[\"original_train\"] = y_train.copy()\n",
    "y_true[\"original_val\"]   = y_val.copy()\n",
    "y_true[\"original_test\"]  = y_test.copy()\n",
    "\n",
    "# Torch AutoAttack outputs\n",
    "aa_path = autoattack_out / \"x_adv_test_std_l2_eps5.npy\"\n",
    "x_adv = np.load(aa_path).astype(np.float32)\n",
    "\n",
    "if x_adv.ndim == 4 and x_adv.shape[1] == 3 and x_adv.shape[-1] != 3:\n",
    "    x_adv = np.transpose(x_adv, (0, 2, 3, 1))\n",
    "    \n",
    "samples[\"AA_std_test\"] = x_adv\n",
    "y_true[\"AA_std_test\"] = y_test.copy()\n",
    "\n",
    "# ART APGD outputs -> key = \"{tag}_{split}_{weak/strong}\"\n",
    "for p in sorted(apgd_out.glob(\"*.npz\")):\n",
    "    if 'from' in str(p):\n",
    "        continue\n",
    "    d = np.load(p)\n",
    "\n",
    "    split = str(d[\"split\"]).split('_')[0] if \"split\" in d else \"unknown\"\n",
    "    # released sample has no tag.\n",
    "    tag   = str(d[\"tag\"])   if \"tag\"   in d else \"U0\"\n",
    "    it    = int(d[\"max_iter\"]) if \"max_iter\" in d else None\n",
    "\n",
    "    adv_name = \"APGD_weak\" if it == 2 else \"APGD_strong\"\n",
    "    sample_name = f\"{tag}_{adv_name}_{split}\" if tag else f\"{adv_name}_{split}\"\n",
    "\n",
    "    samples[sample_name] = d[\"x_adv\"].astype(np.float32)\n",
    "    y_true[sample_name]  = d[\"y\"].astype(np.int64).reshape(-1)\n",
    "\n",
    "def make_union(tag: str, split: str):\n",
    "    return (\n",
    "        np.concatenate([samples[f\"original_{split}\"],\n",
    "                        samples[f\"{tag}_APGD_weak_{split}\"],\n",
    "                        samples[f\"{tag}_APGD_strong_{split}\"]], axis=0).astype(np.float32),\n",
    "        np.concatenate([y_true[f\"original_{split}\"],\n",
    "                        y_true[f\"{tag}_APGD_weak_{split}\"],\n",
    "                        y_true[f\"{tag}_APGD_strong_{split}\"]], axis=0).astype(np.int64),\n",
    "    )\n",
    "\n",
    "\n",
    "samples[\"U0_union_train\"], y_true[\"U0_union_train\"] = make_union(\"U0\", \"train\")\n",
    "samples[\"U0_union_val\"],   y_true[\"U0_union_val\"]   = make_union(\"U0\", \"val\")\n",
    "samples[\"U0_union_test\"],  y_true[\"U0_union_test\"]  = make_union(\"U0\", \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f2a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since released file has no 'tag', put tag manually.\n",
    "## If it's generated via the notebooks, then this procedure is automatically skipped.\n",
    "\n",
    "for p in sorted(apgd_out.glob(\"*.npz\")):\n",
    "    if not 'first_layer' in str(p) and not 'weak-SP' in str(p):\n",
    "        continue\n",
    "    d = np.load(p)\n",
    "    if 'first_layer' in str(p):\n",
    "        tag = 'U1'\n",
    "    elif 'weak-SP' in str(p):\n",
    "        tag = 'SPs'\n",
    "    \n",
    "    split = str(d[\"split\"]).split('_')[0] if \"split\" in d else \"unknown\"\n",
    "    it    = int(d[\"max_iter\"]) if \"max_iter\" in d else None\n",
    "    adv_name = \"APGD_weak\" if it == 2 else \"APGD_strong\"\n",
    "    \n",
    "    sample_name = f\"{tag}_{adv_name}_{split}\" if tag else f\"{adv_name}_{split}\"\n",
    "    samples[sample_name] = d[\"x_adv\"].astype(np.float32)\n",
    "    y_true[sample_name]  = d[\"y\"].astype(np.int64).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d4a0d2",
   "metadata": {},
   "source": [
    "### Load models and build first-layer committee (U0 âˆª U1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e828549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.helpers04note import load_models_from_dir\n",
    "\n",
    "u0 = load_models_from_dir(tf_model_dir, pattern=\"*.keras\")\n",
    "sps = load_models_from_dir(SPs_model_dir, pattern=\"*.keras\")\n",
    "u1 = {}\n",
    "u1.update(u0)\n",
    "u1.update(sps)\n",
    "\n",
    "# committees\n",
    "u0_keys = sorted(u0.keys())\n",
    "u1_keys = sorted(u1.keys())\n",
    "sps_keys = sorted(sps.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0250642e",
   "metadata": {},
   "source": [
    "### Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ec8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = input(\"Type 'Y' if you want to clear the cache: \")\n",
    "if z == 'Y':\n",
    "    utils.clear_folder(str(cache_root), dry_run=False)\n",
    "else:\n",
    "    print(\"Cache not cleared.\")\n",
    "caching = cache_store.ResultStore(root=str(cache_root))\n",
    "\n",
    "\n",
    "# predict-cache: U1 (== First_layer) only (contains U0 + SPs)\n",
    "participating_models = u1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abcfe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for sample_name, X in samples.items():\n",
    "    Y = y_true[sample_name]\n",
    "    print(f\"{len(participating_models)} models caching preds on {sample_name}...\", end=\"---\")\n",
    "\n",
    "    for model_name, model in participating_models.items():\n",
    "        pred = model.predict(X, verbose=0)\n",
    "        caching.set_pred(model_name, sample_name, pred)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # entropy for U0\n",
    "    print(f\"caching entropy (U0) on {sample_name}...\", end=\"---\")\n",
    "    P0 = np.stack([caching.get_pred(k, sample_name) for k in u0_keys], axis=0)  # (K,N,C)\n",
    "    ent0 = utils.cross_entropy(P0)\n",
    "    caching.set_entropy(u0_keys, sample_name, ent0)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # entropy for U1 (= U0 + SPs)\n",
    "    print(f\"caching entropy (U1) on {sample_name}...\", end=\"---\")\n",
    "    P1 = np.stack([caching.get_pred(k, sample_name) for k in u1_keys], axis=0)\n",
    "    ent1 = utils.cross_entropy(P1)\n",
    "    caching.set_entropy(u1_keys, sample_name, ent1)\n",
    "    print(\"Done.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58f77cc",
   "metadata": {},
   "source": [
    "## Find suitable total entropy threshold for each dataset\n",
    "\n",
    "whose validation sample exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d43595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming all raw predictions already cached\n",
    "\n",
    "def ens_probs(keys: List[str], sample_name: str) -> np.ndarray:\n",
    "    P = np.stack([caching.get_pred(k, sample_name) for k in keys], axis=0)  # (K,N,C)\n",
    "    return P.mean(axis=0)  # (N,C)\n",
    "\n",
    "def core_mask(keys: List[str], sample_name: str, ent_th: float) -> np.ndarray:\n",
    "    ent = caching.get_entropy(keys, sample_name)\n",
    "    return ent <= float(ent_th)\n",
    "\n",
    "# Target acc is based on the (clean) validation accuracy of baseline (U0)\n",
    "TARGET_ACC = 0.95\n",
    "# Or..\n",
    "# TARGET_ACC = np.mean(np.argmax(ens_probs(u0_keys, 'original_val'), axis=1) == y_true['original_val'])\n",
    "\n",
    "# We could use searching algorithm\n",
    "# But here we use a simple grid search\n",
    "th_grid = np.arange(2.0, 1e-2, -0.01)\n",
    "\n",
    "\n",
    "def find_th_for_core_acc(keys: List[str], sample_name: str, y: np.ndarray):\n",
    "    '''\n",
    "    return (entrotpy threshold, core acc, core coverage, core count)\n",
    "    or (0, nan, 0, 0) if no threshold satisfies the target accuracy.\n",
    "    It happens if the participating committee tells different answers at almost all samples.\n",
    "    \n",
    "    '''\n",
    "    p = ens_probs(keys, sample_name)\n",
    "    for th in th_grid:\n",
    "        core = core_mask(keys, sample_name, th)\n",
    "        if int(core.sum()) < 10:\n",
    "            # If core size is too small, that means the entropy threshold is too strict.\n",
    "            # and hence we don't need to investigate it further\n",
    "            break\n",
    "        acc = float(np.mean(np.argmax(p[core], axis=1) == y[core]))\n",
    "        if acc >= TARGET_ACC:\n",
    "            return float(th), acc, float(core.mean()), int(core.sum())\n",
    "    \n",
    "    return 0, float('nan'), 0, 0\n",
    "\n",
    "ent_th_dict = {}\n",
    "for sample_name in samples:\n",
    "    if not sample_name.endswith(\"_val\"):\n",
    "        continue\n",
    "    head = \"_\".join(sample_name.split(\"_\")[:-1])\n",
    "    out = find_th_for_core_acc(u1_keys, sample_name, y_true[sample_name])\n",
    "    ent_th_dict[head] = out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6717615",
   "metadata": {},
   "source": [
    "## Plot graphs : Entropy sweep of U1 committee on validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb460adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_entropy_fuzzy(\n",
    "    committee_keys: List[str],\n",
    "    sample_name: str,\n",
    "    ent_th: float,\n",
    ") -> Dict[str, Any]:\n",
    "    '''\n",
    "    Committee provides fuzzy evaluation based on entropy threshold.\n",
    "    \n",
    "    return\n",
    "    -------\n",
    "        \"overall_acc\": simple average accuracy (on whole given dataset),\n",
    "        \"low_entropy_count\": core count,\n",
    "        \"low_entropy_acc\": core accuracy,\n",
    "        \"high_entropy_count\": out of core count,\n",
    "        \"high_entropy_acc\": out of core accuracy,\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    y = y_true[sample_name].reshape(-1)\n",
    "    ent = caching.get_entropy(committee_keys, sample_name)\n",
    "    low = ent <= float(ent_th)\n",
    "    high = ~low\n",
    "\n",
    "    p = ens_probs(committee_keys, sample_name)\n",
    "    overall = float(np.mean(np.argmax(p, axis=1) == y))\n",
    "\n",
    "    low_acc  = float(np.mean(np.argmax(p[low], axis=1) == y[low])) if low.any() else 0.0\n",
    "    high_acc = float(np.mean(np.argmax(p[high], axis=1) == y[high])) if high.any() else 0.0\n",
    "\n",
    "    return {\n",
    "        \"overall_acc\": overall,\n",
    "        \"low_entropy_count\": int(low.sum()),\n",
    "        \"low_entropy_acc\": low_acc,\n",
    "        \"high_entropy_count\": int(high.sum()),\n",
    "        \"high_entropy_acc\": high_acc,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40196737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ent_th_sweep(\n",
    "    df : pd.DataFrame,\n",
    "    overall_acc,\n",
    "    sample_name=\"AA_RBST_test\",\n",
    "    outdir=None,\n",
    "    vline_ent_ths: list[float] = [0.0414, 1.0],\n",
    "    show_legend: bool = False,\n",
    "    legend_outside: bool = True,\n",
    "\n",
    "):\n",
    "    ''' vline_ent_ths = [ent_th found on validation sample, high entropy threshold (arbitrary)]'''\n",
    "    assert len(vline_ent_ths) == 2, \"vline_ent_ths must have two thresholds (low and high).\"\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"ent_th\"] = df[\"ent_th\"].astype(float)\n",
    "    df = df.sort_values(\"ent_th\").reset_index(drop=True)\n",
    "\n",
    "    x = df[\"ent_th\"].to_numpy()\n",
    "    core_cnt = df[\"core count\"].to_numpy(dtype=float)\n",
    "    high_cnt = df[\"high count\"].to_numpy(dtype=float)\n",
    "    acc1 = df[\"acc1\"].to_numpy(dtype=float)\n",
    "    acc2 = df[\"acc2\"].to_numpy(dtype=float)\n",
    "\n",
    "    total_n = core_cnt + high_cnt\n",
    "    N = float(total_n[0])\n",
    "\n",
    "    def _save(fig, name):\n",
    "        if outdir is None:\n",
    "            return\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        fig.savefig(os.path.join(outdir, name), dpi=300)\n",
    "\n",
    "    def nearest_idx(arr, val):\n",
    "        arr = np.asarray(arr, dtype=float)\n",
    "        return int(np.argmin(np.abs(arr - float(val))))\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.plot(x, core_cnt / N, label=\"core coverage\", linestyle=\"--\", color=\"orange\")\n",
    "    ax1.plot(x, high_cnt / N, label=\"high coverage\", linestyle=\"-.\", color=\"blue\")\n",
    "\n",
    "    ax2.plot(x, acc1, linestyle=\"-\", label=\"acc (core)\", color=\"black\")\n",
    "    ax2.plot(x, acc2, linestyle=\"-\", label=\"acc (high)\", color=\"red\")\n",
    "\n",
    "    ax2.axhline(float(overall_acc), linestyle=\":\")\n",
    "    ax2.annotate(\n",
    "        f\"overall acc\\n{overall_acc:.4f}\",\n",
    "        (x[-1], float(overall_acc)),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(18, 8),\n",
    "        ha=\"left\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "    ax1.set_xlabel(\"Entropy Threshold\")\n",
    "    ax1.set_ylabel(\"Coverage\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax1.set_ylim(0, 1.1)\n",
    "    ax2.set_ylim(0, 1.1)\n",
    "\n",
    "    ax1.grid(False)\n",
    "    ax2.grid(False)\n",
    "    plt.yticks([])\n",
    "\n",
    "    core_loc = nearest_idx(x, vline_ent_ths[0])\n",
    "\n",
    "    ax1.axvline(x[core_loc], linestyle=\"--\", alpha=0.7)\n",
    "    ax1.annotate(\n",
    "        f\"{vline_ent_ths[0]:.3f}\",\n",
    "        (x[core_loc], 0),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(6, -6),\n",
    "        ha=\"left\",\n",
    "        va=\"top\",\n",
    "        fontsize=11,\n",
    "    )\n",
    "\n",
    "    ax1.plot([x[core_loc]], [core_cnt[core_loc] / N], marker=\"o\", linestyle=\"None\", color=\"black\")\n",
    "    ax1.annotate(\n",
    "        f\"{core_cnt[core_loc] / N:.4f}\",\n",
    "        (x[core_loc], core_cnt[core_loc] / N),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(-6, 4),\n",
    "        ha=\"right\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=11,\n",
    "    )\n",
    "    \n",
    "    ax2.plot([x[core_loc]], [acc1[core_loc]], marker=\"o\", linestyle=\"None\", color=\"black\")\n",
    "    ax2.annotate(\n",
    "        f\"{acc1[core_loc]:.4f}\",\n",
    "        (x[core_loc], acc1[core_loc]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(-6, 4),\n",
    "        ha=\"right\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=11,\n",
    "    )\n",
    "\n",
    "    ax1.set_xscale(\"log\")\n",
    "\n",
    "    fig.subplots_adjust(right=0.85, top=0.82)\n",
    "\n",
    "    if show_legend:\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        lines = lines1 + lines2\n",
    "        labels = labels1 + labels2\n",
    "\n",
    "        if legend_outside:\n",
    "            ax1.legend(\n",
    "                lines,\n",
    "                labels,\n",
    "                loc=\"lower center\",\n",
    "                bbox_to_anchor=(0.5, 1.02),\n",
    "                ncol=len(labels),\n",
    "                frameon=True,\n",
    "                borderaxespad=0.0,\n",
    "                handlelength=2.0,\n",
    "                columnspacing=1.2,\n",
    "            )\n",
    "        else:\n",
    "            ax1.legend(lines, labels, loc=\"best\")\n",
    "\n",
    "    _save(fig, f\"{sample_name}_counts_and_acc_dualaxis.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05479ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust this for each figures.\n",
    "# -----\n",
    "LOW_ENT = 0.1\n",
    "HIGH_ENT = 1.0\n",
    "HIGH_TO_LOW_grid = 8\n",
    "LOW_TO_0p04_grid = 6\n",
    "VERY_LOW_PART_grid = 8\n",
    "\n",
    "participating_models_str = 'u1'\n",
    "participating_models = u1\n",
    "\n",
    "legend_outside = False\n",
    "show_legend = False\n",
    "# -----\n",
    "\n",
    "high_ents = np.linspace(1.05, LOW_ENT, HIGH_TO_LOW_grid)\n",
    "low_ents = np.linspace(LOW_ENT, np.log(1.1)/np.log(10), LOW_TO_0p04_grid)\n",
    "very_low_ents = np.linspace(np.log(1.1)/np.log(10), 0.001, VERY_LOW_PART_grid)\n",
    "\n",
    "base_ent_ths = np.unique(np.concatenate([high_ents, low_ents, very_low_ents]))\n",
    "base_ent_ths = np.sort(base_ent_ths)[::-1]\n",
    "\n",
    "\n",
    "per_sample_ent_th_results = {}\n",
    "\n",
    "sample_name = ''\n",
    "head = \"_\".join(sample_name.split(\"_\")[:-1])  # drop \"_val\"\n",
    "vth = ent_th_dict.get(head, None)\n",
    "\n",
    "# add per-sample vth into sweep thresholds\n",
    "if vth is None:\n",
    "    # If no chosen threshold, then it shows 0.0414 as the low threshold\n",
    "    ent_ths = base_ent_ths\n",
    "    low_and_high = [float(round(np.log(1.1)/np.log(10), 4)), HIGH_ENT]\n",
    "else:\n",
    "    ent_ths = np.unique(np.concatenate([base_ent_ths, np.array([vth], dtype=float)]))\n",
    "    ent_ths = np.sort(ent_ths)[::-1]\n",
    "    low_and_high = [float(round(vth, 4)), HIGH_ENT]\n",
    "    \n",
    "rows = []\n",
    "for ent_th in ent_ths:\n",
    "    ent_th = float(round(float(ent_th), 4))\n",
    "    result = eval_entropy_fuzzy(\n",
    "        participating_models=sorted(participating_models.keys()),\n",
    "        sample_name=sample_name,\n",
    "        ent_th=ent_th\n",
    "    )\n",
    "    per_sample_ent_th_results[(sample_name, ent_th)] = result\n",
    "    ens = result\n",
    "    rows.append({\n",
    "        \"ent_th\": ent_th,\n",
    "        \"core count\": ens[\"low_entropy_count\"],\n",
    "        \"acc1\": ens[\"low_entropy_acc\"],\n",
    "        \"high count\": ens[\"high_entropy_count\"],\n",
    "        \"acc2\": ens[\"high_entropy_acc\"],\n",
    "    })\n",
    "    \n",
    "\n",
    "df_sweep = pd.DataFrame(rows)\n",
    "overall_acc = ens[\"overall_acc\"]\n",
    "\n",
    "outdir = f'./results/figures/{sample_name}_{participating_models_str}_val'\n",
    "plot_ent_th_sweep(\n",
    "    df_sweep,\n",
    "    overall_acc=overall_acc,\n",
    "    sample_name=sample_name,\n",
    "    outdir=outdir,\n",
    "    vline_ent_ths=low_and_high,\n",
    "    show_legend=show_legend,\n",
    "    legend_outside=legend_outside\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89109d47",
   "metadata": {},
   "source": [
    "## Experiment 1.\n",
    "\n",
    "Compute ensemble and single model entropies on each white-box attack against themselves with weak/strong configuration. Then evaluate ensemble and single model performance on each white-box attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031fa6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single model entropy stats (clean vs adv) aggregated by family (resnet/vgg)\n",
    "truth = y_true[\"original_test\"].reshape(-1)\n",
    "\n",
    "ADV_DIR = apgd_out\n",
    "\n",
    "# ------\n",
    "ent_th = 0.95\n",
    "# ------\n",
    "for STRONG in [True, False]:\n",
    "    resnet_res, vgg_res = [], []\n",
    "\n",
    "    for mn, m in u0.items():  # u0 baseline models\n",
    "        stem = Path(mn).stem\n",
    "\n",
    "        if STRONG:\n",
    "            p = ADV_DIR / f\"test_apgdce_l2_eps0p7_step0p2_it10_rinit4_from_{stem}.npz\"\n",
    "        else:\n",
    "            p = ADV_DIR / f\"test_apgdce_l2_eps0p5_step0p2_it2_rinit4_from_{stem}.npz\"\n",
    "\n",
    "        X_adv = np.load(p)[\"x_adv\"].astype(np.float32)\n",
    "\n",
    "        # clean\n",
    "        clean_pred = caching.get_pred(mn, \"original_test\")\n",
    "        clean_lab = np.argmax(clean_pred, axis=1)\n",
    "        clean_ent = -np.sum(clean_pred * np.log(clean_pred + 1e-12) / np.log(10), axis=1)\n",
    "\n",
    "        # adv\n",
    "        adv_pred = caching.get_pred(mn, \"U0_APGD_weak_test\" if not STRONG else \"U0_APGD_strong_test\")\n",
    "        adv_lab = np.argmax(adv_pred, axis=1)\n",
    "        adv_ent = -np.sum(adv_pred * np.log(adv_pred + 1e-12) / np.log(10), axis=1)\n",
    "\n",
    "        adv_core = adv_ent <= ent_th\n",
    "        cln_core = clean_ent <= ent_th\n",
    "\n",
    "        adv_cnt = int(adv_core.sum())\n",
    "        cln_cnt = int(cln_core.sum())\n",
    "\n",
    "        adv_cov = adv_cnt / X_adv.shape[0]\n",
    "        cln_cov = cln_cnt / samples[\"original_test\"].shape[0]\n",
    "\n",
    "        adv_acc = float(np.mean(adv_lab[adv_core] == truth[adv_core])) if adv_cnt > 0 else 0.0\n",
    "        cln_acc = float(np.mean(clean_lab[cln_core] == truth[cln_core])) if cln_cnt > 0 else 0.0\n",
    "\n",
    "        row = [adv_cov, cln_cov, adv_acc, cln_acc]\n",
    "        (resnet_res if \"resnet\" in mn else vgg_res).append(row)\n",
    "    print(f\"STRONG = {STRONG}\")\n",
    "    print(\"adv_cov, clean_cov, adv_acc, clean_acc\")\n",
    "\n",
    "    print(\"resnet\")\n",
    "    R = np.array(resnet_res, dtype=float)\n",
    "    print(R.mean(axis=0))\n",
    "    print(R.std(axis=0))\n",
    "\n",
    "    print(\"vgg\")\n",
    "    V = np.array(vgg_res, dtype=float)\n",
    "    print(V.mean(axis=0))\n",
    "    print(V.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0424ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U0, U1, Sps stats\n",
    "\n",
    "def core_stats(ens, X, y, ent_th, sample_name=\"\"):\n",
    "    core_preds, core, _ = ens.core_preds(X, sample_name=sample_name, ent_th=float(ent_th))\n",
    "    acc = float(np.mean(np.argmax(core_preds, axis=-1) == y[core])) if cnt > 0 else 0.0\n",
    "    cov = float(core.mean())\n",
    "    return acc, cov\n",
    "\n",
    "truth = y_true[\"original_test\"].reshape(-1)\n",
    "ent_th = 0.95\n",
    "\n",
    "from source.helpers04note import ProbAverageEnsembleFromProbModels\n",
    "\n",
    "U0 = ProbAverageEnsembleFromProbModels(u0)\n",
    "SPs = ProbAverageEnsembleFromProbModels(sps)\n",
    "U1 = ProbAverageEnsembleFromProbModels(u1)\n",
    "\n",
    "cases = [\n",
    "    (\"U0_clean_test\",   U0,  samples[\"original_test\"]),\n",
    "    (\"U0_weak_test\",    U0,  samples['U0_weak_test']),\n",
    "    (\"U0_strong_test\",  U0,  samples['U0_strong_test']),\n",
    "    (\"SPs_weak_test\",   SPs, samples['SPs_weak_test']),\n",
    "    (\"SPs_strong_test\", SPs, samples['SPs_strong_test']),\n",
    "    (\"U1_weak_test\",    U1,  samples['U1_weak_test']),\n",
    "    (\"U1_strong_test\",  U1,  samples['U1_strong_test']),\n",
    "]\n",
    "\n",
    "for name, ens, X in cases:\n",
    "    acc, cov, cnt = core_stats(ens, X, truth, ent_th, sample_name=name)\n",
    "    print(f\"{name:10s} | core_acc {acc:.4f} | core_cov {cov:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e749c2dc",
   "metadata": {},
   "source": [
    "## Experiment 2 and 3\n",
    "\n",
    "Evaluate Logifold / IMM (if second layer exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b948402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "First_layer = u1.copy()\n",
    "baseline = u0\n",
    "model_dict = u0.copy()\n",
    "specialized_model_dict = sps\n",
    "\n",
    "# baseline = U0, first layer = U1 defined above cell.\n",
    "baseline = ProbAverageEnsembleFromProbModels(baseline)\n",
    "first_layer = ProbAverageEnsembleFromProbModels(First_layer)\n",
    "experiment_description = '''\n",
    "Baseline : Res + VGG\n",
    "first layer : Res\n",
    "Second layer : Res\n",
    "union : clean + weak + strong\n",
    "'''     \n",
    "def log_text(out_txt, text: str):\n",
    "    os.makedirs(os.path.dirname(out_txt), exist_ok=True)\n",
    "    with open(out_txt, \"a\") as f:\n",
    "        f.write(text + \"\\n\")\n",
    "log_text(\"./results/logs/evaluation_results.txt\", f\"New Experiment started\\nexperimental description:\\n{experiment_description}\")\n",
    "from source.utils import specialize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca7289a",
   "metadata": {},
   "source": [
    "### Get second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273aa573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------\n",
    "\n",
    "'''\n",
    "sample name list\n",
    "\n",
    "U0_APGD_weak_{split}\n",
    "U0_APGD_strong_{split}\n",
    "U0_union_{split}\n",
    "original_{split}\n",
    "...\n",
    "U0 can be replaced by U1 or SPs\n",
    "'''\n",
    "sample_name = \"U0_union_test\"\n",
    "# ------\n",
    "sample = samples[sample_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6fd941",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokens = sample_name.split('_')\n",
    "split = tokens[-1]\n",
    "\n",
    "sample_name_head = '_'.join(tokens[:-1])\n",
    "sample_val = sample_name_head + '_val'\n",
    "sample_train = sample_name_head + '_train'\n",
    "sample_test = sample_name_head + '_test'\n",
    "\n",
    "truth = y_true[sample_name]\n",
    "ent_first = caching.get_entropy(sorted(First_layer.keys()), sample_name)\n",
    "ent_th = round(ent_th_dict[sample_name_head],3)\n",
    "str_ent_th = str(ent_th).replace('.', 'p')\n",
    "out_of_core_th = ent_th\n",
    "\n",
    "print(sample_name, end = ' | ')\n",
    "print('ent_th:' , ent_th, end = ' | ')\n",
    "print('----------------------------------------')\n",
    "print('-----------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "## Get Specialized 2nd layer experts\n",
    "## Prepare for OoC samples\n",
    "s = np.concatenate([samples[sample_train], samples[sample_val]], axis=0)\n",
    "t = np.concatenate([y_true[sample_train], y_true[sample_val]], axis=0)\n",
    "new_ent = np.concatenate([caching.get_entropy(sorted(First_layer.keys()), sample_train), caching.get_entropy(sorted(First_layer.keys()), sample_val)], axis = 0)\n",
    "OoC_idx = new_ent>out_of_core_th\n",
    "OoC_s = s.copy()[OoC_idx]\n",
    "OoC_t = t.copy()[OoC_idx]\n",
    "if OoC_t.shape[0] < 0.1*t.shape[0]:\n",
    "    raise ValueError(f'[WARN] out_of_core_th={out_of_core_th:.3e} -> OoC size too small. We do not need the second layer for this sample. OoC size is', OoC_t.shape[0])\n",
    "        \n",
    "OoC_x_train, OoC_x_val, OoC_y_train, OoC_y_val = train_test_split(\n",
    "    OoC_s, OoC_t,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=OoC_t\n",
    ")\n",
    "\n",
    "print(f'For 2nd layer experts, we prepared {OoC_y_train.shape} training samples and {OoC_y_val.shape} validation samples')\n",
    "Second_layer={}\n",
    "\n",
    "adv_sample_name = sample_name_head + '_OoC_E1E2'+'_'+str_ent_th\n",
    "for model_name, m in model_dict.items():\n",
    "    \n",
    "    # ResNet only\n",
    "    if 'resnet' not in model_name:\n",
    "        continue\n",
    "    \n",
    "    model_name = model_name[:-6]\n",
    "    model_name = model_name.split('_')[0] if model_name[0] == 'v' else model_name.split('_')[0] + model_name.split('_')[-1] + 'v'+model_name.split('_')[1][-1]\n",
    "    model_path = Path('./data/specialized_models/') / Path(f\"second_layer/{model_name}_{adv_sample_name}-SP.keras\")\n",
    "    if model_path.exists():\n",
    "        Second_layer[f\"{model_name}_{adv_sample_name}_2nd\"] = load_model(model_path)\n",
    "    else:\n",
    "        info = specialize(\n",
    "                (OoC_x_train, to_categorical(OoC_y_train,10)),\n",
    "                (OoC_x_val, to_categorical(OoC_y_val,10)),\n",
    "                m,\n",
    "                new_model_path=f\"second_layer/{model_name}_{adv_sample_name}-SP\",\n",
    "            verbose = 0)\n",
    "\n",
    "        Second_layer[f\"{model_name}_{adv_sample_name}_2nd\"] = info[1]\n",
    "\n",
    "## Reload baseline models\n",
    "model_dict = {}\n",
    "for f_name in sorted(os.listdir(\"./data/models/\")):\n",
    "    if f_name.endswith('keras'):\n",
    "        m = load_model(f\"./data/models/{f_name}\")\n",
    "        model_dict[f_name] = m\n",
    "        \n",
    "First_layer = {}\n",
    "First_layer.update(model_dict)\n",
    "First_layer.update(specialized_model_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f011c9",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1d2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## baseline results\n",
    "\n",
    "p_baseline = ens_probs(sorted(u0.keys()), sample_name)\n",
    "acc_baseline = np.mean(np.argmax(p_baseline, axis=-1) == truth)\n",
    "\n",
    "## First layer results\n",
    "p_first = ens_probs(sorted(u1.keys()), sample_name)\n",
    "acc_first = np.mean(np.argmax(p_first, axis=-1) == truth)\n",
    "\n",
    "first_core_loc = core_mask(sorted(u1.keys()), sample_name, ent_th)\n",
    "first_core_ent = ent_first[first_core_loc]\n",
    "\n",
    "core_p_first = p_first[first_core_loc]\n",
    "core_truth = truth[first_core_loc]\n",
    "core_acc_first = np.mean(np.argmax(core_p_first, axis=-1) == core_truth)\n",
    "OutCore_acc_first = np.mean(np.argmax(p_first[~first_core_loc], axis=-1) == truth[~first_core_loc])\n",
    "\n",
    "## Wrap all_ens and second_layer_ens\n",
    "All_layer = {}\n",
    "All_layer.update(First_layer)\n",
    "All_layer.update(Second_layer)\n",
    "all_ens = ProbAverageEnsembleFromProbModels(All_layer)\n",
    "second_layer_ens = ProbAverageEnsembleFromProbModels(Second_layer)\n",
    "\n",
    "## All ensemble results\n",
    "p_all = all_ens.probs(sample, sample_name)\n",
    "acc_all = float(np.mean(np.argmax(p_all, axis=-1) == truth))\n",
    "\n",
    "## Second results\n",
    "p_second = second_layer_ens.probs(sample, sample_name)\n",
    "acc_second = float(np.mean(np.argmax(p_second, axis=-1) == truth))\n",
    "\n",
    "p_second_OutCore = p_second[~first_core_loc]\n",
    "acc_second_OutCore = float(np.mean(np.argmax(p_second_OutCore, axis=-1) == truth[~first_core_loc]))\n",
    "\n",
    "preds = []\n",
    "\n",
    "for model_name in Second_layer.keys():\n",
    "    if (cache_root/sample_name/model_name).exists():\n",
    "        pred = caching.get_pred(model_name,sample_name)\n",
    "    else:\n",
    "        pred = Second_layer[model_name].predict(samples[sample_name])\n",
    "        caching.set_pred(model_name, sample_name, pred)\n",
    "    preds.append(pred)\n",
    "preds = np.array(preds)\n",
    "ent_second = utils.cross_entropy(preds, 10)\n",
    "ent_second = ent_second[~first_core_loc]\n",
    "second_core = ent_second<out_of_core_th\n",
    "p_second_core = p_second_OutCore[second_core]\n",
    "acc_second_core_of_OutCore = np.mean(np.argmax(p_second_core, axis=-1) == truth[~first_core_loc][second_core])\n",
    "\n",
    "## Gated results (IMM)\n",
    "acc_gated = np.sum(np.argmax(p_first[first_core_loc], axis=-1) == truth[first_core_loc]) + np.sum(np.argmax(p_second_OutCore, axis=-1) == truth[~first_core_loc]) \n",
    "acc_gated /= sample.shape[0]\n",
    "\n",
    "# See behaviour on AutoAttack samples\n",
    "sample_name_aa = 'AA_std_test'\n",
    "sample_aa = samples[sample_name_aa]\n",
    "truth_aa = y_true[sample_name_aa]\n",
    "\n",
    "aa_acc_baseline = float(np.mean(np.argmax(baseline.probs(sample_aa, sample_name_aa), axis=-1) == truth_aa))\n",
    "\n",
    "\n",
    "aa_p_first_layer = first_layer.probs(sample_aa, sample_name_aa)\n",
    "aa_acc_first = float(np.mean(np.argmax(aa_p_first_layer, axis=-1) == truth_aa))\n",
    "aa_first_ent = caching.get_entropy(sorted(First_layer.keys()), sample_name_aa)\n",
    "aa_first_core_loc = aa_first_ent<out_of_core_th\n",
    "\n",
    "aa_core_p_first = aa_p_first_layer[aa_first_core_loc]\n",
    "aa_core_truth = truth_aa[aa_first_core_loc]\n",
    "aa_core_acc_first = np.mean(np.argmax(aa_core_p_first, axis=-1) == aa_core_truth)\n",
    "\n",
    "aa_p_second = second_layer_ens.probs(sample_aa, sample_name_aa)\n",
    "aa_acc_second = float(np.mean(np.argmax(aa_p_second, axis=-1) == truth_aa))\n",
    "\n",
    "aa_p_second_OutCore = aa_p_second[~aa_first_core_loc]\n",
    "aa_acc_second_OutCore = float(np.mean(np.argmax(aa_p_second_OutCore, axis=-1) == truth_aa[~aa_first_core_loc])) \n",
    "preds = []\n",
    "for model_name in Second_layer.keys():\n",
    "    if (cache_root/sample_name_aa/model_name).exists():\n",
    "        pred = caching.get_pred(model_name,sample_name_aa)\n",
    "    else:\n",
    "        pred = Second_layer[model_name].predict(samples[sample_name_aa])\n",
    "        caching.set_pred(model_name, sample_name_aa, pred)\n",
    "    preds.append(pred)\n",
    "preds = np.array(preds)\n",
    "aa_ent_second = utils.cross_entropy(preds, 10)\n",
    "aa_ent_second = aa_ent_second[~aa_first_core_loc]\n",
    "aa_second_core = aa_ent_second<out_of_core_th\n",
    "aa_p_second_core = aa_p_second_OutCore[aa_second_core]\n",
    "aa_acc_second_core_of_OutCore = np.mean(np.argmax(aa_p_second_core, axis=-1) == truth_aa[~aa_first_core_loc][aa_second_core])\n",
    "\n",
    "aa_acc_gated = np.sum(np.argmax(aa_p_first_layer[aa_first_core_loc], axis=-1) == truth_aa[aa_first_core_loc]) + np.sum(np.argmax(aa_p_second_OutCore, axis=-1) == truth_aa[~aa_first_core_loc]) \n",
    "aa_acc_gated /= sample_aa.shape[0]\n",
    "\n",
    "aa_acc_all = float(np.mean(np.argmax(all_ens.probs(sample_aa, sample_name_aa), axis=-1) == truth_aa))\n",
    "\n",
    "\n",
    "txt = \"\\n\".join([\n",
    "    \"----------------------------------------\",\n",
    "    f\"Sample name : {sample_name} (size : {sample.shape[0]}), out_of_core_th : {out_of_core_th}\",\n",
    "    f\"First layer acc : {acc_first}\",\n",
    "    f\"Baseline acc : {acc_baseline}\",\n",
    "    f\"Second layer acc : {acc_second}\",\n",
    "    f\"All members ensemble acc : {acc_all}\",\n",
    "    f\"Entropy threshold for first layer : {out_of_core_th}\",\n",
    "    f\"First layer core acc : {core_acc_first} (size : {np.sum(first_core_loc)})\",\n",
    "    f\"First layer OutCore acc : {OutCore_acc_first} (size : {np.sum(~first_core_loc)})\",\n",
    "    f\"First layer core entropy : {np.sum(first_core_ent)}\",\n",
    "    f\"First layer entropy : {np.sum(ent_first)}\",\n",
    "    f\"First layer OutCore entropy : {np.sum(ent_first[~first_core_loc])}\",\n",
    "    f\"First layer core coverage : {np.sum(first_core_loc) / sample.shape[0]}\",\n",
    "    f\"Second layer acc on OutCore from first layer  : {acc_second_OutCore} (size : {np.sum(~first_core_loc)})\",\n",
    "    f\"Second layer core acc on OutCore from first layer : {acc_second_core_of_OutCore} (size : {np.sum(second_core)})\",\n",
    "    f\"Second layer core entropy on OutCore from first layer : {np.sum(ent_second[second_core])}\",\n",
    "    f\"Second layer entropy on OutCore from first layer : {np.sum(ent_second)}\",\n",
    "    f\"out core of second layer count : {np.sum(~second_core)}\",\n",
    "    f\"Second layer OutCore entropy on OutCore from first layer : {np.sum(ent_second[~second_core])}\",\n",
    "    f\"Second layer core coverage on OutCore from first layer : {np.sum(second_core) / ent_second.shape[0]}\",\n",
    "    f\"Gated ensemble acc : {acc_gated}\",\n",
    "    f\"total entropy = first layer core ent + second layer ent on OutCore from first layer\\n{np.sum(first_core_ent)} + {np.sum(ent_second)} = {np.sum(first_core_ent)+np.sum(ent_second)}\",\n",
    "    f\"total core entropy (first layer) + core entropy (second layer) : {np.sum(first_core_ent) + np.sum(ent_second[second_core])}\",\n",
    "    f\"maximum acc among baseline, first layer, second layer : {max(acc_baseline, acc_first, acc_second)}\",\n",
    "    f\"difference between gated ensemble and maximum acc : {acc_gated - max(acc_baseline, acc_first, acc_second)}\",\n",
    "    \"\",\n",
    "    \"AutoAttack result\",\n",
    "    f\"AA Sample name : {sample_name_aa} (size : {sample_aa.shape[0]}), out_of_core_th : {out_of_core_th}\",\n",
    "    f\"First layer acc : {aa_acc_first}\",\n",
    "    f\"Baseline acc : {aa_acc_baseline}\",\n",
    "    f\"Second layer acc : {aa_acc_second}\",\n",
    "    f\"All members ensemble acc : {aa_acc_all}\",\n",
    "    f\"Entropy threshold for first layer : {out_of_core_th}\",\n",
    "    f\"First layer core acc : {aa_core_acc_first} (size : {np.sum(aa_first_core_loc)})\",\n",
    "    f\"Second layer acc on OutCore from first layer  : {aa_acc_second_OutCore} (size : {np.sum(~aa_first_core_loc)})\",\n",
    "    f\"Second layer core acc on OutCore from first layer : {aa_acc_second_core_of_OutCore} (size : {np.sum(aa_second_core)})\",\n",
    "    f\"Gated ensemble  acc : {aa_acc_gated}\",\n",
    "    f\"First layer entropy : {np.sum(aa_first_ent)}\",\n",
    "    f\"First layer core entropy : {np.sum(aa_first_ent[aa_first_core_loc])}\",\n",
    "    f\"Total entropy = first layer core entropy + 2nd layer ent on outcore : {np.sum(aa_first_ent[aa_first_core_loc])} + {np.sum(aa_ent_second)}\"\n",
    "])\n",
    "\n",
    "log_text(\"./results/logs/evaluation_results.txt\", txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66edc615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('----------------------------------------')\n",
    "print('----------------------------------------')\n",
    "print(f\"Sample name : {sample_name} (size : {sample.shape[0]}), out_of_core_th : {out_of_core_th}\")\n",
    "print(f\"First layer acc : {acc_first}\")\n",
    "print(f\"Baseline acc : {acc_baseline}\")\n",
    "print(f\"Second layer acc : {acc_second}\")\n",
    "print(f\"All members ensemble acc : {acc_all}\")\n",
    "print(f\"Entropy threshold for first layer : {out_of_core_th}\")\n",
    "print(f\"First layer core acc : {core_acc_first} (size : {np.sum(first_core_loc)})\")\n",
    "print(f\"First layer OutCore acc : {OutCore_acc_first} (size : {np.sum(~first_core_loc)})\")\n",
    "print(f\"First layer core entropy : {np.sum(first_core_ent)}\")\n",
    "print(f\"First layer entropy : {np.sum(ent_first)}\")\n",
    "print(f\"First layer OutCore entropy : {np.sum(ent_first[~first_core_loc])}\")\n",
    "print(f\"First layer core coverage : {np.sum(first_core_loc) / sample.shape[0]}\")\n",
    "print(f\"Second layer acc on OutCore from first layer  : {acc_second_OutCore} (size : {np.sum(~first_core_loc)})\")\n",
    "print(f\"Second layer core acc on OutCore from first layer : {acc_second_core_of_OutCore} (size : {np.sum(second_core)})\")\n",
    "print(f\"Second layer core entropy on OutCore from first layer : {np.sum(ent_second[second_core])}\")\n",
    "print(f\"Second layer entropy on OutCore from first layer : {np.sum(ent_second)}\")\n",
    "print(f\"Second layer OutCore entropy on OutCore from first layer : {np.sum(ent_second[~second_core])}\")\n",
    "print(f\"Second layer core coverage on OutCore from first layer : {np.sum(second_core) / ent_second.shape[0]}\")\n",
    "print(f\"out core of second layer count : {np.sum(~second_core)}\")\n",
    "print(f\"Gated ensemble acc : {acc_gated}\")\n",
    "print(f\"total entropy = first layer core ent + second layer ent on OutCore from first layer\\n{np.sum(first_core_ent)} + {np.sum(ent_second)} = {np.sum(first_core_ent)+np.sum(ent_second)}\")\n",
    "print(\"total core entropy (first layer) + core entropy (second layer) : \", np.sum(first_core_ent) + np.sum(ent_second[second_core]))\n",
    "print(f\"maximum acc among baseline, first layer, second layer : {max(acc_baseline, acc_first, acc_second)}\")\n",
    "print(f\"difference between gated ensemble and maximum acc : {acc_gated - max(acc_baseline, acc_first, acc_second)}\")\n",
    "\n",
    "print()\n",
    "print('AutoAttack result')\n",
    "print(f\"AA Sample name : {sample_name_aa} (size : {sample_aa.shape[0]}), out_of_core_th : {out_of_core_th}\")\n",
    "print(f\"First layer acc : {aa_acc_first}\")\n",
    "print(f\"Baseline acc : {aa_acc_baseline}\")\n",
    "print(f\"Second layer acc : {aa_acc_second}\")\n",
    "print(f\"All members ensemble acc : {aa_acc_all}\")\n",
    "print(f\"Entropy threshold for first layer : {out_of_core_th}\")\n",
    "print(f\"First layer core acc : {aa_core_acc_first} (size : {np.sum(aa_first_core_loc)})\")\n",
    "print(f\"Second layer acc on OutCore from first layer  : {aa_acc_second_OutCore} (size : {np.sum(~aa_first_core_loc)})\")\n",
    "print(f\"Second layer core acc on OutCore from first layer : {aa_acc_second_core_of_OutCore} (size : {np.sum(aa_second_core)})\")\n",
    "print(f\"Gated ensemble  acc : {aa_acc_gated}\")\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
