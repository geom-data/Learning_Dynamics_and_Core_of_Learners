# '25. January Experiment note


# Experiment1.

Aim : to detect the pollution rate.

Since filter has high accuracy rate, it may measure how much given dataset is polluted.

1. Choose a model from the two filters in previous experiments: :  
specialized from 10 targets and specialized from 20 targets.

2. Create datasets with ratios of 1:0, 1:1, 2:1, 3:1, 4:1, and 10:1 to verify if the ratio is accurately measured.
The original portion is always set as the whole, while adversaries are chosen randomly according to the specified ratio.

--> PGD Filters distinguish PGD adversaries well, but perform poorly in distinguishing CWL2 adversaries.

## Experiment2 design... if experiment 1 works
Since filter doesn't work well, models based on PGD adversaries will not work with CWL2 adversaries.
~~Aim : See if it still works well against CWL2~~

~~1. With the same logifold in the previous experiment, try to test with CWL2. We can try with (original + PGD attack) validation dataset and (original + CWL2) validation dataset.~~

# Experiment 3

Aim : to see if having adversarial models improve the performance.

(Full and fine only) 0.8155 -> (experts specialized from full and fine) 0.9052 

Full and fine + experts specialized from full and fine -> 0.8983

Base -> Experts specialized from base : 0.8772

Base + Full and Fine : 0.8787

Base : 0.8785

# Conclusion and future experiments

Our adversarial examples are generated by ResNet56v2, and we obtained full and fine model trained over original + adversarial dataset whose pollution rate is 1:1. Having specialized onto original / adversarial and turned into filter from the full and fine model, we can achieve pollution rate detection ability and better performance in classifying given instance.

However, the structure generating adversarial examples and classifying model (simply I'll say attacking model and defending model) are the same. Even if I change the number of parameters, still both structures are similar in essence.

Eventhough attacking model and defending models are the same, if method used to generate the adversarial example differs from PGD on which the full and fine model is trained, then it has no ability to detect the pollution rate and classifiying given instance. Shortly, it fails to defend other type of adversarial attack.

We can do followings:

1. Keep the same structure in both attack & defense, and differ the 'direction' of adversaries. For instance, it was 'untargeted' adversaries. So we may try 'adversarial target to least likely class or 2nd likely class'.

2. We can do migration the full and fine model onto CWL2 adversarials.

3. Or, since having full and fine model would be costly, we can specialize and migrate base model onto adversarial + original.

4. Now, keep the type and direction of adversarial attack, but choose different attacking model structure from defending model structure. It may include Simple CNN structure and ... what else?