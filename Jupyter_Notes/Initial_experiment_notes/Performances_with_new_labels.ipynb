{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ee81730",
   "metadata": {},
   "source": [
    "### note\n",
    "\n",
    "In this note, we will investigate whether specialization on finer labels improve single model and logifold performances. We expect that finer labels doesn't helpful to single model performances, but may improve logifold performance. \n",
    "\n",
    "- Get finer labels\n",
    "- Specialize with the finer labels each committee member. For comparison, we specialize with original label each committee member.\n",
    "\n",
    "Above steps are done in the previous note:[`Make_New_label_using_adv_attacks.ipynb`](./Make_New_label_using_adv_attacks.ipynb).\n",
    "\n",
    "-------\n",
    "\n",
    "Then \n",
    "1. compare baseline models and specialized model with new labels. The specialized models answer in $0,\\ldots, 19$, hence we subtract 10 from answers greater or equal to 10. \n",
    "2. compare baseline logifold and specialized models. For the latter, we may need 'filter' and 'experts'. For now, we simply put them together without filter nor experts.\n",
    "- And get filter and experts from the Committee. Filter distinguish if it's 'dangerous' or not. Experts will be tuned on the 'dangerous' samples. Since training sample has very little dangerous Original and CWL2 samples (about one or two), we train them over the 'largely' perturbed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213002e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries and set experiments.\n",
    "\n",
    "# load necessary libraries\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras import Model\n",
    "from scipy.stats import entropy\n",
    "import math\n",
    "loaded_models = {}\n",
    "# load additional models from another folder\n",
    "# assuming these models are also saved in Keras format with '.keras' extension\n",
    "# Say 'SB-type', where 'SB' stands for 'Specialized Base'.\n",
    "folder = 'CIFAR10models/more_tunned/'\n",
    "pattern = os.path.join(folder, '*_more_specialized*.keras')\n",
    "file_list = sorted(glob.glob(pattern))\n",
    "loaded_models.update({os.path.basename(f): load_model(f) for f in file_list})\n",
    "\n",
    "label_dict = {(3,1) : 'ResNet20v1',\n",
    "        (3,2) : 'ResNet20v2',\n",
    "        (9,1): 'ResNet56v1',\n",
    "        (9,2): 'ResNet56v2'}\n",
    "\n",
    "(x, y), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_v, y_train, y_v = train_test_split(x, y, test_size=0.2, \n",
    "                                              random_state=42) # random state has been always 42.\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_val = x_v.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train_categorical_10 = to_categorical(y_train,10)\n",
    "y_val_categorical_10 = to_categorical(y_v,10)\n",
    "y_test_categorical_10 = to_categorical(y_test,10)\n",
    "y_test_categorical_20 = to_categorical(y_test,20)\n",
    "\n",
    "# Set the predetermined random seed\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "# Define ranges\n",
    "range1 = [range(8), range(4), range(4), range(4)]\n",
    "range2 = [range(8), range(4), [0], range(4)]\n",
    "\n",
    "# Sample one tuple from each\n",
    "tuple1 = tuple(rng.choice(r) for r in range1)\n",
    "\n",
    "print(\"Comm 1:\", tuple1)\n",
    "comm_SB_1 = []\n",
    "for file_name, model in loaded_models.items():\n",
    "    # Extract model name from the file name\n",
    "    base = file_name.replace('.keras','')\n",
    "    parts = base.split('_')\n",
    "    if parts[0] == 'n':\n",
    "        model_name = label_dict[(int(parts[1]),int(parts[2][-1]))] + '_B_'+parts[-1][-1]\n",
    "    else:\n",
    "        model_name = parts[0] + '_SB_' + parts[-1]\n",
    "    # print(f\"Model: {model_name}\")\n",
    "    parts = model_name.split('_')\n",
    "    if parts[0] == 'ResNet20v1':\n",
    "        if parts[1] == 'SB':\n",
    "            if int(parts[-1]) == tuple1[0]:\n",
    "                comm_SB_1.append((model_name,model))\n",
    "    if parts[0] == 'ResNet20v2':\n",
    "        if parts[1] == 'SB':\n",
    "            if int(parts[-1]) == tuple1[1]:\n",
    "                comm_SB_1.append((model_name,model))\n",
    "    if parts[0] == 'ResNet56v1':\n",
    "        if parts[1] == 'SB':\n",
    "            if int(parts[-1]) == tuple1[2]:\n",
    "                comm_SB_1.append((model_name,model))\n",
    "    if parts[0] == 'ResNet56v2':\n",
    "        if parts[1] == 'SB':\n",
    "            if int(parts[-1]) == tuple1[3]:\n",
    "                comm_SB_1.append((model_name,model))\n",
    "                \n",
    "print('comm_SB_1', comm_SB_1)\n",
    "\n",
    "\n",
    "# As SB model shows better performance on the certain part, let use SB type models to analyze the behavior on the certain part against adversarial examples.\n",
    "\n",
    "# Load the adversarial examples\n",
    "\n",
    "## set all files.\n",
    "## Possibly there is no merged testing dataset, or validation dataset.\n",
    "## We only look for testing dataset.\n",
    "import re\n",
    "\n",
    "pattern1 = re.compile(r'_\\d+to\\d+_')\n",
    "pattern2 = re.compile(r'test')\n",
    "\n",
    "folder = './adversarial_examples/gen_by_ResNet'\n",
    "all_files = os.listdir(folder)\n",
    "\n",
    "filtered_files = [\n",
    "    fname for fname in all_files\n",
    "    if fname.endswith('.npy') and not pattern1.search(fname) and not pattern2.search(fname)\n",
    "]\n",
    "\n",
    "\n",
    "## load all the training and validation dataset\n",
    "\n",
    "loaded_training_dataset = {}\n",
    "loaded_validation_dataset = {}\n",
    "import re\n",
    "for f in filtered_files:\n",
    "    \n",
    "    base = f.replace('.keras.npy','').replace('.npy','')\n",
    "    parts = base.split('_')\n",
    "    '''\n",
    "    cwl2_x_tr_untargeted\n",
    "    cwl2_x_v_untargeted.npy\n",
    "    pgd_0.376_x_target_to_ll.npy\n",
    "    pgd_0.376_x_target_to_second.npy\n",
    "    pgd_0.376_x_untarget.npy\n",
    "    pgd_0.376_x_val_target_to_ll.npy\n",
    "    pgd_0.376_x_val_target_to_second.npy\n",
    "    pgd_0.376_x_val_untarget.npy\n",
    "    '''\n",
    "    \n",
    "    attack_type = parts[0]\n",
    "    if attack_type == 'cwl2':\n",
    "        direction = 'untargeted'\n",
    "        if parts[2] == 'tr':\n",
    "            train_or_val = 'train'\n",
    "        else:\n",
    "            train_or_val = 'val'\n",
    "    else:\n",
    "        attack_type = 'PGD by ResNet56v1'\n",
    "        direction = parts[-1]\n",
    "        if direction == 'll':\n",
    "            direction = 'targeted to least'\n",
    "        elif direction =='second':\n",
    "            direction = 'targeted to 2nd'\n",
    "        else:\n",
    "            direction = 'untargeted'\n",
    "        if parts[3] == 'val':\n",
    "            train_or_val = 'val'\n",
    "        else:\n",
    "            train_or_val = 'train'\n",
    "    key = (attack_type, direction, train_or_val)\n",
    "    if train_or_val == 'train':\n",
    "        loaded_training_dataset[key] = np.load(os.path.join(folder, f))\n",
    "    else:\n",
    "        loaded_validation_dataset[key] = np.load(os.path.join(folder, f))\n",
    "\n",
    "print('-'*50, 'keys for adv samples', '-'*50)\n",
    "for k in loaded_training_dataset.keys():\n",
    "    print(k)\n",
    "    \n",
    "for k in loaded_validation_dataset.keys():\n",
    "    print(k)\n",
    "    \n",
    "    \n",
    "def get_entropy_array(committee : list[tuple[str,Model]], sample : np.ndarray): \n",
    "    '''\n",
    "    param committee: list of tuple (member, model)\n",
    "    param sample: numpy array of (n,) shape\n",
    "    \n",
    "    return tuple of dict, numpy array of (n,) shape\n",
    "    dictionary : committee member -> list of [answer to the sample, certainty to the sample]\n",
    "    '''\n",
    "    comm_pred = {}\n",
    "    for member, model in committee:\n",
    "        p = model.predict(sample,verbose = 0)\n",
    "        comm_pred[member] = [\n",
    "                                np.argmax(p,axis=-1),\n",
    "                                np.max(p,axis=-1)\n",
    "                                ]\n",
    "    stacked = np.stack([member_pred[0] for member_pred in comm_pred.values()], axis=1)\n",
    "    counts = np.apply_along_axis(lambda x: np.bincount(x, minlength=10), axis=1, arr=stacked)\n",
    "    probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "    disagreements = entropy(probs, axis=1, base = 10)\n",
    "    \n",
    "    return disagreements\n",
    "\n",
    "committee = comm_SB_1\n",
    "\n",
    "## CWL2 train and val\n",
    "cwl2_train = loaded_training_dataset[('cwl2', 'untargeted', 'train')]\n",
    "cwl2_val = loaded_validation_dataset[('cwl2', 'untargeted', 'val')]\n",
    "entropy_cwl2_train = np.load('./entropies/entropy_SB1_cwl2_untargeted_train.npy')\n",
    "entropy_cwl2_val = np.load('./entropies/entropy_SB1_cwl2_untargeted_val.npy')\n",
    "cwl2_new_label_train = np.load('./new_labels/SB1/new_label_cwl2_untargeted_train.npy')\n",
    "cwl2_new_label_val= np.load('./new_labels/SB1/new_label_cwl2_untargeted_val.npy')\n",
    "cwl2_new_label_test= np.load('./new_labels/SB1/new_label_cwl2_untargeted_test.npy')\n",
    "## PGD untargeted train and val\n",
    "PGD_train = loaded_training_dataset[('PGD by ResNet56v1', 'untargeted', 'train')]\n",
    "PGD_val = loaded_validation_dataset[('PGD by ResNet56v1', 'untargeted', 'val')]\n",
    "entropy_PGD_untargeted_train = np.load('./entropies/entropy_SB1_PGD by ResNet56v1_untargeted_train.npy')\n",
    "entropy_PGD_untargeted_val = np.load('./entropies/entropy_SB1_PGD by ResNet56v1_untargeted_val.npy')\n",
    "PGD_untargeted_new_label_train = np.load('./new_labels/SB1/new_label_PGD_untargeted_train.npy')\n",
    "PGD_untargeted_new_label_val = np.load('./new_labels/SB1/new_label_PGD_untargeted_val.npy')\n",
    "PGD_untargeted_new_label_test = np.load('./new_labels/SB1/new_label_PGD_untargeted_test.npy')\n",
    "## PGD targeted train and val\n",
    "PGD_targeted_train = loaded_training_dataset[('PGD by ResNet56v1', 'targeted to least', 'train')]\n",
    "PGD_targeted_val = loaded_validation_dataset[('PGD by ResNet56v1', 'targeted to least', 'val')]\n",
    "entropy_PGD_targeted_train = np.load('./entropies/entropy_SB1_PGD by ResNet56v1_targeted to least_train.npy')\n",
    "entropy_PGD_targeted_val = np.load('./entropies/entropy_SB1_PGD by ResNet56v1_targeted to least_val.npy')\n",
    "PGD_targeted_new_label_train = np.load('./new_labels/SB1/new_label_PGD_targeted_train.npy')\n",
    "PGD_targeted_new_label_val = np.load('./new_labels/SB1/new_label_PGD_targeted_val.npy')\n",
    "PGD_targeted_new_label_test = np.load('./new_labels/SB1/new_label_PGD_targeted_test.npy')\n",
    "\n",
    "def committee_answers(committee, sample, entropy, mask = None):\n",
    "    if mask is None:\n",
    "        # Create a mask for non-zero entropy values\n",
    "        mask = entropy != 0\n",
    "\n",
    "    # Prepare data for the table\n",
    "    table_data = {\n",
    "        'Original Label': y_train[:sample.shape[0]][mask].reshape(-1),\n",
    "        'Entropy': np.round(entropy[mask], 3)\n",
    "    }\n",
    "\n",
    "    # Loop through the committee to get predictions\n",
    "    for member, model in committee:\n",
    "        # Predictions on CWL2 adversarial data\n",
    "        p  = model.predict(sample[mask], verbose=0)\n",
    "        table_data[f'{member[-1]}p'] = np.argmax(p , axis=-1)\n",
    "        table_data[f'{member[-1]}c'] = np.round(np.max(p , axis=-1), 3)\n",
    "\n",
    "    # Create and display the DataFrame\n",
    "    df = pd.DataFrame(table_data)\n",
    "    return df\n",
    "\n",
    "def make_new_label(y_sample :np.ndarray, \n",
    "                   entropy_array : np.ndarray, \n",
    "                   entropy_thresholds : list[float] | float = 0.4, \n",
    "                   num_classes : int = 10):\n",
    "    size = entropy_array.shape[0]\n",
    "    y_sample_copy = y_sample.copy()[:size]\n",
    "    new_label_cnt = 1\n",
    "    if isinstance(entropy_thresholds,float):\n",
    "        mask = entropy_array > entropy_thresholds\n",
    "        y_sample_copy[mask] = y_sample_copy[mask]+num_classes*new_label_cnt\n",
    "        return y_sample_copy, new_label_cnt\n",
    "    elif len(entropy_thresholds) == 1:\n",
    "        mask = entropy_array > entropy_thresholds[0]\n",
    "        y_sample_copy[mask] = y_sample_copy[mask]+num_classes*new_label_cnt\n",
    "        return y_sample_copy, new_label_cnt\n",
    "    else:\n",
    "        prev_th = entropy_thresholds[0]\n",
    "        for curr_th in entropy_thresholds[1:]:\n",
    "            mask = np.prod([entropy_array>prev_th,entropy_array<=curr_th],\n",
    "                     axis=0)==1\n",
    "            y_sample_copy[mask] = y_sample_copy[mask]+num_classes*new_label_cnt\n",
    "            new_label_cnt += 1\n",
    "        return y_sample_copy, new_label_cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adecaa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_models = {}\n",
    "folder = './new_labels/specialists/SB1/'\n",
    "pattern = os.path.join(folder, '*_baseline.keras')\n",
    "file_list = sorted(glob.glob(pattern))\n",
    "baseline_models.update({os.path.basename(f): load_model(f) for f in file_list})\n",
    "\n",
    "newly_labeled_models = {}\n",
    "folder = './new_labels/specialists/SB1/'\n",
    "pattern = os.path.join(folder, '*_new_label.keras')\n",
    "file_list = sorted(glob.glob(pattern))\n",
    "newly_labeled_models.update({os.path.basename(f): load_model(f) for f in file_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb5dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwl2_test = np.load('./adversarial_examples/gen_by_ResNet/cwl2_x_test_untargeted.npy')\n",
    "PGD_untargeted_test = np.load('./adversarial_examples/gen_by_ResNet/pgd_0.376_x_test_untarget_gen_by_n_9_v1_cifar10.keras.npy')\n",
    "PGD_targeted_test = np.load('./adversarial_examples/gen_by_ResNet/pgd_0.376_x_test_target_to_ll_gen_by_n_9_v1_cifar10.keras.npy')\n",
    "\n",
    "table_data = {\n",
    "        'Testing dataset': ['Original','CWL2','PGD untargeted','PGD targeted'],\n",
    "    '20v1_Baseline':[0,0,0,0],\n",
    "    '20v1_new_label':[0,0,0,0],\n",
    "    '20v2_Baseline':[0,0,0,0],\n",
    "    '20v2_new_label':[0,0,0,0],\n",
    "    '56v1_Baseline':[0,0,0,0],\n",
    "    '56v1_new_label':[0,0,0,0],\n",
    "    '56v2_Baseline':[0,0,0,0],\n",
    "    '56v2_new_label':[0,0,0,0],\n",
    "    }\n",
    "\n",
    "for model_name, baseline_model in baseline_models.items():\n",
    "    base = model_name.replace('.keras','')\n",
    "    parts = base.split('_')\n",
    "    parts.pop()\n",
    "    adv_type = parts[-1]\n",
    "    table_values = [0,0,0,0]\n",
    "    if adv_type == 'cwl2':\n",
    "        testing_x = cwl2_test\n",
    "        idx = 1\n",
    "    elif adv_type == 'untargeted':\n",
    "        adv_type = 'PGD untargeted'\n",
    "        idx = 2\n",
    "        testing_x = PGD_untargeted_test\n",
    "    elif adv_type == 'targeted':\n",
    "        adv_type = 'PGD targeted'\n",
    "        idx = 3\n",
    "        testing_x = PGD_targeted_test\n",
    "    comparison_model = '_'.join(parts)+'_new_label.keras'\n",
    "#     print(f\"Model: {parts[1]+' '+ adv_type} ---- Baseline ----- Comparison\")\n",
    "    comparison_model = newly_labeled_models.get(comparison_model)\n",
    "    \n",
    "    pred_on_original_base = baseline_model.predict(x_test,verbose=0)\n",
    "    pred_on_original_comparison = comparison_model.predict(x_test,verbose=0)\n",
    "    a_base = np.argmax(pred_on_original_base,axis=-1)\n",
    "    a_comp = np.argmax(pred_on_original_comparison,axis=-1)\n",
    "    a_comp[a_comp>9] = a_comp[a_comp>9]-10\n",
    "    acc_base = np.mean(a_base == y_test.reshape(-1))\n",
    "    acc_comp = np.mean(a_comp == y_test.reshape(-1))\n",
    "#     print(f\"original test set: {acc_base:.4f} , {acc_comp:.4f} \")\n",
    "    table_data[parts[1][-4:]+'_Baseline'][0] = acc_base\n",
    "    table_data[parts[1][-4:]+'_new_label'][0] = acc_comp\n",
    "    pred_on_testing_base = baseline_model.predict(testing_x,verbose=0)\n",
    "    pred_on_testing_comparison = comparison_model.predict(testing_x,verbose=0)\n",
    "    a_base = np.argmax(pred_on_testing_base,axis=-1)\n",
    "    a_comp = np.argmax(pred_on_testing_comparison,axis=-1)\n",
    "    acc_base = np.mean(a_base == y_test.reshape(-1))\n",
    "    a_comp[a_comp>9] = a_comp[a_comp>9]-10\n",
    "    acc_comp = np.mean(a_comp == y_test.reshape(-1))\n",
    "#     print(f\"{adv_type} test set: {acc_base:.4f} , {acc_comp:.4f}\")\n",
    "    table_data[parts[1][-4:]+'_Baseline'][idx] = acc_base\n",
    "    table_data[parts[1][-4:]+'_new_label'][idx] = acc_comp\n",
    "df = pd.DataFrame(table_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e06e85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 22:41:31.500459: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from logifold_modules.logifoldv1_4 import Logifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec767b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_logifold = Logifold(10, name = 'Baseline_',\n",
    "                             x_tr = x_train, y_tr = y_train_categorical_10,\n",
    "                             x_v = x_val, y_v = y_val_categorical_10,\n",
    "                             path = os.path.abspath('../')+'/logifolds/exp_new_labels/baseline/')\n",
    "\n",
    "comparison_logifold = Logifold(20, name = 'comparison_',\n",
    "                             x_tr = x_train, y_tr = y_train_categorical_10,\n",
    "                             x_v = x_val, y_v = y_val_categorical_10,\n",
    "                             path = os.path.abspath('../')+'/logifolds/exp_new_labels/comparison/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182defde",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, baseline_model in baseline_models.items():\n",
    "    # model name : specialist_{member}_{adv_name}_baseline.keras\n",
    "    base = model_name.replace('.keras','')\n",
    "    parts = base.split('_')\n",
    "    parts[-2] = adv_type\n",
    "    if adv_type == 'cwl2':\n",
    "        first_idx = 1\n",
    "    elif adv_type == 'untargeted':\n",
    "        first_idx = 2\n",
    "    elif adv_type == 'targeted':\n",
    "        first_idx = 3\n",
    "    # parts[1] = ResNet##v#\n",
    "    second_idx = int(parts[1][5:7]+ parts[1][-1])\n",
    "    key = (first_idx, second_idx)\n",
    "    # second index: 201, 202, 561, 562\n",
    "    baseline_logifold.add(baseline_model, key)\n",
    "    \n",
    "for model_name, comparison_model in newly_labeled_models.items():\n",
    "    # model name : specialist_{member}_{adv_name}_new_label.keras\n",
    "    base = model_name.replace('.keras','')\n",
    "    parts = base.split('_')\n",
    "    parts[-3] = adv_type\n",
    "    if adv_type == 'cwl2':\n",
    "        first_idx = 1\n",
    "    elif adv_type == 'PGD untargeted':\n",
    "        first_idx = 2\n",
    "    elif adv_type == 'PGD targeted':\n",
    "        first_idx = 3\n",
    "    # parts[1] = ResNet##v#\n",
    "    second_idx = int(parts[1][5:7]+ parts[1][-1])\n",
    "    key = (first_idx, second_idx)\n",
    "    # second index: 201, 202, 561, 562\n",
    "    comparison_logifold.add(comparison_model, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecdabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cwl2_new_label_test\n",
    "PGD_untargeted_new_label_test\n",
    "PGD_targeted_new_label_test\n",
    "'''\n",
    "# new label original test dataset\n",
    "entropy_array = get_entropy_array(committee,x_test)\n",
    "np.save('./new_labels/SB1/new_label_original_test.npy',\n",
    "       make_new_label(y_test, entropy_array)[0]\n",
    "        )\n",
    "\n",
    "original_new_label_test = np.load('./new_labels/SB1/new_label_original_test.npy')\n",
    "testing_datasets_no_new_label = [y_test_categorical_10,\n",
    "                                 to_categorical(cwl2_test,10),\n",
    "                                 to_categorical(PGD_untargeted_test,10),\n",
    "                                 to_categorical(PGD_targeted_test,10)\n",
    "                                ]\n",
    "testing_datasets_new_label = [to_categorical(original_new_label_test,20),\n",
    "                   to_categorical(cwl2_new_label_test,20),\n",
    "                   to_categorical(PGD_untargeted_new_label_test,20),\n",
    "                   to_categorical(PGD_targeted_new_label_test,20)\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac42e53",
   "metadata": {},
   "source": [
    "Computation fuzzy domain follows the target label. For instance, tager labels are 0 - 9, then we sample from original dataset.\n",
    "\n",
    "\n",
    "Compare with separation by method.. (by PGD or Step size) \n",
    "\n",
    "Logifold1 : label by methods\n",
    "\n",
    "Logifold2 : label by entropies\n",
    "\n",
    "Union of original and PGD\n",
    "\n",
    "-> by original and PGD\n",
    "\n",
    "-> by entropy\n",
    "\n",
    "in terms of filter first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b8daaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
