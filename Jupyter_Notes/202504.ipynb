{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test logifoldv5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "Modules are loaded. (local)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import sys, os\n",
    "print(tf.__version__)\n",
    "sys.path.append(\"/Users/inkeejung/Library/CloudStorage/Dropbox-BOSTONUNIVERSITY/Inkee Jung/Inkee Jung’s files/Interpretability/AdvLogifold/computer/modules\")\n",
    "sys.path.append(\"/Users/inkeejung/Library/CloudStorage/Dropbox-BOSTONUNIVERSITY/Inkee Jung/Inkee Jung’s files/Interpretability/AdvLogifold/github/logifold\")\n",
    "import pickle\n",
    "import pandas as pd\n",
    "print('Modules are loaded. (local)')\n",
    "\n",
    "dropbox_path = \"/Users/inkeejung/Library/CloudStorage/Dropbox-BOSTONUNIVERSITY/Inkee Jung/Inkee Jung’s files/Interpretability/AdvLogifold/computer/\"\n",
    "base_models_path = dropbox_path + 'Base_Models/'\n",
    "adversarial_examples_path = dropbox_path + 'Adversarial_Examples/'\n",
    "adversarial_models_path = dropbox_path + 'Adversarial_Models/'\n",
    "\n",
    "test_logifold_path = dropbox_path + 'testfolder/logifold_test' + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 01:27:12.744989: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "dropbox_path = \"/Users/inkeejung/Library/CloudStorage/Dropbox-BOSTONUNIVERSITY/Inkee Jung/Inkee Jung’s files/Interpretability/AdvLogifold/computer/\"\n",
    "base_models_path = dropbox_path + 'Base_Models/'\n",
    "base_model = load_model(base_models_path+'CIFAR10models/ResNet/n_9_v1_cifar10.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "(x, y), (x_test, y_test)= cifar10.load_data()\n",
    "x_tr, x_v, y_tr, y_v = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "x_tr = x_tr.reshape(x_tr.shape[0],32,32,3)\n",
    "x_v = x_v.reshape(x_v.shape[0],32,32,3)\n",
    "x_v = x_v.astype('float32')\n",
    "x_tr = x_tr.astype('float32')\n",
    "x_v /= 255\n",
    "x_tr /= 255\n",
    "x_train, x_val, y_train, y_val = x_tr,x_v,y_tr,y_v\n",
    "x_test = x_test.reshape(x_test.shape[0],32,32,3)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shrink the size of data.\n",
    "small_x_tr, small_y_tr = x_tr[:100] , y_tr[:100]\n",
    "small_x_v, small_y_v = x_v[:100] , y_v[:100]\n",
    "small_y_v = to_categorical(small_y_v,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import Any, Optional\n",
    "import os \n",
    "import numpy as np\n",
    "import pickle, importlib\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_x_tr = np.load(adversarial_examples_path + 'by_ResNet/' + \"pgd_0.376_x_untarget.npy\")\n",
    "adv_x_val = np.load(adversarial_examples_path + 'by_ResNet/' + \"pgd_0.376_x_val_untarget.npy\")\n",
    "adv_x_test = np.load(adversarial_examples_path + 'by_ResNet/'+\"pgd_0.376_x_test_untarget.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, flat_target : set):\n",
    "        '''\n",
    "        In target tree, we can remove or add tree node in logifold.\n",
    "        And each node, which is labeled by flattened target, there is corresponding targets and keys.\n",
    "        \n",
    "        For instance, model A has target [[2],[1,0]] with key (0,) and model B and C have target [[0],[1],[2]] with keys (1,) and (2,).\n",
    "        Their targets are flattened to (0,1,2).\n",
    "        They are stored as [(1,), (0,), (2,)] in self.keys.\n",
    "        \n",
    "        And self.target_to_keys = {\n",
    "            ( (0,1), (2,) ) :  ( (0,), 0.8 ),\n",
    "            ( (0,), (1,), (2,) ) : ( (1,), 0.9 ), ( (2,), 0.7 )\n",
    "        }\n",
    "        , self.targets = {\n",
    "            ( (0,1), (2,) ), ( (0,), (1,), (2,) )\n",
    "        }.\n",
    "        '''\n",
    "        self.flat_target : set = flat_target\n",
    "        \n",
    "        \n",
    "        self.NextNodes : list[TreeNode] = []\n",
    "        self.PrevNodes : list[TreeNode] = []\n",
    "        \n",
    "        # set of targets whose flattening is this flat target\n",
    "        self.targets : set[tuple] = set() \n",
    "        # list of model keys corr. to this flat target. Union of target_to_keys. \n",
    "        self.keys : list[tuple] = [] \n",
    "        # dictionary { (thick) targets  -> model keys }\n",
    "        self.target_to_keys : dict[tuple, list[tuple]] = collections.defaultdict(list) \n",
    "        \n",
    "        \n",
    "        self.refinement = Refinement()\n",
    "        self.enumerated_refinement = tuple()\n",
    "    def append_node(self, nextnode : \"TreeNode\"):\n",
    "        self.NextNodes.append(nextnode)\n",
    "        nextnode.PrevNodes.append(self)\n",
    "        return\n",
    "    def is_leaf(self):\n",
    "        return len(self.NextNodes) == 0\n",
    "    def is_root(self):\n",
    "        return len(self.PrevNodes) == 0\n",
    "    def __node__(self):\n",
    "        return f'Node : {self.flat_target}'\n",
    "    \n",
    "    def _show_tree_from_this_node_(self, level=0):\n",
    "        print(\"  \" * level + repr(self.flat_target))\n",
    "        for nextnode in self.NextNodes:\n",
    "            nextnode._show_tree_from_this_node_(level + 1)\n",
    "    \n",
    "    def _update_refinement_(self):\n",
    "        '''\n",
    "        At each node, \"targets\" contains distict T_1, ... T_n sharing the same flattening, which is updated as new model added.\n",
    "        However, we update the information of refinment only when it is needed. \n",
    "        Therefore the information 'targets' may differ from the up-to-date 'targets'.\n",
    "        It could have removed one, or newly added one.\n",
    "        '''\n",
    "        \n",
    "        # If up-to-date targets are merely bigger than before, it's easy to update its refinement.\n",
    "        if self.refinement.targets.issubset(self.targets):\n",
    "            difference = self.targets - self.refinement.targets\n",
    "            self.refinement.update_refinement(difference)\n",
    "        # Otherwise, we should check each refinement without the removed one.\n",
    "        else:\n",
    "            self.refinement = Refinement()\n",
    "            self.refinement.update_refinement(self.targets)\n",
    "                \n",
    "            \n",
    "             \n",
    "class TargetTree:\n",
    "    def __init__(self):\n",
    "        self.nodes: list[TreeNode] = []\n",
    "        self.flat_target_to_node : dict[tuple,TreeNode] = {}\n",
    "        \n",
    "    def add_node(self,\n",
    "                 thick_targets : list,\n",
    "                 flattened_target : tuple[tuple[int,],...],\n",
    "                 model_key : tuple,\n",
    "                 root : bool = False):\n",
    "        '''\n",
    "        thick_targets (list[list[int]]) : (Thick) Targets of model. It is supposed to be ordered.\n",
    "        E.g., [[3],[2,1]] should be converted into [[1,2],[3]]. It'll be stored as ((1,2),(3,))\n",
    "        flattened_target (tuple[tuple[int]]) : Each flattened target is of the form 1-length tuple, (t,).\n",
    "        E.g., ( (1,), (2,), (3,) ). flatTuple(target) provides this format.\n",
    "        model_key(tuple) : Model key.\n",
    "        root (Optional[bool]) : False by default. True if you add a root node.\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Assuming thick targets is sorted in a unique way, we need to conver it to hashable one.\n",
    "        thick_targets_as_a_tuple = tuple(sorted(tuple(sorted(t)) for t in thick_targets))\n",
    "        \n",
    "        flattened_target_as_a_set = set(flattened_target)\n",
    "        # if node exists in TargetTree\n",
    "        if flattened_target in self.flat_target_to_node:\n",
    "            node = self.flat_target_to_node[flattened_target]\n",
    "            \n",
    "            node.keys.append(model_key)\n",
    "            \n",
    "            node.targets.add(thick_targets_as_a_tuple)\n",
    "            \n",
    "            node.target_to_keys[thick_targets_as_a_tuple].append(model_key)\n",
    "        # if node is new\n",
    "        else:\n",
    "            node = TreeNode(flattened_target_as_a_set)\n",
    "            self.flat_target_to_node[flattened_target] = node\n",
    "            node.keys.append(model_key)\n",
    "            node.targets.add(thick_targets_as_a_tuple)\n",
    "            node.target_to_keys[thick_targets_as_a_tuple].append(model_key)\n",
    "            for other_node in self.nodes:\n",
    "                if flattened_target_as_a_set.issubset(other_node.flat_target):\n",
    "                    other_node.append_node(node)\n",
    "                if other_node.flat_target.issubset(flattened_target_as_a_set):\n",
    "                    node.append_node(other_node)\n",
    "            if root:\n",
    "                self.nodes.insert(0,node)\n",
    "            else:\n",
    "                self.nodes.append(node)\n",
    "            \n",
    "        \n",
    "        \n",
    "    def remove_node(self,thick_targets : list, flattened_target : tuple, model_key : tuple):\n",
    "        '''\n",
    "        thick_targets (list[list[int]]) : (Thick) Targets of model. It is supposed to be ordered.\n",
    "        E.g., [[3],[2,1]] should be converted into [[1,2],[3]].\n",
    "        flattened_target (tuple[tuple[int]]) : Each flattened target is of the form 1-length tuple, (t,).\n",
    "        E.g., ( (1,), (2,), (3,) ). flatTuple(target) provides this format.\n",
    "        model_key (tuple) : corresponding model key you'd like to remove from the tree.\n",
    "        '''\n",
    "        thick_targets_as_a_tuple = tuple(tuple(t) for t in thick_targets)\n",
    "        print(f'Deleting {model_key} from the Target Tree...')\n",
    "        if flattened_target in self.flat_target_to_node:\n",
    "            node = self.flat_target_to_node[flattened_target]\n",
    "            print(f'Corresponding {node.__node__()}', end = ' ')\n",
    "            for key_and_accuracy in node.keys:\n",
    "                if key_and_accuracy[0] == model_key:\n",
    "                    node.keys.remove(key_and_accuracy)\n",
    "                    node.target_to_keys[thick_targets_as_a_tuple].remove(key_and_accuracy)\n",
    "                    print(f'{model_key} has been removed.')\n",
    "                break\n",
    "            if not node.target_to_keys[thick_targets_as_a_tuple]:\n",
    "                node.targets.remove(thick_targets_as_a_tuple)\n",
    "                print(f'{thick_targets} has been deleted. Because there is no other models corresponding to {thick_targets}')\n",
    "            if not node.keys:\n",
    "                self.flat_target_to_node.pop(flattened_target)\n",
    "                prev_nodes = node.PrevNodes\n",
    "                for prev_node in prev_nodes:\n",
    "                    prev_node.NextNodes.remove(node)\n",
    "                next_nodes = node.NextNodes\n",
    "                for next_node in next_nodes:\n",
    "                    next_node.PrevNodes.remove(node)\n",
    "                print(f'{node.__node__()} is removed from tree because there is no corresponding model.')\n",
    "        else:\n",
    "            print(f\"node corresponding to {flattened_target} doesn't exist in Target Tree.\")\n",
    "    def _print_tree_(self):\n",
    "        if not self.nodes:\n",
    "            print(\"No nodes\")\n",
    "        else:\n",
    "            root = self.nodes[0]\n",
    "            root._show_tree_from_this_node_()\n",
    "\n",
    "class Refinement:\n",
    "    \"\"\"\n",
    "    A class to maintain and update target refinements.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Enumerated targetes, [T_0, T_1, ...]\n",
    "        self.enumerated_targets : list[tuple] = []\n",
    "        \n",
    "        # In order to check if it's up-to-date. It'll be compared with treenode.targetes\n",
    "        self.targets : set[tuple] = set()\n",
    "        \n",
    "        self.combinations : list[tuple[int,...]] = []\n",
    "        \n",
    "        # mapping (valid) combinations to the corresponding refinement.\n",
    "        self.valid_refinements : dict[tuple[int,...],tuple] = collections.defaultdict(tuple)\n",
    "        # self.max_comb = max_combo\n",
    "    \n",
    "    def update_refinement(self,set_of_thick_targets : set[tuple]):\n",
    "        \"\"\"\n",
    "    Update the refinement data given a set of newly added thick targets.\n",
    "        \"\"\"\n",
    "        for thick_target in set_of_thick_targets:\n",
    "            # thick_target is supposed to be new.\n",
    "            self.enumerated_targets.append(thick_target)\n",
    "            self.targets.add(thick_target)\n",
    "            n = len(thick_target)\n",
    "            \n",
    "            if not self.combinations:\n",
    "                new_combinations = [(i,) for i in range(n)]\n",
    "                new_refinements = collections.defaultdict(tuple)\n",
    "                for i in range(n):\n",
    "                    new_refinements[(i,)] = (thick_target[i])\n",
    "            else:\n",
    "                new_combinations = []\n",
    "                new_refinements = collections.defaultdict(tuple)\n",
    "            while self.combinations:\n",
    "                old_combination = self.combinations.pop()\n",
    "                refinement = self.valid_refinements[old_combination]\n",
    "                for i in range(n):\n",
    "                    new_combinations.append(old_combination + (i,))\n",
    "                    if refinement:\n",
    "                        new_refinement = tuple(\n",
    "                            sorted(tuple(\n",
    "                            set.intersection(\n",
    "                                set(refinement),set(thick_target[i])\n",
    "                                )\n",
    "                            ))\n",
    "                            )\n",
    "                        if new_refinement:\n",
    "                            new_refinements[old_combination+(i,)] = new_refinement\n",
    "            \n",
    "            combo_count = len(new_combinations)\n",
    "            # if combo_count > self.max_comb:\n",
    "            #     raise RuntimeError(\n",
    "            #         f\"[Safety Break] Too many combinations ({combo_count}). \"\n",
    "            #         f\"Max allowed is {self.max_comb}.\"\n",
    "            #     )\n",
    "            self.combinations = new_combinations\n",
    "            self.valid_refinements = new_refinements\n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatTuple(target,ascending=True) -> tuple[tuple[int,...]]:\n",
    "    \n",
    "    out = ()\n",
    "    for a in target:\n",
    "        out+=tuple(sorted(a))\n",
    "    if ascending:\n",
    "        out = tuple(sorted(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4, 5)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatTuple([[3,1,2],[5],[4]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1, 0.2, 0.3]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [0.1,0.3,0.2]\n",
    "\n",
    "test.sort()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 in [0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "targetTree = TargetTree()\n",
    "x = small_x_v\n",
    "model_keys = []\n",
    "epsilon = 1e-3\n",
    "fuzdoms = collections.defaultdict(dict)\n",
    "models = {}\n",
    "raw_predictions : dict[tuple,np.ndarray]= collections.defaultdict(lambda : np.array([],dtype=float))\n",
    "certain_parts : dict[tuple,dict[float,np.ndarray]]= collections.defaultdict(dict)\n",
    "combined_answers : dict[TreeNode,dict[float,np.ndarray]] = collections.defaultdict(dict)\n",
    "combined_answers_along_a_path : dict[tuple[TreeNode,...], dict[float,np.ndarray]] = {}\n",
    "\n",
    "use_history = False\n",
    "optimal_paths = []\n",
    "optimal_thresholds = []\n",
    "for node in targetTree.nodes:\n",
    "    node._update_refinement_()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_paths : list[list[list[TreeNode]]]= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0, 0.5,0.7310585786300049,0.8807970779778823,0.9525741268224334,0.9820137900379085,0.9933071490757153,0.9975273768433653,0.9990889488055994,0.9996646498695336,0.9998766054240137]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: -inf,\n",
       " 0.5: 0.0,\n",
       " 0.7310585786300049: 1.0,\n",
       " 0.8807970779778823: 2.0,\n",
       " 0.9525741268224334: 3.0,\n",
       " 0.9820137900379085: 4.0,\n",
       " 0.9933071490757153: 5.0,\n",
       " 0.9975273768433653: 6.0,\n",
       " 0.9990889488055994: 7.0,\n",
       " 0.9996646498695336: 8.0,\n",
       " 0.9998766054240137: 9.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inverse_sigmoid(x):\n",
    "    x = round(math.log(x**(-1)-1),2)\n",
    "    if x == -float('inf'):\n",
    "        return x\n",
    "    else:\n",
    "        x = abs(x)\n",
    "        return x\n",
    "ths_for_fuzdom = {0 : -float('inf')}\n",
    "for th in thresholds[1:]:\n",
    "    ths_for_fuzdom[th] = inverse_sigmoid(th)\n",
    "ths_for_fuzdom"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import one model  : Base ResNet56v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "charts: dict[tuple, dict[str, Any]] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = load_model(base_models_path+'CIFAR10models/ResNet/n_9_v1_cifar10.keras')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Label For Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newLabelForTarget(x : np.ndarray , y: np.ndarray, target :list[list[int]],\n",
    "    ):\n",
    "    new_out = len(target)\n",
    "    base = np.argmax(y, -1)\n",
    "    out = np.full(base.size, -1, dtype=int)\n",
    "    for new_lbl, t in enumerate(target):\n",
    "        out[np.isin(base, t)] = new_lbl\n",
    "    \n",
    "    relabel = out\n",
    "    kept_y = relabel != -1\n",
    "    x, new_y = x[kept_y], relabel[kept_y]\n",
    "    return x, new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y =  newLabelForTarget(small_x_tr, small_y_tr, target = [[0,1,2],[3,4],[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 2, 2, 1, 2, 1, 2, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 2, 1, 1, 0, 0, 1, 2, 0, 2, 0, 1,\n",
       "       2, 2, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "charts[(0,)] = {\n",
    "    'target': [[i] for i in range(10)],\n",
    "    'active': True,\n",
    "    'filetype': 'keras',\n",
    "    'description': 'ResNet56v1, standard',\n",
    "    'spec for loading model': None\n",
    "}\n",
    "models[(0,)] = base_model\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GetFuzDom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFuzDoms(key):\n",
    "    model : keras.Model= models[key]\n",
    "    fuzDom : dict = {}\n",
    "    print(\"Computing fuzzy domains...\")\n",
    "    x = small_x_v\n",
    "    y = small_y_v\n",
    "    target = charts[key]['target']\n",
    "    x_k, y_k = newLabelForTarget(x, y, target)\n",
    "    predict = model.predict(x_k, batch_size=32, verbose=1)\n",
    "    ans = np.argmax(predict, axis=-1)\n",
    "    certainty = np.max(predict, axis=-1)\n",
    "    threshold = -float('inf')\n",
    "    mark = (ans == y_k)\n",
    "    class_accs = []\n",
    "    ind_cl = [ans == i for i in range(len(target))]\n",
    "    y_cl = [y_k == i for i in range(len(target))]\n",
    "\n",
    "    for i in range(len(target)):\n",
    "        chosen_in_cl = ind_cl[i]\n",
    "        if np.sum(chosen_in_cl) > 0:\n",
    "            marked_class = ans[chosen_in_cl]==y_k[chosen_in_cl] # True positive\n",
    "            class_accs.append(\n",
    "                np.sum(marked_class)/len(marked_class)\n",
    "                )\n",
    "        else:\n",
    "            class_accs.append(float('nan'))\n",
    "    fuzDom[threshold] = (\n",
    "        (np.sum(mark)/len(mark), 1.0),              # coverage is 100% for alpha = -inf\n",
    "        np.array(class_accs),\n",
    "        np.ones(len(target))               \n",
    "    )\n",
    "    threshold = 0.0\n",
    "    total_len = len(y_k)\n",
    "    minValNo = len(y) * 0.01\n",
    "    while True:\n",
    "        req_certainty = sigmoid(threshold)\n",
    "        certain = (certainty > req_certainty)\n",
    "        covered_len = np.sum(certain)\n",
    "        if covered_len <= minValNo:\n",
    "            break\n",
    "\n",
    "        mark_certain = (ans[certain] == y_k[certain])\n",
    "        overall_acc = np.mean(mark_certain)\n",
    "        coverage_rate = covered_len / total_len\n",
    "\n",
    "        class_accs = []\n",
    "        class_coverages = []\n",
    "        for i in range(len(target)):\n",
    "            # coverage for each true class\n",
    "            if np.sum(y_cl[i]) > 0:\n",
    "                class_coverages.append(np.sum(certain & y_cl[i]) / np.sum(y_cl[i]))\n",
    "            else:\n",
    "                class_coverages.append(0.0)\n",
    "            # accuracy for each predicted class\n",
    "            chosen_in_cl = (certain & ind_cl[i])\n",
    "            if np.sum(chosen_in_cl) > 0:\n",
    "                mark_cl = ans[chosen_in_cl] == y_k[chosen_in_cl] # True Positive\n",
    "                \n",
    "                class_accs.append(np.mean(mark_cl))\n",
    "            else:\n",
    "                class_accs.append(float('nan'))\n",
    "\n",
    "        fuzDom[threshold] = (\n",
    "            (overall_acc, coverage_rate),\n",
    "            np.array(class_accs),\n",
    "            np.array(class_coverages)\n",
    "        )\n",
    "        threshold += 0.5\n",
    "    charts[key]['fuzDom'] = fuzDom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing fuzzy domains...\n",
      "4/4 [==============================] - 1s 66ms/step\n"
     ]
    }
   ],
   "source": [
    "getFuzDoms((0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [[i] for i in range(10)]\n",
    "targetTree.add_node(target,\n",
    "                    flatTuple(target),\n",
    "                    (0,),\n",
    "                    root=True\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Fuz dom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_fuz_dom(k, accuracy = True, coverage = True):\n",
    "    model_key = k\n",
    "    data : dict = charts[model_key]['fuzDom']\n",
    "    table_data = {}\n",
    "    for k, v in data.items():\n",
    "        if sigmoid(k) in thresholds:\n",
    "            if accuracy and coverage:\n",
    "                \n",
    "                table_data[(sigmoid(k), 'acc')] = [np.round(v[0][0],3)]+[np.round(r,3) for r in v[1]]\n",
    "                table_data[(sigmoid(k), 'cov')] = [np.round(v[0][1],3)]+[np.round(r,3) for r in v[2]]\n",
    "            elif accuracy and not coverage:\n",
    "                table_data[(sigmoid(k))] = [np.round(v[0][0],3)]+[np.round(r,3) for r in v[1]]\n",
    "            elif not accuracy and coverage:\n",
    "                table_data[(sigmoid(k))] = [np.round(v[0][1],3)]+[np.round(r,3) for r in v[2]]\n",
    "    df = pd.DataFrame(table_data).T\n",
    "    df.columns = ['all']+[f'{t}' for t in charts[model_key]['target']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>[0]</th>\n",
       "      <th>[1]</th>\n",
       "      <th>[2]</th>\n",
       "      <th>[3]</th>\n",
       "      <th>[4]</th>\n",
       "      <th>[5]</th>\n",
       "      <th>[6]</th>\n",
       "      <th>[7]</th>\n",
       "      <th>[8]</th>\n",
       "      <th>[9]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.000000</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.500000</th>\n",
       "      <td>0.97</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.714</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.731059</th>\n",
       "      <td>0.93</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.889</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.880797</th>\n",
       "      <td>0.89</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.778</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.952574</th>\n",
       "      <td>0.86</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.889</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.778</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.982014</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.889</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.993307</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.889</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.997527</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.999089</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.999665</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.999877</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           all    [0]    [1]    [2]    [3]  ...    [5]    [6]    [7]    [8]    [9]\n",
       "0.000000  1.00  1.000  1.000  1.000  1.000  ...  1.000  1.000  1.000  1.000  1.000\n",
       "0.500000  0.97  1.000  1.000  1.000  0.714  ...  1.000  1.000  1.000  1.000  0.875\n",
       "0.731059  0.93  1.000  1.000  1.000  0.714  ...  0.833  0.889  1.000  1.000  0.875\n",
       "0.880797  0.89  1.000  1.000  1.000  0.714  ...  0.667  0.778  1.000  1.000  0.875\n",
       "0.952574  0.86  1.000  0.889  1.000  0.714  ...  0.667  0.778  1.000  0.923  0.875\n",
       "0.982014  0.82  0.933  0.889  1.000  0.714  ...  0.667  0.667  0.778  0.923  0.875\n",
       "0.993307  0.72  0.733  0.889  1.000  0.571  ...  0.333  0.667  0.778  0.923  0.625\n",
       "0.997527  0.66  0.667  0.889  0.714  0.429  ...  0.333  0.667  0.667  0.923  0.625\n",
       "0.999089  0.60  0.533  0.889  0.571  0.429  ...  0.333  0.556  0.556  0.923  0.625\n",
       "0.999665  0.54  0.467  0.667  0.571  0.429  ...  0.250  0.556  0.556  0.923  0.625\n",
       "0.999877  0.49  0.400  0.667  0.429  0.429  ...  0.167  0.444  0.556  0.846  0.625\n",
       "\n",
       "[11 rows x 11 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_fuz_dom((0,), accuracy=False, coverage=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras import Model\n",
    "from keras import layers\n",
    "from keras.layers import (\n",
    "    Dense)\n",
    "# Concatenate, BatchNormalization, Dropout,\n",
    "#     Conv2D, Flatten, LeakyReLU, Activation, add, AveragePooling2D, Input\n",
    "# )\n",
    "\n",
    "\n",
    "def train(\n",
    "    self, key: tuple,\n",
    "    *,\n",
    "    x_tr=None, y_tr=None, x_v=None, y_v=None,\n",
    "    new_key : tuple = (),\n",
    "    active: bool = True,\n",
    "    epochs: int = 50,\n",
    "    lr: float = 1e-3,\n",
    "    batch_size: int = 32,\n",
    "    \n",
    "    \n",
    "    ini_th: float = 0.0,\n",
    "    step: float = 0.5,\n",
    "    min_val_percentage: float = 0.01,\n",
    "    lr_schedule=None,\n",
    "    save_each: bool = False,\n",
    "    new_label: bool = True,\n",
    "    monitor_coverage_level=None,\n",
    "    verbose: int = 0,\n",
    "    description: str | None = None,\n",
    "    autosave: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"Fit key and return a short summary.\"\"\"\n",
    "    if new_key is not None:\n",
    "        save_to_new = True\n",
    "        \n",
    "    model : Model = models[key]\n",
    "    target = charts[key][\"target\"]\n",
    "    filetype = charts[key][\"filetype\"]\n",
    "    model_name = ''.join(f\"_{i:03d}\" for i in key)\n",
    "    if len(target) < num_classes and new_label:\n",
    "        x_tr, y_tr = newLabelForTarget(x_tr, y_tr, target)\n",
    "        x_v, y_v   = newLabelForTarget(x_v, y_v, target)\n",
    "      \n",
    "    initial_description = str()\n",
    "    \n",
    "    if new_key is not None:\n",
    "        charts[new_key]={}\n",
    "        charts[new_key]['fuzDom'] = None\n",
    "        charts[new_key]['target'] = charts[key]['target']\n",
    "        charts[new_key]['active'] = active\n",
    "        charts[new_key]['filetype'] = filetype\n",
    "        \n",
    "        #Since Keras-pretrained model saves like usual model, no spec is needed for loading the saved one.\n",
    "        if charts[key]['spec for loading model'] is not None and charts[key]['spec for loading model']['name'] != 'Keras-pretrained':\n",
    "            charts[new_key]['spec for loading model'] = charts[key]['spec for loading model']\n",
    "        else:\n",
    "            charts[new_key]['spec for loading model'] = None\n",
    "        text = \"Trained from Model %s.\\n\"%(model_name)\n",
    "        if description is not None:\n",
    "            text += description + \"\\n\"\n",
    "            initial_description += description + \"\\n\"\n",
    "        if charts[key]['description'] is not None:           \n",
    "            text += \"Description of Model %s: %s\\n\"%(model_name, \n",
    "                                                    charts[key]['description'])\n",
    "            initial_description += \"Description of Model %s: %s\\n\"%(model_name, \n",
    "                                                    charts[key]['description'])\n",
    "        charts[new_key]['description'] = text\n",
    "        \n",
    "    else:\n",
    "        new_key = key\n",
    "    opt = tf.keras.optimizers.Adam(lr)\n",
    "    metrics = [\"accuracy\"]\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                loss=\"categorical_crossentropy\",\n",
    "                metrics=metrics)\n",
    "\n",
    "    \n",
    "    ckpt_path = ('.'+model_name+'.keras')\n",
    "\n",
    "    # callbacks\n",
    "    monitor = \"val_accuracy\" if monitor_coverage_level is None else \"val_covermet\"\n",
    "    callbacks = [ModelCheckpoint(ckpt_path, monitor=monitor,\n",
    "                                save_best_only=not save_each, verbose=verbose)]\n",
    "    if lr_schedule:\n",
    "        callbacks += [LearningRateScheduler(lr_schedule),\n",
    "                    ReduceLROnPlateau(factor=np.sqrt(0.1), patience=5, min_lr=5e-7)]\n",
    "\n",
    "    # fit \n",
    "    hist = model.fit(x_tr, y_tr, batch_size=batch_size,\n",
    "                    validation_data=(x_v, y_v),\n",
    "                    epochs=epochs, callbacks=callbacks, verbose=verbose)\n",
    "    \n",
    "    \n",
    "    train_summary = str()\n",
    "    getFuzDoms(iniTh=ini_th, step=step, \n",
    "                    minValPercentage=min_val_percentage, keys=[new_key], autosave=autosave)\n",
    "    \n",
    "\n",
    "    # ---------- summary ----------\n",
    "    metric = \"val_accuracy\" if monitor_coverage_level is None else \"val_covermet\"\n",
    "    best = np.max(hist.history[metric])\n",
    "    first = hist.history[metric][0]\n",
    "    return train_summary + f\"best {metric} {best:.3f} (first {first:.3f})\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn specialist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 5, 10, 15, 18 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 18:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 15:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 10:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 5:\n",
    "        lr *= 1e-1\n",
    "#     print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "def turn_specialist(\n",
    "        key: tuple,\n",
    "        new_key: tuple,\n",
    "        new_target: list,\n",
    "        *,\n",
    "        train: bool = True,\n",
    "        x_v: np.ndarray | None = None,\n",
    "        y_v: np.ndarray | None = None,\n",
    "        x_tr: np.ndarray | None = None,\n",
    "        y_tr: np.ndarray | None = None,\n",
    "        epochs: int = 21,\n",
    "        lr: float = 1e-3,\n",
    "        batch_size: int = 128,\n",
    "        lr_schedule : LearningRateScheduler | None =None,\n",
    "        datagen=None,\n",
    "        resample: bool = False,\n",
    "        ini_th: float = 0.0,\n",
    "        step: float = 0.5,\n",
    "        min_val_percentage: float = 0.01,\n",
    "        active: bool = True,\n",
    "        save_each: bool = False,\n",
    "        save_above_acc: float | None = None,\n",
    "        save_bests: int | None = None,\n",
    "        description: str = \"\",\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "        \n",
    "        \n",
    "        chart = charts[key]\n",
    "        model : Model = models[key]\n",
    "        suffix = ''.join(f\"_{i:03d}\" for i in key)\n",
    "        \n",
    "        tag = f\"model{suffix}\"\n",
    "        for w in model.weights:\n",
    "            if tag not in w.name:\n",
    "                w._handle_name = f\"{w.name}_{tag}\"\n",
    "        for layer in model.layers:\n",
    "            if tag not in layer.name:\n",
    "                layer._name = f\"{layer.name}_{tag}\"\n",
    "\n",
    "        # build specialist network\n",
    "        base = Model(inputs = model.inputs, outputs = model.layers[-2].output, name=f\"{new_key}_base\")\n",
    "        x    = keras.Input(shape=base.input_shape[1:], name=f\"{new_key}_in\")\n",
    "        y    = Dense(len(new_target), name=f\"{new_key}_dense\")(base(x))\n",
    "        z    = layers.Softmax(name=f\"{new_key}_softmax\")(y)\n",
    "        specialist = Model(inputs = x, outputs = z)\n",
    "        specialist.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        suffix = ''.join(f\"_{i:03d}\" for i in key)\n",
    "        \n",
    "        specialist.save(f\"model{suffix}.keras\")\n",
    "\n",
    "        charts[new_key] = {\n",
    "            \"target\": new_target,\n",
    "            \"active\": active,\n",
    "            \"fuzDom\": None,\n",
    "            \"filetype\": chart[\"filetype\"],\n",
    "            \"spec for loading model\": None,\n",
    "            \"description\": (\n",
    "                (description or \"\")\n",
    "                if description and verbose\n",
    "                else f\"specialized from {tag}.\"\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # optional\n",
    "        if train:\n",
    "            lr_scheduler = lr_schedule or LearningRateScheduler(lr_schedule)\n",
    "            train_summary = train(\n",
    "                new_key,\n",
    "                x_v = x_v,\n",
    "                y_v = y_v,\n",
    "                x_tr = x_tr,\n",
    "                y_tr= y_tr,\n",
    "                epochs=epochs,\n",
    "                lr=lr,\n",
    "                batch_size=batch_size,\n",
    "                datagen=datagen,\n",
    "                lr_schedule=lr_scheduler,\n",
    "                resample=resample,\n",
    "                iniTh=ini_th,\n",
    "                step=step,\n",
    "                minValPercentage=min_val_percentage,\n",
    "                save_each=save_each,\n",
    "                saveAboveAcc=save_above_acc,\n",
    "                saveBests=save_bests,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "            print(train_summary)\n",
    "        return specialist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function : is fine, is c in, find valid pahts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_fine_(node : TreeNode):\n",
    "    return all([len(t)<=1 for t in node.refinement.valid_refinements.values()])\n",
    "def _is_c_in_(node : TreeNode, t_class : int):\n",
    "    return t_class in node.flat_target\n",
    "    \n",
    "def _find_valid_paths_(t_class : int):\n",
    "    root = targetTree.nodes[0]\n",
    "    path : list[TreeNode]= []\n",
    "    valid_paths : list[list[TreeNode]]= []\n",
    "    def dfs(node : TreeNode):\n",
    "        if _is_c_in_(node,t_class):\n",
    "            path.append(node)\n",
    "            if _is_fine_(node):\n",
    "                valid_paths.append(path.copy())\n",
    "            for next_node in node.NextNodes:\n",
    "                dfs(next_node)\n",
    "            path.pop()\n",
    "    dfs(root)\n",
    "    return valid_paths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function : get raw pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_raw_prediction_(k):\n",
    "    if raw_predictions[k].size>0:\n",
    "        return raw_predictions[k]\n",
    "    else:\n",
    "        print(f'model {k} is thinking...')\n",
    "        model : keras.Model = models[k]\n",
    "        raw_predictions[k] = model.predict(x)\n",
    "        return raw_predictions[k]\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function : find cert part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_certain_part_(k,th):\n",
    "        \"\"\"\n",
    "        Find mask for certain samples for a given certainty threshold.\n",
    "        \n",
    "        \n",
    "        Args:\n",
    "            model_key: The key identifying a model in self.charts.\n",
    "            predictions (np.ndarray): The raw model predictions, shape (N, #classes).\n",
    "            certainty_threshold (float): The certainty threshold for max(g).\n",
    "        Returns:\n",
    "            np.ndarray of bool : True if model is certain on that sample w.r.t. the certainty_threshold\n",
    "        \"\"\"\n",
    "        if raw_predictions[k].size == 0:\n",
    "            p = _get_raw_prediction_(k)\n",
    "        else:\n",
    "            p = raw_predictions[k]\n",
    "        if p.ndim == 2:\n",
    "            certainty = np.max(p, axis=-1)  # shape (N,)\n",
    "        elif p.ndim == 1:\n",
    "            certainty = p\n",
    "        certain_parts[k][th] = certainty > sigmoid(th)\n",
    "        return "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function : rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_rho_(model_keys : list[tuple], thresholds : list[float] = []):\n",
    "        '''\n",
    "        rho indicates if model is certain about the sample given the certainty threshold.\n",
    "        \n",
    "        Time complexity : O(|Sample|*|keys|).\n",
    "        '''\n",
    "        rho : dict[tuple[tuple,float],np.ndarray] = collections.defaultdict(lambda: np.array([], dtype=float))\n",
    "        e = epsilon\n",
    "        for k in model_keys:\n",
    "            for th in thresholds:\n",
    "                if th in certain_parts[k] and certain_parts[k][th].size>0:\n",
    "                    certain_part = certain_parts[k][th]\n",
    "                else:\n",
    "                    _find_certain_part_(k, th)\n",
    "                    certain_part = certain_parts[k][th]\n",
    "                rho[k, th] = np.where(certain_part, 1, e) # shape = (N,)\n",
    "        return rho"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function : phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_phi_(model_keys, thresholds : list[float] = []):\n",
    "        '''\n",
    "        phi refers to the sum of weights by overall accuracy at the certainty threshold, where each weight by accuracy is activated according to whether it's certain or not.\n",
    "        \n",
    "        phi maps threshold to sum of weights(overall accuracies) involving only certain models.\n",
    "        rho maps (model key, threshold) to an array of floating either 1 or epsilon indicating if it's certain for the sample or not.\n",
    "        '''\n",
    "        phi : dict[float, np.ndarray] = collections.defaultdict(lambda: np.array([]))\n",
    "        rho = _compute_rho_(model_keys, thresholds= thresholds)\n",
    "        if not thresholds:\n",
    "            thresholds = thresholds\n",
    "        for th in thresholds:\n",
    "            total_phi = np.sum([rho[model_key,th]*charts[model_key]['fuzDom'][ths_for_fuzdom[th]][0][0] for model_key in model_keys],axis=0)\n",
    "            phi[th] = total_phi # shape = (N,)\n",
    "        return phi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function : weighted preds and psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weighted_preds_and_psi_( \n",
    "            node : TreeNode,\n",
    "            thresholds : list[float] = []):\n",
    "        \n",
    "        weighted_preds : dict[tuple,dict[float,np.ndarray]]= {}\n",
    "        phi : dict[tuple,dict[float, np.ndarray]] = {}\n",
    "        if not thresholds:\n",
    "            thresholds = thresholds\n",
    "            \n",
    "        for unflattened_T in node.targets:\n",
    "            model_keys = node.target_to_keys[unflattened_T]\n",
    "            rho = _compute_rho_(model_keys, thresholds= thresholds)\n",
    "            weighted_preds[unflattened_T] = {}\n",
    "            phi[unflattened_T] = _compute_phi_(model_keys, thresholds= thresholds)\n",
    "            for th in thresholds:\n",
    "                # rho[k,th], phi[th] : (N,)\n",
    "                # raw_predictions[k] : (N, |targets|)\n",
    "                weighted_preds[unflattened_T][th] = np.sum([rho[k,th][:,np.newaxis]*charts[k]['fuzDom'][ths_for_fuzdom[th]][0][0]*raw_predictions[k] for k in model_keys],axis=0)\n",
    "                weighted_preds[unflattened_T][th] = np.divide(weighted_preds[unflattened_T][th],phi[unflattened_T][th][:,np.newaxis])\n",
    "                # shape = (N,|targets|)\n",
    "        \n",
    "        normalizer : dict[float,np.ndarray]= {}\n",
    "        \n",
    "        for th in thresholds:\n",
    "            # psi_temp[thick_target][th] = (N,)\n",
    "            normalizer[th] = np.sum([phi[thick_target][th] for thick_target in node.targets],axis=0)\n",
    "        \n",
    "                 \n",
    "        psi : dict[tuple,dict[float,np.ndarray]]= {}\n",
    "        \n",
    "        for unflattened_T in node.targets:\n",
    "            psi[unflattened_T] = {}\n",
    "            for th in thresholds:  \n",
    "                psi[unflattened_T][th] = np.divide(phi[unflattened_T][th],normalizer[th])\n",
    "                # (N,) shape of floating values\n",
    "        \n",
    "        return weighted_preds, psi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function :  beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_beta_(\n",
    "                       node : TreeNode, \n",
    "                       weighted_preds : dict[tuple,dict[float,np.ndarray]],\n",
    "                       thresholds : list[float] = []):\n",
    "        '''\n",
    "        distribution factor at a node denoted by beta in the paper.\n",
    "        from_thick_t : tuple = index tuple for the enumerated thick target stored in refinement.\n",
    "        Return:\n",
    "        beta maps (a thick target, valid combination) to {threshold -> N shape of floating numbers, which are distribution factor.}\n",
    "        \n",
    "        More precisely, it maps tuple of (thick target = tuple) and combination, which corresponds to the valid refinements contained in the thick target. The mapped value is dictionary mapping threshold to the beta, the distribution factor.\n",
    "        \n",
    "        E.g.\n",
    "        \n",
    "        [1,2,3], [4,5] with prediction values (x,y)\n",
    "        [1,2], [3,4], [5] with prediction values (a,b)\n",
    "        \n",
    "        Refinements : [1,2] ,[3], [4], [5]\n",
    "        Since [1,2,3] \\cap [5] is empty, we need to distribute answer for [1,2,3]\\cap[5] to other valid refinements. For [1,2,3], there are two refinements [1,2] and [3]. And distribution factor is {xa/(xa+xb)} = a/a+b from [1,2,3] to [1,2], and b/a+b from [1,2,3] to [3].\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        beta : dict[\n",
    "            tuple,dict[ # a thick target ->\n",
    "                tuple[int,...],dict[ # a valid combination ->\n",
    "                    float,np.ndarray] # threshold - > distribution value determined by predictions on valid refinements\n",
    "                ]\n",
    "            ] = collections.defaultdict(dict)\n",
    "        \n",
    "        preds_per_valid_c : dict[\n",
    "            tuple[int,...], dict[float, np.ndarray] # a valid combination -> (th -> preds, of shape (N,))\n",
    "            ] = collections.defaultdict(dict)\n",
    "        \n",
    "        c_contained_in_ : dict[tuple,list[tuple[int,...]]] = {}\n",
    "        if not thresholds:\n",
    "            thresholds = thresholds\n",
    "        normalizer = {}\n",
    "        for c in node.refinement.valid_refinements:\n",
    "            for th in thresholds:\n",
    "                preds_per_valid_c[c][th] = np.prod([weighted_preds[node.refinement.enumerated_targets[i]][th][:,j] for i, j in enumerate(c)],axis=0) # shape = (N,)\n",
    "        \n",
    "        for i, unflattened_T in enumerate(node.refinement.enumerated_targets):\n",
    "            for j, t_ij in enumerate(unflattened_T):\n",
    "                if beta[t_ij]:\n",
    "                    continue\n",
    "                else:\n",
    "                    normalizer : dict[float,np.ndarray]= collections.defaultdict(lambda: np.zeros(len(x)))\n",
    "                    c_contained_in_[t_ij] = []\n",
    "                    for c , _ in node.refinement.valid_refinements.items():\n",
    "                        if _ and c[i]==j: # if given valid refinement is contained in t_ij\n",
    "                            c_contained_in_[t_ij].append(c)\n",
    "                            beta[t_ij][c] = {}\n",
    "                            for th in thresholds:\n",
    "                                beta[t_ij][c][th] = preds_per_valid_c[c][th]\n",
    "                                normalizer[th] += preds_per_valid_c[c][th]\n",
    "                    for c in c_contained_in_[t_ij]:\n",
    "                        for th in thresholds:\n",
    "                            beta[t_ij][c][th] = np.divide(beta[t_ij][c][th],normalizer[th])\n",
    "         \n",
    "        return preds_per_valid_c, beta, c_contained_in_\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function : voting at node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting_at_node(node : TreeNode, thresholds : list[float]  = []):\n",
    "        '''\n",
    "        At a node, all appended models have the same flattening and possibly distict thick targets. We combine votes from each thick target, and distribute votes from invalid to valid refinements.\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        weighted_p, psi = _weighted_preds_and_psi_(node, thresholds= thresholds)\n",
    "        preds_per_valid_c , beta , c_contained_in_= _compute_beta_(node, weighted_p,thresholds=thresholds)\n",
    "        n = len(x)\n",
    "        \n",
    "        refined_targets_temp = node.refinement.valid_refinements.values()\n",
    "        refined_targets = []\n",
    "        for _ in refined_targets_temp:\n",
    "            if _:\n",
    "                refined_targets.append(_)\n",
    "        refined_targets = tuple(sorted(refined_targets))\n",
    "        m = len(refined_targets)        \n",
    "        \n",
    "        answer : dict[float,np.ndarray] = collections.defaultdict(lambda : np.zeros((n,m), dtype=float))\n",
    "        \n",
    "        # Time complexity here may matter.\n",
    "        # Suppose the length of targets in flattening of target at a node = K\n",
    "        # Then, at worst, the number of possible combination is O(K^K). In case of 10, it's already more then 10^5.\n",
    "        # Let's focus on more practical case.\n",
    "        # suppose the number of target is 1000.\n",
    "        # If we have 3 types of different 10 chunks of targets, then that results in 100^3.\n",
    "        # So practically we may assume that there will be no drastic variety of choice in targets.\n",
    "        if not thresholds:\n",
    "            thresholds = thresholds\n",
    "        for c in node.refinement.combinations:\n",
    "            refinement = node.refinement.valid_refinements[c]\n",
    "            if refinement:\n",
    "                index = refined_targets.index(refinement)\n",
    "                for th in thresholds:\n",
    "                    answer[th][:,index] += preds_per_valid_c[c][th]\n",
    "            else:\n",
    "                pred_c : dict[float, np.ndarray] = collections.defaultdict(lambda : np.array((len(x),), dtype= float))\n",
    "                for th in thresholds:\n",
    "                    pred_c[th] = np.prod([weighted_p[node.refinement.enumerated_targets[i]][th][:,j] for i, j in enumerate(c)],axis=0)\n",
    "                \n",
    "                for i, j in enumerate(c):\n",
    "                    for valid_c in c_contained_in_[node.refinement.enumerated_targets[i][j]]:\n",
    "                        index = refined_targets.index(node.refinement.valid_refinements[c])\n",
    "                        for th in thresholds:\n",
    "                            answer[th][:,index] += np.prod([pred_c[th],psi[node.refinement.enumerated_targets[i]][th],beta[node.refinement.enumerated_targets[i][j]][valid_c][th]],axis=0)\n",
    "        node.enumerated_refinement = refined_targets\n",
    "        combined_answers[node] = answer\n",
    "        return answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function : voting along path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting_along_a_path(path: list[TreeNode], thresholds : list[float] = []):\n",
    "    \n",
    "    for node in path:\n",
    "        if not combined_answers[node]:\n",
    "            voting_at_node(node, thresholds=thresholds)\n",
    "            \n",
    "    last_node : TreeNode = path[-1]\n",
    "    \n",
    "    enumerated_refinement = [r[0] for r in last_node.enumerated_refinement]\n",
    "    m = len(enumerated_refinement)\n",
    "    answers_along_path : dict[float,np.ndarray]= collections.defaultdict(lambda : np.zeros((len(x),10), dtype=float))\n",
    "    \n",
    "    index_seq_along_path : list[list] = []\n",
    "    for i, c in enumerate(enumerated_refinement):\n",
    "        index_seq_along_path.append(list())\n",
    "        for node in path:\n",
    "            for index, refinement in enumerate(node.enumerated_refinement):\n",
    "                if c in refinement:\n",
    "                    index_seq_along_path[i].append(index)\n",
    "                    break\n",
    "    if not thresholds:\n",
    "        thresholds = thresholds\n",
    "    for i in range(m):\n",
    "        for th in thresholds:\n",
    "            answers_along_path[th][:,enumerated_refinement[i]] = np.prod([combined_answers[node][th][:,index_seq_along_path[i][j]] for j, node in enumerate(path)], axis=0)\n",
    "    path_key = tuple(path)\n",
    "    combined_answers_along_a_path[path_key] = answers_along_path\n",
    "    \n",
    "    return answers_along_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(targetTree.nodes[0].flat_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict_use_history_(paths : list[tuple[TreeNode,...]], thresholds : list[float]):\n",
    "        \n",
    "        x = small_x_v\n",
    "        y = small_y_v\n",
    "        n = len(x)\n",
    "        answer : np.ndarray = np.zeros((n,num_classes), dtype=float)\n",
    "        for c_i in range(num_classes):\n",
    "            path = paths[c_i]\n",
    "            threshold = thresholds[c_i]\n",
    "            answer_temp = []\n",
    "            for node in path:\n",
    "                answer_at_node : np.ndarray = np.zeros((n,), dtype=float)\n",
    "                weighted_p, psi = _weighted_preds_and_psi_(node, thresholds=[threshold])\n",
    "                preds_per_valid_c , beta , _= _compute_beta_(node, weighted_p, thresholds=[threshold])\n",
    "                refined_targets_temp = node.refinement.valid_refinements.items()\n",
    "                # We only look at the refinement containing c_i.\n",
    "                for (i, (key, value)) in enumerate(refined_targets_temp):\n",
    "                    if c_i in value:\n",
    "                        r = value\n",
    "                        comb = key\n",
    "                # We only look at the invalid refinement whose thick components have c_i.\n",
    "                for c in node.refinement.combinations:\n",
    "                    escape = True\n",
    "                    thick_targets_containing_c_i = set()\n",
    "                    for i, j in enumerate(c):\n",
    "                        if c_i in node.refinement.enumerated_targets[i][j]:\n",
    "                            escape = False\n",
    "                            thick_targets_containing_c_i.add((i,j))\n",
    "                    if escape:\n",
    "                        continue\n",
    "                        \n",
    "                    refinement = node.refinement.valid_refinements[c]\n",
    "                    \n",
    "                    if refinement and refinement == r:\n",
    "                        answer_at_node += preds_per_valid_c[c][threshold]\n",
    "                    else: # the current refinement is invalid, but still a thick compoenet composing the refinement contains c_i, which means this invalid combination will be distributed to the refinement containing c_i.\n",
    "                        pred_c = np.prod([weighted_p[node.refinement.enumerated_targets[i]][threshold][:,j] for i, j in enumerate(c)],axis=0)\n",
    "                        \n",
    "                        for (i,j) in thick_targets_containing_c_i:\n",
    "                        \n",
    "                            answer_at_node += np.prod([pred_c,psi[node.refinement.enumerated_targets[i]][threshold],beta[node.refinement.enumerated_targets[i][j]][comb][threshold]],axis=0)\n",
    "                answer_temp.append(answer_at_node)\n",
    "            answer[:,c_i] += np.prod(answer_temp, axis=0)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_pred(use_history : bool = False, thresholds : list[float] = [], optimal_paths = None, optimal_thresholds = None):\n",
    "    thresholds = [0, 0.5,0.7310585786300049,0.8807970779778823,0.9525741268224334,0.9820137900379085,0.9933071490757153,0.9975273768433653,0.9990889488055994,0.9996646498695336,0.9998766054240137] if not thresholds else thresholds\n",
    "    if use_history:\n",
    "        return _predict_use_history_(optimal_paths,optimal_thresholds)\n",
    "    else:\n",
    "        for c in range(num_classes):\n",
    "            paths = _find_valid_paths_(c)\n",
    "            valid_paths.append(paths)\n",
    "            for path in paths:\n",
    "                voting_along_a_path(path,thresholds=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def predict(model_keys, use_history : bool = False, predOutputFile :str = None, compute_expected_accuracy : bool = False, optimal_paths = None, optimal_thresholds = None):\n",
    "        x = small_x_v\n",
    "        y = small_y_v\n",
    "        y_true = np.argmax(y,axis=-1)\n",
    "        # targetTree = TargetTree()\n",
    "        # for k in model_keys:\n",
    "        #     target = charts[k]['target']\n",
    "        #     key = k\n",
    "        #     targetTree.add_node(\n",
    "        #     target,\n",
    "        #     flatTuple(target),\n",
    "        #     key            \n",
    "        # )\n",
    "        # prediction_machine = Prediction_machine(self,x,model_keys,thresholds = thresholds, use_history=use_history)\n",
    "        if use_history:\n",
    "            answer = temp_pred(use_history=use_history, optimal_paths=optimal_paths, optimal_thresholds= optimal_thresholds)\n",
    "            pred_label = np.argmax(answer, axis=-1)  \n",
    "            accuracy = np.mean(pred_label == y_true)\n",
    "            print(f\"Accuracy={accuracy:.4%}\")\n",
    "            np.save('answer_use_history.npy',answer)\n",
    "            return None, None, None, None, None\n",
    "        else:\n",
    "            temp_pred(thresholds=thresholds)\n",
    "            if predOutputFile is None:\n",
    "                predOutputFile = 'predOutputFile.csv'\n",
    "            with open(predOutputFile, \"w\", newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\"TargetClass\", \"PathIndex\", \"PathNodes\", \"Threshold\", \"Accuracy\"])\n",
    "                if y is not None:\n",
    "                    expected_accuracy : list[dict[tuple[list[TreeNode],float],float]] = []\n",
    "                    for c_i, paths_for_ci in enumerate(valid_paths):\n",
    "                        expected_accuracy.append(dict())\n",
    "                        print(f\"\\n=== Target Class {c_i} ===\")\n",
    "                        if not paths_for_ci:\n",
    "                            print(\"  No valid paths found.\")\n",
    "                            writer.writerow([c_i, None, \"No valid paths\", None, None])\n",
    "                            continue\n",
    "\n",
    "                        for idx, path in enumerate(paths_for_ci):\n",
    "                            print(f\"  Path {idx}: {[str(node.flat_target) for node in path]}\")\n",
    "                            \n",
    "                            # If you stored the path as a tuple in your dictionary:\n",
    "                            path_key = tuple(path)\n",
    "                            # (Adjust if needed. For instance, if it's a list in your dictionary, use path as-is.)\n",
    "\n",
    "                            answers_dict = combined_answers_along_a_path.get(path_key, {})\n",
    "                            if not answers_dict:\n",
    "                                print(\"    No predictions recorded for this path.\")\n",
    "                                writer.writerow([c_i, idx+1, str([n.flat_target for n in path]), None, None])\n",
    "                                continue\n",
    "\n",
    "                            # Sort by threshold to present them in ascending order\n",
    "                            for threshold, logits in sorted(answers_dict.items(), key=lambda x: x[0]):\n",
    "                                \n",
    "                                pred_label = np.argmax(logits, axis=-1)\n",
    "                                \n",
    "                                mask = pred_label == c_i\n",
    "                                accuracy = np.sum(y_true[mask] == c_i)/np.sum(mask) if any(mask) else np.nan\n",
    "                                \n",
    "                                writer.writerow([\n",
    "                                c_i, # target class\n",
    "                                idx+1, # path index\n",
    "                                str([n.flat_target for n in path]), # path nodes\n",
    "                                f\"{threshold:.4f}\", \n",
    "                                f\"{accuracy:.4%}\"\n",
    "                            ])\n",
    "                                expected_accuracy[c_i][(tuple(path), threshold)] = accuracy\n",
    "                    if compute_expected_accuracy:\n",
    "                        optimal_paths = []\n",
    "                        optimal_thresholds = []\n",
    "                        best_value = []\n",
    "                        for i, d in enumerate(expected_accuracy):\n",
    "                            if not d:  \n",
    "                                print(f\"Expected_accuracy #{i} is empty.\")\n",
    "                                continue\n",
    "\n",
    "                            # 'key=d.get' tells max() to pick the dict key whose value is largest\n",
    "                            optimal_path, th = max(d, key=d.get)\n",
    "                            optimal_paths.append(optimal_path)\n",
    "                            optimal_thresholds.append(th)\n",
    "                            best_value.append(d[(optimal_path, th)])\n",
    "                        return optimal_paths, optimal_thresholds, best_value, raw_predictions, certain_parts\n",
    "                    else:\n",
    "                        return None, None, None, None, None\n",
    "                else:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(0,), (1,), (2,), (3,)])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0.5, 0.7310585786300049, 0.8807970779778823, 0.9525741268224334, 0.9820137900379085, 0.9933071490757153, 0.9975273768433653, 0.9990889488055994, 0.9996646498695336, 0.9998766054240137]\n",
      "[0, 0.5, 0.7310585786300049, 0.8807970779778823, 0.9525741268224334, 0.9820137900379085, 0.9933071490757153, 0.9975273768433653, 0.9990889488055994, 0.9996646498695336, 0.9998766054240137]\n",
      "[0, 0.5, 0.7310585786300049, 0.8807970779778823, 0.9525741268224334, 0.9820137900379085, 0.9933071490757153, 0.9975273768433653, 0.9990889488055994, 0.9996646498695336, 0.9998766054240137]\n",
      "[0, 0.5, 0.7310585786300049, 0.8807970779778823, 0.9525741268224334, 0.9820137900379085, 0.9933071490757153, 0.9975273768433653, 0.9990889488055994, 0.9996646498695336, 0.9998766054240137]\n",
      "[0, 0.5, 0.7310585786300049, 0.8807970779778823, 0.9525741268224334, 0.9820137900379085, 0.9933071490757153, 0.9975273768433653, 0.9990889488055994, 0.9996646498695336, 0.9998766054240137]\n",
      "[0, 0.5, 0.7310585786300049, 0.8807970779778823, 0.9525741268224334, 0.9820137900379085, 0.9933071490757153, 0.9975273768433653, 0.9990889488055994, 0.9996646498695336, 0.9998766054240137]\n",
      "[0, 0.5, 0.7310585786300049, 0.8807970779778823, 0.9525741268224334, 0.9820137900379085, 0.9933071490757153, 0.9975273768433653, 0.9990889488055994, 0.9996646498695336, 0.9998766054240137]\n",
      "[0, 0.5, 0.7310585786300049, 0.8807970779778823, 0.9525741268224334, 0.9820137900379085, 0.9933071490757153, 0.9975273768433653, 0.9990889488055994, 0.9996646498695336, 0.9998766054240137]\n",
      "[0, 0.5, 0.7310585786300049, 0.8807970779778823, 0.9525741268224334, 0.9820137900379085, 0.9933071490757153, 0.9975273768433653, 0.9990889488055994, 0.9996646498695336, 0.9998766054240137]\n",
      "[0, 0.5, 0.7310585786300049, 0.8807970779778823, 0.9525741268224334, 0.9820137900379085, 0.9933071490757153, 0.9975273768433653, 0.9990889488055994, 0.9996646498695336, 0.9998766054240137]\n",
      "\n",
      "=== Target Class 0 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "    Threshold=0.0000, Accuracy=100.0000%\n",
      "    Threshold=0.5000, Accuracy=100.0000%\n",
      "    Threshold=0.7311, Accuracy=100.0000%\n",
      "    Threshold=0.8808, Accuracy=100.0000%\n",
      "    Threshold=0.9526, Accuracy=100.0000%\n",
      "    Threshold=0.9820, Accuracy=100.0000%\n",
      "    Threshold=0.9933, Accuracy=100.0000%\n",
      "    Threshold=0.9975, Accuracy=100.0000%\n",
      "    Threshold=0.9991, Accuracy=100.0000%\n",
      "    Threshold=0.9997, Accuracy=100.0000%\n",
      "    Threshold=0.9999, Accuracy=100.0000%\n",
      "\n",
      "=== Target Class 1 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "    Threshold=0.0000, Accuracy=100.0000%\n",
      "    Threshold=0.5000, Accuracy=100.0000%\n",
      "    Threshold=0.7311, Accuracy=100.0000%\n",
      "    Threshold=0.8808, Accuracy=100.0000%\n",
      "    Threshold=0.9526, Accuracy=100.0000%\n",
      "    Threshold=0.9820, Accuracy=100.0000%\n",
      "    Threshold=0.9933, Accuracy=100.0000%\n",
      "    Threshold=0.9975, Accuracy=100.0000%\n",
      "    Threshold=0.9991, Accuracy=100.0000%\n",
      "    Threshold=0.9997, Accuracy=100.0000%\n",
      "    Threshold=0.9999, Accuracy=100.0000%\n",
      "\n",
      "=== Target Class 2 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "    Threshold=0.0000, Accuracy=100.0000%\n",
      "    Threshold=0.5000, Accuracy=100.0000%\n",
      "    Threshold=0.7311, Accuracy=100.0000%\n",
      "    Threshold=0.8808, Accuracy=100.0000%\n",
      "    Threshold=0.9526, Accuracy=100.0000%\n",
      "    Threshold=0.9820, Accuracy=100.0000%\n",
      "    Threshold=0.9933, Accuracy=100.0000%\n",
      "    Threshold=0.9975, Accuracy=100.0000%\n",
      "    Threshold=0.9991, Accuracy=100.0000%\n",
      "    Threshold=0.9997, Accuracy=100.0000%\n",
      "    Threshold=0.9999, Accuracy=100.0000%\n",
      "\n",
      "=== Target Class 3 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "    Threshold=0.0000, Accuracy=75.0000%\n",
      "    Threshold=0.5000, Accuracy=75.0000%\n",
      "    Threshold=0.7311, Accuracy=75.0000%\n",
      "    Threshold=0.8808, Accuracy=75.0000%\n",
      "    Threshold=0.9526, Accuracy=75.0000%\n",
      "    Threshold=0.9820, Accuracy=75.0000%\n",
      "    Threshold=0.9933, Accuracy=75.0000%\n",
      "    Threshold=0.9975, Accuracy=75.0000%\n",
      "    Threshold=0.9991, Accuracy=75.0000%\n",
      "    Threshold=0.9997, Accuracy=75.0000%\n",
      "    Threshold=0.9999, Accuracy=75.0000%\n",
      "\n",
      "=== Target Class 4 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "    Threshold=0.0000, Accuracy=100.0000%\n",
      "    Threshold=0.5000, Accuracy=100.0000%\n",
      "    Threshold=0.7311, Accuracy=100.0000%\n",
      "    Threshold=0.8808, Accuracy=100.0000%\n",
      "    Threshold=0.9526, Accuracy=100.0000%\n",
      "    Threshold=0.9820, Accuracy=100.0000%\n",
      "    Threshold=0.9933, Accuracy=100.0000%\n",
      "    Threshold=0.9975, Accuracy=100.0000%\n",
      "    Threshold=0.9991, Accuracy=100.0000%\n",
      "    Threshold=0.9997, Accuracy=100.0000%\n",
      "    Threshold=0.9999, Accuracy=100.0000%\n",
      "\n",
      "=== Target Class 5 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "    Threshold=0.0000, Accuracy=90.0000%\n",
      "    Threshold=0.5000, Accuracy=90.0000%\n",
      "    Threshold=0.7311, Accuracy=90.0000%\n",
      "    Threshold=0.8808, Accuracy=90.0000%\n",
      "    Threshold=0.9526, Accuracy=90.0000%\n",
      "    Threshold=0.9820, Accuracy=90.0000%\n",
      "    Threshold=0.9933, Accuracy=90.0000%\n",
      "    Threshold=0.9975, Accuracy=90.0000%\n",
      "    Threshold=0.9991, Accuracy=90.0000%\n",
      "    Threshold=0.9997, Accuracy=90.0000%\n",
      "    Threshold=0.9999, Accuracy=90.0000%\n",
      "\n",
      "=== Target Class 6 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "    Threshold=0.0000, Accuracy=88.8889%\n",
      "    Threshold=0.5000, Accuracy=88.8889%\n",
      "    Threshold=0.7311, Accuracy=88.8889%\n",
      "    Threshold=0.8808, Accuracy=88.8889%\n",
      "    Threshold=0.9526, Accuracy=88.8889%\n",
      "    Threshold=0.9820, Accuracy=88.8889%\n",
      "    Threshold=0.9933, Accuracy=88.8889%\n",
      "    Threshold=0.9975, Accuracy=88.8889%\n",
      "    Threshold=0.9991, Accuracy=88.8889%\n",
      "    Threshold=0.9997, Accuracy=88.8889%\n",
      "    Threshold=0.9999, Accuracy=88.8889%\n",
      "\n",
      "=== Target Class 7 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "    Threshold=0.0000, Accuracy=81.8182%\n",
      "    Threshold=0.5000, Accuracy=81.8182%\n",
      "    Threshold=0.7311, Accuracy=81.8182%\n",
      "    Threshold=0.8808, Accuracy=81.8182%\n",
      "    Threshold=0.9526, Accuracy=81.8182%\n",
      "    Threshold=0.9820, Accuracy=81.8182%\n",
      "    Threshold=0.9933, Accuracy=81.8182%\n",
      "    Threshold=0.9975, Accuracy=81.8182%\n",
      "    Threshold=0.9991, Accuracy=81.8182%\n",
      "    Threshold=0.9997, Accuracy=81.8182%\n",
      "    Threshold=0.9999, Accuracy=81.8182%\n",
      "\n",
      "=== Target Class 8 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "    Threshold=0.0000, Accuracy=86.6667%\n",
      "    Threshold=0.5000, Accuracy=86.6667%\n",
      "    Threshold=0.7311, Accuracy=86.6667%\n",
      "    Threshold=0.8808, Accuracy=86.6667%\n",
      "    Threshold=0.9526, Accuracy=86.6667%\n",
      "    Threshold=0.9820, Accuracy=86.6667%\n",
      "    Threshold=0.9933, Accuracy=86.6667%\n",
      "    Threshold=0.9975, Accuracy=86.6667%\n",
      "    Threshold=0.9991, Accuracy=86.6667%\n",
      "    Threshold=0.9997, Accuracy=86.6667%\n",
      "    Threshold=0.9999, Accuracy=86.6667%\n",
      "\n",
      "=== Target Class 9 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "    Threshold=0.0000, Accuracy=100.0000%\n",
      "    Threshold=0.5000, Accuracy=100.0000%\n",
      "    Threshold=0.7311, Accuracy=100.0000%\n",
      "    Threshold=0.8808, Accuracy=100.0000%\n",
      "    Threshold=0.9526, Accuracy=100.0000%\n",
      "    Threshold=0.9820, Accuracy=100.0000%\n",
      "    Threshold=0.9933, Accuracy=100.0000%\n",
      "    Threshold=0.9975, Accuracy=100.0000%\n",
      "    Threshold=0.9991, Accuracy=100.0000%\n",
      "    Threshold=0.9997, Accuracy=100.0000%\n",
      "    Threshold=0.9999, Accuracy=100.0000%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for node in targetTree.nodes:\n",
    "    node._update_refinement_()\n",
    "optimal_paths, optimal_thresholds, best_value, raw_predictions, certain_parts = predict(model_keys = [(0,)], compute_expected_accuracy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=92.0000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None, None, None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model_keys = [(0,)], use_history=True, optimal_paths=optimal_paths, optimal_thresholds=optimal_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = np.load('answer_use_history.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 74ms/step\n"
     ]
    }
   ],
   "source": [
    "model = models[(0,)]\n",
    "raw_answer = model.predict(small_x_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.argmax(raw_answer,axis=-1) == np.argmax(small_y_v,axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing fuzzy domains...\n",
      "4/4 [==============================] - 0s 25ms/step\n",
      "Computing fuzzy domains...\n",
      "4/4 [==============================] - 0s 43ms/step\n",
      "Computing fuzzy domains...\n",
      "4/4 [==============================] - 1s 134ms/step\n"
     ]
    }
   ],
   "source": [
    "base_model1 = load_model(base_models_path+'CIFAR10models/ResNet/n_3_v1_cifar10.keras')\n",
    "charts[(1,)] = {\n",
    "    'target': [[i] for i in range(10)],\n",
    "    'active': True,\n",
    "    'filetype': 'keras',\n",
    "    'description': 'ResNet20v1, standard',\n",
    "    'spec for loading model': None\n",
    "}\n",
    "models[(1,)] = base_model1\n",
    "getFuzDoms((1,))\n",
    "\n",
    "base_model2 = load_model(base_models_path+'CIFAR10models/ResNet/n_3_v2_cifar10.keras')\n",
    "charts[(2,)] = {\n",
    "    'target': [[i] for i in range(10)],\n",
    "    'active': True,\n",
    "    'filetype': 'keras',\n",
    "    'description': 'ResNet20v2, standard',\n",
    "    'spec for loading model': None\n",
    "}\n",
    "models[(2,)] = base_model2\n",
    "getFuzDoms((2,))\n",
    "\n",
    "base_model3 = load_model(base_models_path+'CIFAR10models/ResNet/n_9_v2_cifar10.keras')\n",
    "charts[(3,)] = {\n",
    "    'target': [[i] for i in range(10)],\n",
    "    'active': True,\n",
    "    'filetype': 'keras',\n",
    "    'description': 'ResNet56v2, standard',\n",
    "    'spec for loading model': None\n",
    "}\n",
    "models[(3,)] = base_model3\n",
    "getFuzDoms((3,))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous logifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_y_tr = to_categorical(small_y_tr,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logifoldv1_4 as logifold\n",
    "lgfd = logifold.Logifold(10,\n",
    "                        name = \"test_\",\n",
    "                        x_tr = small_x_tr,\n",
    "                        y_tr = small_y_tr,\n",
    "                        x_v = small_x_v,\n",
    "                        y_v = small_y_v, \n",
    "                        path = 'test_/',\n",
    "                        storyFile='story_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing fuzzy domains...\n",
      "The fuzzy domains have been computed.\n",
      "Computing fuzzy domains...\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7a32fef400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "The fuzzy domains have been computed.\n",
      "Computing fuzzy domains...\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7a32fef490> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "The fuzzy domains have been computed.\n",
      "Computing fuzzy domains...\n",
      "The fuzzy domains have been computed.\n"
     ]
    }
   ],
   "source": [
    "lgfd.add(models[(0,)],(0,),'keras', target = [[n] for n in range(10)],description = 'Base model, Resnet56v1')\n",
    "lgfd.add(models[(1,)],(1,),'keras', target = [[n] for n in range(10)],description = 'Base model, Resnet20v1')\n",
    "lgfd.add(models[(2,)],(2,),'keras', target = [[n] for n in range(10)],description = 'Base model, Resnet20v2')\n",
    "lgfd.add(models[(3,)],(3,),'keras', target = [[n] for n in range(10)],description = 'Base model, Resnet56v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vote counting...\n"
     ]
    }
   ],
   "source": [
    "answer = lgfd.predict(small_x_v,y=small_y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc by refined vote</th>\n",
       "      <th>acc by refined vote restricted on confident domain</th>\n",
       "      <th>size of certain part</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.000000</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.500000</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.731059</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.880797</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.952574</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.982014</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.989247</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.993307</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.997527</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.999089</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.999665</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.999877</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          acc by refined vote  ...  size of certain part\n",
       "0.000000                 0.94  ...                   100\n",
       "0.500000                 0.94  ...                   100\n",
       "0.731059                 0.94  ...                   100\n",
       "0.880797                 0.94  ...                   100\n",
       "0.952574                 0.94  ...                    99\n",
       "0.982014                 0.94  ...                    93\n",
       "0.993307                 0.94  ...                    91\n",
       "0.997527                 0.94  ...                    91\n",
       "0.999089                 0.94  ...                    91\n",
       "0.999665                 0.94  ...                    91\n",
       "0.999877                 0.94  ...                    91\n",
       "\n",
       "[11 rows x 3 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer[4][-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetTree = TargetTree()\n",
    "for k in [(0,),(1,),(2,),(3,)]:\n",
    "    target = charts[k]['target']\n",
    "    key = k\n",
    "    targetTree.add_node(\n",
    "    target,\n",
    "    flatTuple(target),\n",
    "    key            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,), (1,), (2,), (3,)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetTree.nodes[0].keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Target Class 0 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 1 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 2 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 3 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 4 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 5 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 6 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 7 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 8 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 9 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 10 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 11 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 12 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 13 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 14 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 15 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 16 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 17 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 18 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "\n",
      "=== Target Class 19 ===\n",
      "  Path 0: ['{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}']\n",
      "Accuracy=94.0000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None, None, None)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for node in targetTree.nodes:\n",
    "    node._update_refinement_()\n",
    "optimal_paths, optimal_thresholds, best_value, raw_predictions, certain_parts = predict(model_keys = [(0,),(1,),(2,),(3,)], compute_expected_accuracy=True)\n",
    "predict(model_keys = [(0,),(1,),(2,),(3,)], use_history=True, optimal_paths=optimal_paths, optimal_thresholds=optimal_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing fuzzy domains...\n",
      "The fuzzy domains have been computed.\n",
      "compared to 0.530 in the first epoch.\n",
      "The best validation accuracy was 0.620\n",
      "Computing fuzzy domains...\n",
      "The fuzzy domains have been computed.\n",
      "compared to 0.580 in the first epoch.\n",
      "The best validation accuracy was 0.770\n"
     ]
    }
   ],
   "source": [
    "lgfd.turnSpecialist((0,), (0,0),[[0,1,2,3,4],[5,6,7,8,9]], epochs = 20)\n",
    "lgfd.turnSpecialist((1,), (1,0),[[0,1,2],[3,4,5],[6,7,8,9]], epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vote counting...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc by refined vote</th>\n",
       "      <th>acc by refined vote restricted on confident domain</th>\n",
       "      <th>size of certain part</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.000000</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.500000</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.731059</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.880797</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.952574</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.982014</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.989247</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.993307</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.997527</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.999089</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.999665</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.999877</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          acc by refined vote  ...  size of certain part\n",
       "0.000000                 0.94  ...                   100\n",
       "0.500000                 0.94  ...                   100\n",
       "0.731059                 0.94  ...                   100\n",
       "0.880797                 0.94  ...                   100\n",
       "0.952574                 0.94  ...                    99\n",
       "0.982014                 0.94  ...                    93\n",
       "0.993307                 0.94  ...                    91\n",
       "0.997527                 0.94  ...                    91\n",
       "0.999089                 0.94  ...                    91\n",
       "0.999665                 0.94  ...                    91\n",
       "0.999877                 0.94  ...                    91\n",
       "\n",
       "[11 rows x 3 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = lgfd.predict(small_x_v,y=small_y_v)\n",
    "answer[4][-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing fuzzy domains...\n",
      "4/4 [==============================] - 1s 65ms/step\n"
     ]
    }
   ],
   "source": [
    "specialist_1 = load_model('model_000_000.keras')\n",
    "charts[(0,0)] = {\n",
    "    'target': [[0,1,2,3,4],[5,6,7,8,9]],\n",
    "    'active': True,\n",
    "    'filetype': 'keras',\n",
    "    'description': 'ResNet20v1, standard',\n",
    "    'spec for loading model': None\n",
    "}\n",
    "models[(0,0)] = specialist_1\n",
    "getFuzDoms((0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing fuzzy domains...\n",
      "4/4 [==============================] - 0s 96ms/step\n"
     ]
    }
   ],
   "source": [
    "specialist_2 = load_model('model_001_000.keras')\n",
    "charts[(1,0)] = {\n",
    "    'target': [[0,1,2],[3,4,5],[6,7,8,9]],\n",
    "    'active': True,\n",
    "    'filetype': 'keras',\n",
    "    'description': 'ResNet20v1, standard',\n",
    "    'spec for loading model': None\n",
    "}\n",
    "models[(1,0)] = specialist_1\n",
    "getFuzDoms((1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(dropbox_path+'model_000_000_001.keras')\n",
    "charts[(0,0,1)] = {\n",
    "    'target': [[n for n in range(10)],[n for n in range(10,20)]],\n",
    "    'active': True,\n",
    "    'filetype': 'keras',\n",
    "    'description': 'ResNet56v1, trained on adv+original, and specialized to filter',\n",
    "    'spec for loading model': None\n",
    "}\n",
    "models[(0,0,1)] = model\n",
    "target = charts[(0,0,1)]['target']\n",
    "targetTree.add_node(target,\n",
    "                    flatTuple(target),\n",
    "                    (0,0,1),\n",
    "                    root=True\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetTree = TargetTree()\n",
    "for k in [(0,),(1,),(2,),(3,),(0,0),(1,0)]:\n",
    "    target = charts[k]['target']\n",
    "    key = k\n",
    "    targetTree.add_node(\n",
    "    target,\n",
    "    flatTuple(target),\n",
    "    key            \n",
    ")\n",
    "for node in targetTree.nodes:\n",
    "    node._update_refinement_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,), (1,), (2,), (3,), (0, 0), (1, 0)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetTree.nodes[0].keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(dropbox_path+'model_000_001_000.keras')\n",
    "charts[(0,1)] = {\n",
    "    'target': [[n] for n in range(10,20)],\n",
    "    'active': True,\n",
    "    'filetype': 'keras',\n",
    "    'description': 'ResNet56v1, trained on adv+original, and specialized to filter',\n",
    "    'spec for loading model': None\n",
    "}\n",
    "models[(0,1)] = model\n",
    "target = charts[(0,1)]['target']\n",
    "targetTree.add_node(target,\n",
    "                    flatTuple(target),\n",
    "                    (0,1),\n",
    "                    root=False\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_x = np.load('answer_use_history.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    0.    0.    0.003 0.026 0.223 0.    0.751 0.    0.   ] 0.7513570868465549 7\n",
      "[0.    0.    0.    0.008 0.    0.029 0.    0.    0.963 0.   ] 0.9634229550758998 8\n",
      "[0.983 0.    0.001 0.011 0.    0.002 0.    0.    0.002 0.   ] 0.9833228925863903 0\n",
      "[0.    0.    0.    0.    0.001 0.    0.94  0.    0.059 0.   ] 0.93963573343224 6\n",
      "[0.    0.993 0.    0.001 0.    0.    0.    0.    0.005 0.   ] 0.9934552679459254 1\n",
      "[0.    0.    0.    0.    0.    0.    0.998 0.    0.001 0.   ] 0.9977194190424633 6\n",
      "[0.    0.    0.    0.002 0.    0.    0.    0.    0.997 0.   ] 0.9974107762177785 8\n",
      "[0.955 0.    0.    0.    0.    0.    0.    0.    0.045 0.   ] 0.9546907991170883 0\n",
      "[0.009 0.    0.016 0.016 0.004 0.    0.956 0.    0.    0.   ] 0.9559777388970058 6\n",
      "[0.037 0.016 0.017 0.241 0.011 0.457 0.192 0.001 0.027 0.001] 0.45664549246430397 5\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(np.round(answer_x[i],3), max(answer_x[i]), np.argmax(answer_x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(dropbox_path+'model_001_000_000.keras')\n",
    "charts[(1,)] = {\n",
    "    'target': [[n] for n in range(10)],\n",
    "    'active': True,\n",
    "    'filetype': 'keras',\n",
    "    'description': 'ResNet56v1, trained on adv+original, and specialized to filter',\n",
    "    'spec for loading model': None\n",
    "}\n",
    "models[(1,)] = model\n",
    "target = charts[(1,)]['target']\n",
    "targetTree.add_node(target,\n",
    "                    flatTuple(target),\n",
    "                    (1,),\n",
    "                    root=False\n",
    "                    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
