# Notes on Adversarial Experiments

This directory contains four experiment notes, each comparing **Logifold performance** against a **simple ensemble baseline** under various adversarial settings.  

---

##  Adversarial Datasets

We generated adversarial samples using several attack methods. Below are the configurations:

### 1. **Projected Gradient Descent (PGD)**  
Generated by **ResNet56v1** with the following parameters:  

- **Epsilon:** $\frac{216}{255}$  
- **Step size :** $\frac{96}{255}$  
- **Iterations (nb\_iter):** 8  
- **Norm:** $L_2$  
- **Variants:**  
  - Untargeted attack  
  - Targeted to **2nd highest logit**  
  - Targeted to **least logit**  

We also experiment with the standard PGD call:  

```python
pgd.projected_gradient_descent(
    model,
    sample,
    eps=0.3,
    eps_iter=0.01,
    nb_iter=40,
    norm=2,
    clip_max=1.0,
    clip_min=0.0
)
```

---

### 2. **Carlini & Wagner $L_2$ (CWL2)**  
We used the **default CleverHans implementation** with parameters:

```python
def __init__(
    self,
    model_fn,
    y=None,
    targeted=False,
    batch_size=128,
    clip_min=0.0,
    clip_max=1.0,
    binary_search_steps=5,
    max_iterations=1000,
    abort_early=True,
    confidence=0.0,
    initial_const=1e-2,
    learning_rate=5e-3,
):
```

---

### 3. **Fast Gradient Method (FGM)**  
- **Epsilon :** $\frac{216}{255}$
- **Norm:** $L_2$

---

### 4. **DeepFool**  
Parameters:  
- **clip\_min = 0.0**  
- **clip\_max = 1.0**  
- **max\_iter = 50**  
- **overshoot = 0.02**  

DeepFool iteratively decreases the fuzziness difference $f_i - f_j$.  

### Summary

| Attack Method | Model   | Variants |
|---------------|---------|----------|
| **CWL2**      | ResNet  | Targeted to 2nd <br>Targeted to least <br> Untargeted (train / val / test) |
| **PGD (big step)** | ResNet | Targeted to 2nd <br> Targeted to least <br> Untargeted |
|               | VGG     | Untargeted (train / val / test)<br> Targeted to least (train / val / test) |
| **PGD (standard)** <br>*(not yet generated)* | ResNet | Targeted to 2nd <br> Targeted to least <br> Untargeted<br> |
<br>*(not yet generated)* | VGG | Targeted to 2nd <br> Targeted to least <br> Untargeted<br> |
| **FGM (big eps)** | ResNet |  Targeted to 2nd <br> Targeted to least <br> Untargeted |
| **DeepFool**  | ResNet  | (processing) |

##  Experiment Setup

### Dataset
We use the **CIFAR-10 dataset**, denoted as  

$\mathcal{D}_{\textrm{original}} = \left(\mathcal{X}_{\textrm{original}}, \mathcal{Y}_{\textrm{original}}\right)$

---

### Model Types
We consider the following single-model baselines, all based on **ResNet architectures**:

1. **Original-only model**  
   - Trained solely on $\mathcal{D}_{\textrm{original}}$  
   - Fine-tuned by specialization on original samples.  

2. **Union-trained model (Original + P1)**  
   - Trained on the union of original samples and perturbed samples $P_1$.

3. **Adversarial training model (Baseline)**  
   - Trained on the union of original + $P_1$.  
   - Fine-tuned by specialization on original samples.  
   - This serves as the actual adversarial training baseline.

---

### Evaluation Strategy

- All models are tested on the samples we have. 
- The evaluation highlights cases where:
  - A model trained on polluted samples performs reasonably on originals,  
  - but fails on adversarial samples (including unseen attacks).  

This exposes weaknesses of adversarially trained models.

---

### Logifold Setup
- The **Logifold ensemble** consists of:
  - **Judge model** (original),  
  - **Experts** derived via specialization.  
- Expert models are specialized on the **union of high-entropy samples** (from both original and $P_1$), allowing them to focus on ambiguous or adversarially perturbed regions of the data space.

---

### Key Insight

We already know that **Logifold outperforms adversarially trained baselines**.  
By leveraging the perturbed sample sets $P_1$, the **performance gap becomes even more dramatic**, emphasizing the robustness of the Logifold structure.