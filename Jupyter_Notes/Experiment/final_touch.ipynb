{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10cc30d",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "We expereienced logifold augmented by 'largely perturbed sample' with entropy method works well. But for the original and for adversarial samples perturbed little we got negative bar (in the bar graph). \n",
    "\n",
    "Therefore we can try to match the ratio of testing dataset with the validation dataset. (ratio of low entropy to high entropy).\n",
    "\n",
    "Here, we'll going to use PGD largely perturbed.\n",
    "\n",
    "Also, for the standard perturbation, we experienced over-fitting issue. Perhaps high entropy dataset may be skewed little. So, try to make them balanced.\n",
    "\n",
    "Here, we'll going to use \n",
    "\n",
    "DeepFool, CWL2, PGD std. (total three samples).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bce072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "DATA = ROOT / \"data\"\n",
    "MODELS_DIR = DATA / \"models\"\n",
    "ADV_MODELS_DIR = DATA / \"adversarial_models\"\n",
    "ADV_SAMPLES = DATA / \"samples\"\n",
    "EXPERTS_DIR = DATA / \"specialized_models\"\n",
    "LOGIFOLD_MODS = (ROOT / \"logifold_modules\") \n",
    "\n",
    "\n",
    "CACHE = DATA / \"cache\"\n",
    "CACHE_PREDS = CACHE / \"preds\"\n",
    "CACHE_METRICS = CACHE / \"metrics\"\n",
    "CACHE_INDEX = CACHE / \"index\"\n",
    "ANALYSIS = DATA / \"analysis\"\n",
    "ANALYSIS.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES = ANALYSIS / \"figures\"\n",
    "FIGURES.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS = ANALYSIS / \"reports\"\n",
    "REPORTS.mkdir(parents=True, exist_ok=True)\n",
    "LGFD_PATH = DATA / \"logifold/\"\n",
    "LGFD_PATH.mkdir(parents=True, exist_ok=True)\n",
    "# Define Judge\n",
    "JUDGES_DIR = sorted(glob.glob(str(MODELS_DIR / 'resnet*original_tuned-once-on_original*')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logifold_modules.logifoldv1_4_modified import Logifold, _stem_all, int_from_model_path\n",
    "from logifold_modules.resnet_modified import ResNet\n",
    "import logifold_modules.custom_specialization as specialization\n",
    "from adv_logifold import AdvLogifold, get_statistics, plot_disagreements\n",
    "import cache_store\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adv_samples(pattern: str, _print_ : bool = False) -> np.ndarray:\n",
    "    files = sorted(glob.glob(str(ADV_SAMPLES / pattern)))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No samples for pattern: {pattern}\")\n",
    "    if _print_:\n",
    "        print(f\"Loading {len(files)} files matching pattern: {pattern}\")\n",
    "        for f in files:\n",
    "            \n",
    "            print(f\" - {f}\")\n",
    "    samples = [np.load(f) for f in files]\n",
    "    if len(samples) == 1:\n",
    "        samples = samples[0]\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bda5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x, y), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_val = x_val.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "y_train_categorical_10 = to_categorical(y_train,10)\n",
    "y_val_categorical_10 = to_categorical(y_val,10)\n",
    "y_test_categorical_10 = to_categorical(y_test,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d3c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_union_and_specialize(\n",
    "    \n",
    "    x_adv_tr: np.ndarray, x_adv_val: np.ndarray, adv_label: str,\n",
    "    \n",
    ") -> Tuple[tf.keras.Model, tf.keras.Model]:\n",
    "    \"\"\"\n",
    "    Returns (baseline_adv_model, tuned_baseline_adv_model, tuned_history_dict_or_None)\n",
    "    \"\"\"\n",
    "    size = x_adv_tr.shape[0] # CWL2 example training size is not 40000 but 10001.\n",
    "    train_union = np.concatenate([x_train, x_adv_tr], axis=0)\n",
    "    val_union = np.concatenate([x_val, x_adv_val], axis=0)\n",
    "\n",
    "    training_y_long=np.concatenate([y_train,y_train[:size]],axis=0)\n",
    "    validating_y_long=np.concatenate([y_val,y_val],axis=0)\n",
    "    if training_y_long.ndim == 1 or training_y_long.shape[1] != 10:\n",
    "        training_y_long = to_categorical(training_y_long, 10)\n",
    "    if validating_y_long.ndim == 1 or validating_y_long.shape[1] != 10:\n",
    "        validating_y_long = to_categorical(validating_y_long, 10)\n",
    "\n",
    "    path = ADV_MODELS_DIR / f\"ResNet56v1_union-of-original-and-{adv_label}_ver0.keras\"\n",
    "    if path.exists():\n",
    "        base_model = load_model(path)\n",
    "        print(f'load {path} to specialize once')\n",
    "    else:\n",
    "        raise FileNotFoundError(f'no model at {path}')\n",
    "    baseline_before_tuning = base_model\n",
    "    path = ADV_MODELS_DIR / f\"ResNet56v1_union-of-original-and-{adv_label}_tuned-once-on_stratified_union-of-original-and-{adv_label}_ver0.keras\"\n",
    "    if path.exists():\n",
    "        baseline_after_tuning = load_model(path)\n",
    "        print(f'{path} already exists. try to get history of the training procedure')\n",
    "        hist_baseline = specialization.load_history(path) # it could be none.\n",
    "        if hist_baseline is None:\n",
    "            print(f\"[WARN] No history found for {path}\")\n",
    "    else:\n",
    "        print(f'{path} training...')\n",
    "        baseline_after_tuning,hist_baseline = specialization.turn_specialist(base_model, path = path,\n",
    "                                                x_tr=train_union, y_tr=training_y_long,\n",
    "                                                  x_v=val_union,   y_v=validating_y_long,\n",
    "                                                  epochs=21, learning_rate=1e-3, batch_size=128, verbose=1, name=f\"tuned_once\")\n",
    "        hist_baseline = {\"history\": hist_baseline.history, \"params\": hist_baseline.params, \"epoch\": hist_baseline.epoch}\n",
    "\n",
    "    return baseline_before_tuning, baseline_after_tuning, hist_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc686c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttackEntry:\n",
    "    short_tag: str                    # short_tag\n",
    "    glob_pattern: str            # pattern in data/samples\n",
    "    adv_label: str                     # label for cache\n",
    "\n",
    "ATTACKS: List[AttackEntry] = [\n",
    "    AttackEntry(\"CWL2\",            \"*cwl2*untargeted_train_by_resnet56v1_ver0.npy\", \"cwl2-untargeted-gen-by-resnet56v1-ver0\"),\n",
    "    AttackEntry(\"PGD_standard\",    \"*pgd*eps8*untargeted_train_by_resnet56v1_ver0.npy\",\"pgd-eps8-iter2-10steps-untargeted-gen-by-resnet56v1-ver0\"),\n",
    "    AttackEntry(\"DeepFool\",     \"*deepfool_untargeted_train_by_resnet56v1_ver0.npy\", \"deepfool-untargeted-gen-by-resnet56v1-ver0\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68eaf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specialize_Committee(adversarial_lgfd : AdvLogifold, Comm_keys : List[Tuple],  adv_short_tag: str):\n",
    "    # Get adversarial sample corresponding to the adv_short_tag\n",
    "    adv_type = adv_short_tag\n",
    "    for atk in ATTACKS:\n",
    "        if atk.short_tag == adv_type:\n",
    "            adv_sample_name = atk.adv_label\n",
    "            \n",
    "            adv_sample_train = load_adv_samples(atk.glob_pattern)\n",
    "            pattern = atk.glob_pattern.replace(\"train\", \"val\")\n",
    "            adv_sample_val = load_adv_samples(pattern)\n",
    "            break\n",
    "\n",
    "    # Compute entropy of adversarial sample by JUDGE models\n",
    "    ent_original_train =adversarial_lgfd.get_entropy_array(Comm_keys, sample_name = 'original_train', sample = x_train)\n",
    "    ent_adv_train = adversarial_lgfd.get_entropy_array(Comm_keys, sample_name = adv_sample_name + '_train', sample = adv_sample_train)\n",
    "    fp = FIGURES / f\"entropy-disagreements-on-original_train.png\"\n",
    "    if fp.exists():\n",
    "        pass\n",
    "    else:\n",
    "        plot_disagreements(ent_original_train, title = f\"Entropy Disagreements on original_train\", save_path = FIGURES / f\"entropy-disagreements-on-original_train.png\")\n",
    "\n",
    "    plot_disagreements(ent_adv_train, title = f\"Entropy Disagreements on {adv_sample_name}_train\", save_path = FIGURES / f\"entropy-disagreements-on-{adv_sample_name}_train.png\")\n",
    "\n",
    "    ent_original_val = adversarial_lgfd.get_entropy_array(Comm_keys, sample_name = 'original_val', sample = x_val)\n",
    "    fp = FIGURES / f\"entropy-disagreements-on-original_val.png\"\n",
    "    if fp.exists():\n",
    "        pass\n",
    "    else:\n",
    "        plot_disagreements(ent_original_val, title = f\"Entropy Disagreements on original_train\", save_path = FIGURES / f\"entropy-disagreements-on-original_val.png\")\n",
    "\n",
    "    ent_adv_val = adversarial_lgfd.get_entropy_array(Comm_keys, sample_name = adv_sample_name + '_val', sample = adv_sample_val)\n",
    "    plot_disagreements(ent_adv_val, title = f\"Entropy Disagreements on {adv_sample_name}_val\", save_path = FIGURES / f\"entropy-disagreements-on-{adv_sample_name}_val.png\")\n",
    "    \n",
    "    # Including original sample, compute average of entropy\n",
    "    stats = {}\n",
    "    stats[('original','train')] = get_statistics(ent_original_train)\n",
    "    stats[('original','val')] = get_statistics(ent_original_val)\n",
    "    stats[('adv','train')] = get_statistics(ent_adv_train)\n",
    "    stats[('adv','val')] = get_statistics(ent_adv_val)\n",
    "    train_alpha_union = (stats[('original','train')]['average'] + stats[('adv','train')]['average'])/2\n",
    "    val_alpha_union = (stats[('original','val')]['average'] + stats[('adv','val')]['average'])/2\n",
    "\n",
    "    # separate union of original and adversarial samples into high entropy and low entropy samples\n",
    "    loc_1_original_train = ent_original_train>=train_alpha_union\n",
    "    loc_1_adv_train = ent_adv_train>=train_alpha_union\n",
    "    loc_1_original_val = ent_original_val>=val_alpha_union\n",
    "    loc_1_adv_val = ent_adv_val>=val_alpha_union\n",
    "    print('alpha for train: {}, for val: {}'.format(train_alpha_union, val_alpha_union))\n",
    "    print('the number of data greater than alpha:')\n",
    "    print(f'Training set original + {adv_type}:', np.sum(loc_1_original_train), '+',np.sum(loc_1_adv_train), '=', np.sum(loc_1_original_train) + np.sum(loc_1_adv_train))\n",
    "    print(f'Validation set original + {adv_type}:', np.sum(loc_1_original_val), '+', np.sum(loc_1_adv_val), '=', np.sum(loc_1_original_val) + np.sum(loc_1_adv_val))\n",
    "    \n",
    "    DATASETS = {\"Experts_union\":dict(train = (np.concatenate([x_train[loc_1_original_train], adv_sample_train[loc_1_adv_train]]), \n",
    "                                            to_categorical(\n",
    "                                                np.concatenate(\n",
    "                                                [y_train[loc_1_original_train], y_train[:adv_sample_train.shape[0]][loc_1_adv_train]]\n",
    "                                                ), 10)\n",
    "                                            ),\n",
    "                                    val=(np.concatenate([x_val[loc_1_original_val], adv_sample_val[loc_1_adv_val]]), \n",
    "                                        to_categorical(\n",
    "                                            np.concatenate(\n",
    "                                                [y_val[loc_1_original_val], y_val[loc_1_adv_val]]\n",
    "                                                ),10)))}\n",
    "    \n",
    "    # specialize Judge models on the high entropy samples\n",
    "    EXPERTS_KEYS = []\n",
    "    experts_paths = []\n",
    "    \n",
    "    for a_judge_key in Comm_keys: \n",
    "        a_judge = adversarial_lgfd.getModel(a_judge_key)\n",
    "        a_judge_name = adversarial_lgfd.model_source_name(a_judge_key)\n",
    "        \n",
    "        path = EXPERTS_DIR / f\"{a_judge_name}_specialized-once-on_high-entropy-union-of-original-and-{adv_sample_name}_ver0.keras\"\n",
    "        \n",
    "        if path.exists():\n",
    "            print(f\"There is specialized Judge {a_judge_name} on union of original and {adv_type} samples.\")\n",
    "        \n",
    "            specialist = load_model(str(path))\n",
    "        else:\n",
    "            print(f\"Specializing Judge {a_judge_name} on union of original and {adv_type} samples...\")\n",
    "        \n",
    "            specialist, _ = specialization.turn_specialist(model = a_judge, path = path,\n",
    "                                           x_tr = DATASETS[\"Experts_union\"][\"train\"][0], y_tr = DATASETS[\"Experts_union\"][\"train\"][1],\n",
    "                                           x_v = DATASETS[\"Experts_union\"][\"val\"][0], y_v = DATASETS[\"Experts_union\"][\"val\"][1],\n",
    "                                           epochs = 21, learning_rate = 1e-3, batch_size = 128, verbose = 0, \n",
    "                                           name = f\"specialized_once_on_high-entropy_union_of_original_and_{adv_sample_name}\")\n",
    "            # Add them to Advlogifold\n",
    "        key = (a_judge_key[0],int_from_model_path(f\"{a_judge_name}_specialized-once-on_high-entropy-union-of-original-and-{adv_sample_name}_ver0.keras\"))\n",
    "        print('prepared key:', key)\n",
    "        if key in adversarial_lgfd.keys():\n",
    "            print(f'specialized model is already a member of logifold')\n",
    "        else:\n",
    "            print(f'Adding specialized model...')\n",
    "            adversarial_lgfd.add(specialist,\n",
    "                             key = key,\n",
    "                             model_path = _stem_all(path),\n",
    "                             description = f'specialized on high entropy union of original and {adv_sample_name}', \n",
    "                             fuzDom = {})\n",
    "        # compute fuzdom\n",
    "        adversarial_lgfd.getFuzDoms(keys = [key],\n",
    "                            x = DATASETS[\"Experts_union\"][\"val\"][0], y = DATASETS[\"Experts_union\"][\"val\"][1], sample_name = f'union_of_original_and_{adv_sample_name}_val',\n",
    "                            update = False, autosave = False, verbose = 0)\n",
    "        EXPERTS_KEYS.append(key)\n",
    "        experts_paths.append(path)\n",
    "        \n",
    "        alpha = val_alpha_union\n",
    "    return EXPERTS_KEYS, experts_paths, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f87e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- imports & globals ---\n",
    "import os, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "OUTDIR = \"ratio_summary\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# --- helpers ---\n",
    "def class_stats(y, mask, uniform=0.1):\n",
    "    \"\"\"Per-class counts/ratios/deviation for y[mask].\"\"\"\n",
    "    yy = y[mask].reshape(-1)\n",
    "    classes, counts = np.unique(yy, return_counts=True)\n",
    "    total = int(counts.sum())\n",
    "    ratios = counts / total if total > 0 else np.zeros_like(counts, dtype=float)\n",
    "    dev = ratios - uniform\n",
    "    out = pd.DataFrame(\n",
    "        {\"count\": counts, \"ratio\": np.round(ratios, 4), \"dev_from_0.1\": np.round(dev, 4)},\n",
    "        index=classes,\n",
    "    )\n",
    "    out.index.name = \"class\"\n",
    "    return out, total\n",
    "\n",
    "def summarize_split(name, y, high_mask):\n",
    "    \"\"\"\n",
    "    Build LOW/HIGH summaries for a single vector y with a boolean mask high_mask.\n",
    "    Convention: LOW = ~high_mask, HIGH = high_mask.\n",
    "    \"\"\"\n",
    "    assert y.shape[0] == high_mask.shape[0], f\"Length mismatch: y={y.shape} vs mask={high_mask.shape}\"\n",
    "    low_df, low_n   = class_stats(y, ~high_mask)\n",
    "    high_df, high_n = class_stats(y,  high_mask)\n",
    "    return {\"name\": name, \"low\": low_df, \"low_total\": low_n, \"high\": high_df, \"high_total\": high_n}\n",
    "\n",
    "def pretty_table(title, df, total, save_basename=None):\n",
    "    df2 = df.copy()\n",
    "    df2[\"%\"] = (df2[\"ratio\"] * 100).round(2)\n",
    "    df2 = df2[[\"count\", \"ratio\", \"%\", \"dev_from_0.1\"]]\n",
    "    print(f\"\\n=== {title} (n={total}) ===\")\n",
    "    display(df2)\n",
    "    if save_basename:\n",
    "        csv_path = os.path.join(OUTDIR, f\"{save_basename}.csv\")\n",
    "        df2.to_csv(csv_path)\n",
    "        print(f\"[Saved table] {csv_path}\")\n",
    "\n",
    "def barplot(title, df, save_basename=None, dpi=160):\n",
    "    df = df.sort_index()\n",
    "    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "    ax.bar(df.index, df[\"ratio\"].values)\n",
    "    ax.axhline(0.1, linestyle=\"--\")\n",
    "    ax.set_title(title); ax.set_xlabel(\"class\"); ax.set_ylabel(\"ratio\")\n",
    "\n",
    "    if save_basename:\n",
    "        png_path = os.path.join(OUTDIR, f\"{save_basename}.png\")\n",
    "        fig.savefig(png_path, bbox_inches=\"tight\", dpi=dpi)\n",
    "        print(f\"[Saved plot] {png_path}\")\n",
    "\n",
    "    # Force inline render as an image (works even when fig repr shows)\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format=\"png\", bbox_inches=\"tight\", dpi=dpi)\n",
    "    buf.seek(0)\n",
    "    display(Image(data=buf.getvalue()))\n",
    "    plt.close(fig)\n",
    "\n",
    "def summarize_union_two(name, y_A, high_A, y_B, high_B):\n",
    "    \"\"\"Union (concat) of two splits (e.g., original & adversarial).\"\"\"\n",
    "    y_union    = np.concatenate([y_A.reshape(-1), y_B.reshape(-1)])\n",
    "    high_union = np.concatenate([high_A.reshape(-1), high_B.reshape(-1)]).astype(bool)\n",
    "    return summarize_split(name, y_union, high_union)\n",
    "\n",
    "def equalize_by_min_per_class(indices, labels, classes=None, seed=42, shuffle=True):\n",
    "    \"\"\"\n",
    "    Downsample `indices` so each class has exactly the minimum available count across classes.\n",
    "\n",
    "    Args:\n",
    "        indices : 1D array[int] -> indices into `labels` you want to stratify (e.g., LOW bucket indices)\n",
    "        labels  : 1D array[int] -> label vector for the *full* dataset (labels[indices] used here)\n",
    "        classes : optional sequence of class ids to enforce presence/order (default: infer from labels[indices])\n",
    "        seed    : RNG seed for reproducibility\n",
    "        shuffle : if True, shuffle the final concatenated result\n",
    "\n",
    "    Returns:\n",
    "        chosen_indices : 1D array[int] of selected indices (balanced, no replacement)\n",
    "        per_class_k    : target count per class (the min count)\n",
    "        counts_before  : dict[class -> original available count in `indices`]\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.asarray(indices)\n",
    "    y   = np.asarray(labels)[idx]\n",
    "\n",
    "    # classes present\n",
    "    if classes is None:\n",
    "        cls = np.unique(y)\n",
    "    else:\n",
    "        cls = np.array(classes)\n",
    "\n",
    "    # counts per class (only within `indices`)\n",
    "    counts_before = {c: int(np.sum(y == c)) for c in cls}\n",
    "    per_class_k = min(counts_before.values()) if len(counts_before) else 0\n",
    "\n",
    "    chosen = []\n",
    "    for c in cls:\n",
    "        c_idx = idx[y == c]\n",
    "        if len(c_idx) < per_class_k:\n",
    "            # If any class is too small (shouldn't happen with per_class_k=min),\n",
    "            # we still bail out safely by sampling all that exist.\n",
    "            take = len(c_idx)\n",
    "        else:\n",
    "            take = per_class_k\n",
    "        chosen.append(rng.choice(c_idx, size=take, replace=False))\n",
    "\n",
    "    if len(chosen):\n",
    "        chosen = np.concatenate(chosen)\n",
    "        if shuffle:\n",
    "            rng.shuffle(chosen)\n",
    "    else:\n",
    "        chosen = np.array([], dtype=int)\n",
    "\n",
    "    return chosen, per_class_k, counts_before\n",
    "\n",
    "# --- main ---\n",
    "def ratio_of_samples(adversarial_lgfd, Comm_keys, adv_short_tag):\n",
    "    \"\"\"\n",
    "    Compute entropy on original & a chosen adversarial set, threshold by the\n",
    "    union-average alpha, and report LOW/HIGH class ratios for the TRAIN union\n",
    "    and VAL union (original U adversarial).\n",
    "\n",
    "    Expects globals available:\n",
    "        - ATTACKS: iterable with .short_tag, .adv_label, .glob_pattern\n",
    "        - x_train, y_train, x_val, y_val\n",
    "        - load_adv_samples, plot_disagreements, FIGURES\n",
    "    \"\"\"\n",
    "    # 1) resolve attack & load samples\n",
    "    atk = next((a for a in ATTACKS if a.short_tag == adv_short_tag), None)\n",
    "    if atk is None:\n",
    "        raise ValueError(f\"Unknown adv_short_tag: {adv_short_tag}\")\n",
    "\n",
    "    adv_sample_name = atk.adv_label\n",
    "    adv_train = load_adv_samples(atk.glob_pattern)\n",
    "    adv_val   = load_adv_samples(atk.glob_pattern.replace(\"train\", \"val\"))\n",
    "    \n",
    "    # 2) entropy arrays (by the JUDGE committee)\n",
    "    ent_orig_tr = adversarial_lgfd.get_entropy_array(Comm_keys, sample_name=\"original_train\", sample=x_train)\n",
    "    ent_orig_tr = ent_orig_tr[:adv_train.shape[0]]\n",
    "    ent_adv_tr  = adversarial_lgfd.get_entropy_array(Comm_keys, sample_name=f\"{adv_sample_name}_train\", sample=adv_train)\n",
    "    ent_orig_va = adversarial_lgfd.get_entropy_array(Comm_keys, sample_name=\"original_val\", sample=x_val)\n",
    "    ent_adv_va  = adversarial_lgfd.get_entropy_array(Comm_keys, sample_name=f\"{adv_sample_name}_val\",   sample=adv_val)\n",
    "\n",
    "\n",
    "    # 3) union alphas (average of means of orig & adv)\n",
    "    def mean_of(a): return float(np.mean(a)) if a.size else np.nan\n",
    "    alpha_tr = 0.5 * (mean_of(ent_orig_tr) + mean_of(ent_adv_tr))\n",
    "    alpha_va = 0.5 * (mean_of(ent_orig_va) + mean_of(ent_adv_va))\n",
    "    print(f\"alpha — train: {alpha_tr}, val: {alpha_va}\")\n",
    "\n",
    "    # 4) HIGH/LOW masks using union alpha (True => HIGH)\n",
    "    high_orig_tr = ent_orig_tr >= alpha_tr\n",
    "    high_adv_tr  = ent_adv_tr  >= alpha_tr\n",
    "    high_orig_va = ent_orig_va >= alpha_va\n",
    "    high_adv_va  = ent_adv_va  >= alpha_va\n",
    "    \n",
    "    # 5) report counts above alpha\n",
    "    print(\"count ≥ alpha (HIGH):\")\n",
    "    print(f\"  TRAIN original + {adv_short_tag}: {high_orig_tr.sum()} + {high_adv_tr.sum()} = {int(high_orig_tr.sum()+high_adv_tr.sum())}\")\n",
    "    print(f\"  VAL   original + {adv_short_tag}: {high_orig_va.sum()} + {high_adv_va.sum()} = {int(high_orig_va.sum()+high_adv_va.sum())}\")\n",
    "    \n",
    "    \n",
    "    # 6) UNION summaries (train & val)\n",
    "    n_adv_tr  = adv_train.shape[0]\n",
    "    x_tr_union = np.concatenate([x_train[:n_adv_tr], adv_train])\n",
    "    y_tr_union = np.concatenate([y_train[:n_adv_tr], y_train[:n_adv_tr]])\n",
    "    y_tr_union = y_tr_union.reshape(-1)\n",
    "    high_tr_union = np.concatenate([high_orig_tr, high_adv_tr])\n",
    "    \n",
    "    x_va_union = np.concatenate([x_val, adv_val])\n",
    "    y_va_union = np.concatenate([y_val, y_val])\n",
    "    y_va_union = y_va_union.reshape(-1)\n",
    "    high_va_union = np.concatenate([high_orig_va, high_adv_va])\n",
    "\n",
    "    train_union = summarize_split(f\"TRAIN union (original U {adv_short_tag})\", y_tr_union, high_tr_union)\n",
    "    val_union   = summarize_split(f\"VAL union (original U {adv_short_tag})\",   y_va_union, high_va_union)\n",
    "\n",
    "\n",
    "    # 7) display tables + plots + save artifacts\n",
    "    for s in (train_union, val_union):\n",
    "        base = s[\"name\"].replace(\" \", \"_\")\n",
    "#         pretty_table(f\"{s['name']} — LOW\",  s[\"low\"],  s[\"low_total\"],  save_basename=f\"{base}_LOW_table\")\n",
    "#         pretty_table(f\"{s['name']} — HIGH\", s[\"high\"], s[\"high_total\"], save_basename=f\"{base}_HIGH_table\")\n",
    "        barplot(f\"{s['name']} — LOW\",  s[\"low\"],  save_basename=f\"{base}_LOW_plot\")\n",
    "        barplot(f\"{s['name']} — HIGH\", s[\"high\"], save_basename=f\"{base}_HIGH_plot\")\n",
    "    \n",
    "    \n",
    "    all_tr_idx = np.arange(len(y_tr_union))\n",
    "    low_tr_idx  = all_tr_idx[~high_tr_union]\n",
    "    high_tr_idx = all_tr_idx[ high_tr_union]\n",
    "\n",
    "    all_va_idx = np.arange(len(y_va_union))\n",
    "    low_va_idx  = all_va_idx[~high_va_union]\n",
    "    high_va_idx = all_va_idx[ high_va_union]\n",
    "    \n",
    "    low_tr_balanced_idx,  k_low_tr,  counts_low_tr  = equalize_by_min_per_class(low_tr_idx,  y_tr_union, seed=42)\n",
    "    high_tr_balanced_idx, k_high_tr, counts_high_tr = equalize_by_min_per_class(high_tr_idx, y_tr_union, seed=42)\n",
    "\n",
    "    low_va_balanced_idx,  k_low_va,  counts_low_va  = equalize_by_min_per_class(low_va_idx,  y_va_union, seed=42)\n",
    "    high_va_balanced_idx, k_high_va, counts_high_va = equalize_by_min_per_class(high_va_idx, y_va_union, seed=42)\n",
    "\n",
    "    print(\"[TRAIN LOW]  per-class =\", k_low_tr,  \" total =\", len(low_tr_balanced_idx), \"before =\", counts_low_tr)\n",
    "    print(\"[TRAIN HIGH] per-class =\", k_high_tr, \" total =\", len(high_tr_balanced_idx), \"before =\", counts_high_tr)\n",
    "    print(\"[VAL   LOW]  per-class =\", k_low_va,  \" total =\", len(low_va_balanced_idx), \"before =\", counts_low_va)\n",
    "    print(\"[VAL   HIGH] per-class =\", k_high_va, \" total =\", len(high_va_balanced_idx), \"before =\", counts_high_va)\n",
    "    x_train_high_bal = x_tr_union[high_tr_balanced_idx]\n",
    "    y_train_high_bal = y_tr_union[high_tr_balanced_idx]\n",
    "    \n",
    "    x_val_high_bal = x_va_union[high_va_balanced_idx]\n",
    "    y_val_high_bal = y_va_union[high_va_balanced_idx]\n",
    "    \n",
    "    print('After balancing, the shape of high entropy training set:', x_train_high_bal.shape, y_train_high_bal.shape)\n",
    "    print('After balancing, the shape of high entropy validation set:', x_val_high_bal.shape, y_val_high_bal.shape)\n",
    "    \n",
    "    # 8) return structured results if you want to program against them\n",
    "    return {\"train_union\": train_union, \"val_union\": val_union, \"alpha_train\": alpha_tr, \"alpha_val\": alpha_va}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5082a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specialize_Committee_stratified_way(adversarial_lgfd, Comm_keys, adv_short_tag):\n",
    "    \"\"\"\n",
    "    Compute entropy on original & a chosen adversarial set, threshold by the\n",
    "    union-average alpha, and report LOW/HIGH class ratios for the TRAIN union\n",
    "    and VAL union (original U adversarial).\n",
    "\n",
    "    Expects globals available:\n",
    "        - ATTACKS: iterable with .short_tag, .adv_label, .glob_pattern\n",
    "        - x_train, y_train, x_val, y_val\n",
    "        - load_adv_samples, plot_disagreements, FIGURES\n",
    "    \"\"\"\n",
    "    # 1) resolve attack & load samples\n",
    "    atk = next((a for a in ATTACKS if a.short_tag == adv_short_tag), None)\n",
    "    if atk is None:\n",
    "        raise ValueError(f\"Unknown adv_short_tag: {adv_short_tag}\")\n",
    "\n",
    "    adv_sample_name = atk.adv_label\n",
    "    adv_train = load_adv_samples(atk.glob_pattern)\n",
    "    adv_val   = load_adv_samples(atk.glob_pattern.replace(\"train\", \"val\"))\n",
    "    \n",
    "    # 2) entropy arrays (by the JUDGE committee)\n",
    "    ent_orig_tr = adversarial_lgfd.get_entropy_array(Comm_keys, sample_name=\"original_train\", sample=x_train)\n",
    "    ent_orig_tr = ent_orig_tr[:adv_train.shape[0]]\n",
    "    ent_adv_tr  = adversarial_lgfd.get_entropy_array(Comm_keys, sample_name=f\"{adv_sample_name}_train\", sample=adv_train)\n",
    "    ent_orig_va = adversarial_lgfd.get_entropy_array(Comm_keys, sample_name=\"original_val\", sample=x_val)\n",
    "    ent_adv_va  = adversarial_lgfd.get_entropy_array(Comm_keys, sample_name=f\"{adv_sample_name}_val\",   sample=adv_val)\n",
    "\n",
    "\n",
    "    # 3) union alphas (average of means of orig & adv)\n",
    "    def mean_of(a): return float(np.mean(a)) if a.size else np.nan\n",
    "    alpha_tr = 0.5 * (mean_of(ent_orig_tr) + mean_of(ent_adv_tr))\n",
    "    alpha_va = 0.5 * (mean_of(ent_orig_va) + mean_of(ent_adv_va))\n",
    "    print(f\"alpha — train: {alpha_tr}, val: {alpha_va}\")\n",
    "\n",
    "    # 4) HIGH/LOW masks using union alpha (True => HIGH)\n",
    "    high_orig_tr = ent_orig_tr >= alpha_tr\n",
    "    high_adv_tr  = ent_adv_tr  >= alpha_tr\n",
    "    high_orig_va = ent_orig_va >= alpha_va\n",
    "    high_adv_va  = ent_adv_va  >= alpha_va\n",
    "    \n",
    "    # 5) report counts above alpha\n",
    "    print(\"count ≥ alpha (HIGH):\")\n",
    "    print(f\"  TRAIN original + {adv_short_tag}: {high_orig_tr.sum()} + {high_adv_tr.sum()} = {int(high_orig_tr.sum()+high_adv_tr.sum())}\")\n",
    "    print(f\"  VAL   original + {adv_short_tag}: {high_orig_va.sum()} + {high_adv_va.sum()} = {int(high_orig_va.sum()+high_adv_va.sum())}\")\n",
    "    \n",
    "    \n",
    "    # 6) UNION summaries (train & val)\n",
    "    n_adv_tr  = adv_train.shape[0]\n",
    "    x_tr_union = np.concatenate([x_train[:n_adv_tr], adv_train])\n",
    "    y_tr_union = np.concatenate([y_train[:n_adv_tr], y_train[:n_adv_tr]])\n",
    "    y_tr_union = y_tr_union.reshape(-1)\n",
    "    high_tr_union = np.concatenate([high_orig_tr, high_adv_tr])\n",
    "    \n",
    "    x_va_union = np.concatenate([x_val, adv_val])\n",
    "    y_va_union = np.concatenate([y_val, y_val])\n",
    "    y_va_union = y_va_union.reshape(-1)\n",
    "    high_va_union = np.concatenate([high_orig_va, high_adv_va])\n",
    "\n",
    "    \n",
    "    all_tr_idx = np.arange(len(y_tr_union))\n",
    "    high_tr_idx = all_tr_idx[ high_tr_union]\n",
    "\n",
    "    all_va_idx = np.arange(len(y_va_union))\n",
    "    high_va_idx = all_va_idx[ high_va_union]\n",
    "    \n",
    "    high_tr_balanced_idx, k_high_tr, counts_high_tr = equalize_by_min_per_class(high_tr_idx, y_tr_union, seed=42)\n",
    "\n",
    "    high_va_balanced_idx, k_high_va, counts_high_va = equalize_by_min_per_class(high_va_idx, y_va_union, seed=42)\n",
    "\n",
    "    print(\"[TRAIN HIGH] per-class =\", k_high_tr, \" total =\", len(high_tr_balanced_idx), \"before =\", counts_high_tr)\n",
    "    print(\"[VAL   HIGH] per-class =\", k_high_va, \" total =\", len(high_va_balanced_idx), \"before =\", counts_high_va)\n",
    "    x_train_high_bal = x_tr_union[high_tr_balanced_idx]\n",
    "    y_train_high_bal = y_tr_union[high_tr_balanced_idx]\n",
    "    \n",
    "    x_val_high_bal = x_va_union[high_va_balanced_idx]\n",
    "    y_val_high_bal = y_va_union[high_va_balanced_idx]\n",
    "    \n",
    "    print('After balancing, the shape of high entropy training set:', x_train_high_bal.shape, y_train_high_bal.shape)\n",
    "    print('After balancing, the shape of high entropy validation set:', x_val_high_bal.shape, y_val_high_bal.shape)\n",
    "    \n",
    "    DATASETS = {\"Experts_union\":dict(train = (x_train_high_bal, to_categorical(\n",
    "                                              y_train_high_bal, 10)),\n",
    "                                    val=(x_val_high_bal, to_categorical(\n",
    "                                              y_val_high_bal, 10)))}\n",
    "    # specialize Judge models on the high entropy samples\n",
    "    EXPERTS_KEYS = []\n",
    "    experts_paths = []\n",
    "    \n",
    "    for a_judge_key in Comm_keys: \n",
    "        a_judge = adversarial_lgfd.getModel(a_judge_key)\n",
    "        a_judge_name = adversarial_lgfd.model_source_name(a_judge_key)\n",
    "        \n",
    "        path = EXPERTS_DIR / f\"{a_judge_name}_specialized-once-on_stratified_high-entropy-union-of-original-and-{adv_sample_name}_ver0.keras\"\n",
    "        \n",
    "        if path.exists():\n",
    "            print(f\"There is specialized Judge {a_judge_name} on union of (stratified) original and {adv_short_tag} samples.\")\n",
    "        \n",
    "            specialist = load_model(str(path))\n",
    "        else:\n",
    "            print(f\"Specializing Judge {a_judge_name} on union of (stratified) original and {adv_short_tag} samples...\")\n",
    "\n",
    "            specialist, _ = specialization.turn_specialist(model = a_judge, path = path,\n",
    "                                           x_tr = DATASETS[\"Experts_union\"][\"train\"][0], y_tr = DATASETS[\"Experts_union\"][\"train\"][1],\n",
    "                                           x_v = DATASETS[\"Experts_union\"][\"val\"][0], y_v = DATASETS[\"Experts_union\"][\"val\"][1],\n",
    "                                           epochs = 21, learning_rate = 1e-3, batch_size = 32, verbose = 0, \n",
    "                                           name = f\"specialized_once_on_stratified_high-entropy_union_of_original_and_{adv_sample_name}\")\n",
    "            # Add them to Advlogifold\n",
    "        key = (a_judge_key[0],int_from_model_path(f\"{a_judge_name}_specialized-once-on_stratified_high-entropy-union-of-original-and-{adv_sample_name}_ver0.keras\"))\n",
    "        print('prepared key:', key)\n",
    "        if key in adversarial_lgfd.keys():\n",
    "            print(f'specialized model is already a member of logifold')\n",
    "        else:\n",
    "            print(f'Adding specialized model...')\n",
    "            adversarial_lgfd.add(specialist,\n",
    "                             key = key,\n",
    "                             model_path = _stem_all(path),\n",
    "                             description = f'specialized on stratified high entropy union of original and {adv_sample_name}', \n",
    "                             fuzDom = {})\n",
    "        # compute fuzdom\n",
    "        adversarial_lgfd.getFuzDoms(keys = [key],\n",
    "                            x = DATASETS[\"Experts_union\"][\"val\"][0], y = DATASETS[\"Experts_union\"][\"val\"][1], sample_name = f'union_of_original_and_{adv_sample_name}_val',\n",
    "                            update = False, autosave = False, verbose = 0)\n",
    "        EXPERTS_KEYS.append(key)\n",
    "        experts_paths.append(path)\n",
    "        \n",
    "        alpha = alpha_va\n",
    "    # 8) return structured results if you want to program against them\n",
    "    return EXPERTS_KEYS, experts_paths, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab35c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "specialized_model_paths = ['resnet20v1_original_tuned-once-on_original_ver0_specialized-once-on_stratified_high-entropy-union-of-original-and-pgd-eps8-iter2-10steps-untargeted-gen-by-resnet56v1-ver0_ver0.history.json',\n",
    "                'resnet20v1_original_tuned-once-on_original_ver6_specialized-once-on_stratified_high-entropy-union-of-original-and-cwl2-untargeted-gen-by-resnet56v1-ver0_ver0.history.json',\n",
    "'resnet20v2_original_tuned-once-on_original_ver0_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet20v1_original_tuned-once-on_original_ver6_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet20v1_original_tuned-once-on_original_ver1_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet56v2_original_tuned-once-on_original_ver0_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet56v1_original_tuned-once-on_original_ver1_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet20v1_original_tuned-once-on_original_ver0_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet20v1_original_tuned-once-on_original_ver7_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet20v2_original_tuned-once-on_original_ver1_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet56v1_original_tuned-once-on_original_ver0_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet56v2_original_tuned-once-on_original_ver2_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet56v1_original_tuned-once-on_original_ver3_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet20v1_original_tuned-once-on_original_ver4_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet20v2_original_tuned-once-on_original_ver2_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet20v1_original_tuned-once-on_original_ver3_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet56v1_original_tuned-once-on_original_ver2_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet56v2_original_tuned-once-on_original_ver3_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet20v1_original_tuned-once-on_original_ver2_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet20v2_original_tuned-once-on_original_ver3_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json', \n",
    "'resnet20v1_original_tuned-once-on_original_ver5_specialized-once-on_stratified_high-entropy-union-of-original-and-deepfool-untargeted-gen-by-resnet56v1-ver0_ver0.history.json'\n",
    "]           \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
