{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97475b78",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "This note will conduct an experiment following this pipeline:\n",
    "\n",
    "1. Compose a Judge Committee, $C_J = \\{f_1, \\ldots, f_N\\}$. Here, we can take two versions of Committee: SB type and B type. SB type Committee consists of specialized ResNet models while B type doesn't.\n",
    "\n",
    "2. Here we define several terminologies:\n",
    "\n",
    "- Let $D_{*} = (X_*, Y_*)$ denote dataset, where the asterisk reserves type of dataset, for instance $D_{\\textrm{tr}}$. $D$ refers dataset (includes unseen data). \n",
    "- Define $I_C : D \\to [0,1]$ which measures disagreement(or entropy) among Committee $C$. \n",
    "- $\\alpha \\in [0,1]$ denotes a given entropy threshold.\n",
    "- For a dataset $D_{*}$, define $D^0_{*}(C) := \\{I_C(x) < \\alpha\\}$ and $D^1_{*}(C) := \\{I_C(x) \\geq \\alpha\\}$.\n",
    "\n",
    "3. Specialize each member of the Judge Committee $f \\in C_J$ into $g$ on $D^1_{*}(C_J)$. We call the newly obtained Committee $C_S = \\{g_1, \\ldots, g_N\\}$.\n",
    "\n",
    "4. When an instance $x$ is given, first determine $I_{C_J}(x)$, and make prediction according to the value. If $I_{C_J}(x) < \\alpha$ then $C_J$ predicts, otherwise $C_S$ does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f330ee22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 22:02:16.667842: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "## load necessary libraries\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras import Model\n",
    "from scipy.stats import entropy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5d599",
   "metadata": {},
   "source": [
    "## Compose Judge Committee\n",
    "\n",
    "We can have two type of $C_J$: SB and B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ff6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## predefined dictionary for label\n",
    "label_dict = {(3,1) : 'ResNet20v1',\n",
    "        (3,2) : 'ResNet20v2',\n",
    "        (9,1): 'ResNet56v1',\n",
    "        (9,2): 'ResNet56v2'}\n",
    "\n",
    "## load models\n",
    "\n",
    "# Say 'B-type' model, where 'B' stands for 'Base'.\n",
    "folder = 'CIFAR10models/ResNet/'\n",
    "pattern = os.path.join(folder, '*cifar10*.keras')\n",
    "file_list = sorted(glob.glob(pattern))\n",
    "loaded_models= {os.path.basename(f): load_model(f) for f in file_list}\n",
    "\n",
    "## Compose Committee\n",
    "Judge_Committee_B = []\n",
    "for file_name, model in loaded_models.items():\n",
    "    # Extract model name from the file name\n",
    "    base = file_name.replace('.keras','')\n",
    "    parts = base.split('_')\n",
    "    # base = n_3_v1_cifar10_1 or n_3_v1_cifar10\n",
    "    # parts = [n, 3, v1, cifar10, 1] or [n, 3, v1, cifar10]\n",
    "    model_name = label_dict[(int(parts[1]),int(parts[2][-1]))] + '_B_' +parts[-1][-1]\n",
    "    Judge_Committee_B.append((model_name, model))\n",
    "print(f\"Total {len(Judge_Committee_B)} B-type models loaded\")\n",
    "\n",
    "# Say 'SB-type', where 'SB' stands for 'Specialized Base'.\n",
    "folder = 'CIFAR10models/more_tunned/'\n",
    "pattern = os.path.join(folder, '*_more_specialized*.keras')\n",
    "file_list = sorted(glob.glob(pattern))\n",
    "loaded_models.update({os.path.basename(f): load_model(f) for f in file_list})\n",
    "\n",
    "## Compose Committee\n",
    "Judge_Committee_SB = []\n",
    "for file_name, model in loaded_models.items():\n",
    "    # Extract model name from the file name\n",
    "    base = file_name.replace('.keras','')\n",
    "    parts = base.split('_')\n",
    "    # base = ResNet20v1_more_specialized_0, ResNet20v1_once-more_specialized_0, or ResNet20v1_twice-more_specialized_0\n",
    "    # parts = [ResNet20v1, more, specialized, 0] or [ResNet20v1, once-more, specialized, 0] and so forth.\n",
    "    if parts[1] != 'more':\n",
    "        continue\n",
    "    model_name = parts[0] + '_SB_' + parts[-1]\n",
    "    Judge_Committee_SB.append((model_name, model))\n",
    "print(f\"Total {len(Judge_Committee_SB)} SB-type models loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf9d08",
   "metadata": {},
   "source": [
    "## Define $I$\n",
    "\n",
    "We can have three measurements: \n",
    "\n",
    "- Let say $f_1(x) = \\left(y^1_1, \\ldots, y^m_1\\right), \\ldots, f_N(x) = \\left(y^1_N, \\ldots, y^m_N\\right)$ with $m$-many classes and $N$-many models. \n",
    "\n",
    "- By taking $\\arg\\max f_i(x)$, we have each answers $a_1, \\ldots , a_N$. $a_1, \\ldots, a_N$ give discrete distribution such that $p_j = P(X=j) = \\frac{\\{a_i = j\\}}{N}$ where $j=1,\\ldots,m$. With this, we define $I^{\\arg\\max}_{C}(x) = -\\sum p_i \\log_{m}p_i$.\n",
    "\n",
    "- With raw predictions, we define $\\frac{\\sum_{k=1}^N y^i_k}{N} = p_i$ and $I^\\textrm{overall}_{C}(x) = -\\sum p_i \\log_{m}p_i$. Note that if $y^i_k = 0 $ or $1$ then it coincides with the previous one.\n",
    "\n",
    "- Also with raw predictions, we define $ I^{\\textrm{cross}} = -\\sum_{k\\neq l} \\sum_{i = 1}^{m} y^i_k \\log_m y^i_l $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5d20173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "\n",
    "# Sequence allows to have anything that behaves like a sequence including lists, tuples, and arrays.\n",
    "def cross_entropy(arrays: Sequence[np.ndarray], log_base: float | None = None) -> float:\n",
    "    \"\"\"\n",
    "    Compute  f = - sum_{k != l} sum_{i=1}^m a_{i,k} * log( a_{i,l} )\n",
    "    where each input is an array of shape (m, 1) (or (m,)).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arrays : sequence of np.ndarray\n",
    "        N arrays, each of shape (m,1) or (m,). They should all have the same m.\n",
    "    log_base : float or None, optional\n",
    "        If provided (e.g., 10), compute logarithm in this base.\n",
    "        If None, uses the natural logarithm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The scalar value of the cross entropy f among the arrays\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Time complexity is O(m N^2) dominated by the matrix multiply, where N is the length of arrays.\n",
    "    \"\"\"\n",
    "    if len(arrays) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Stack to shape (m, N)\n",
    "    cols = [np.asarray(a).reshape(-1) for a in arrays]\n",
    "    m = cols[0].shape[0]\n",
    "    if any(c.shape[0] != m for c in cols):\n",
    "        raise ValueError(\"All arrays must have the same first dimension m.\")\n",
    "    A = np.column_stack(cols)  # shape (m, N)\n",
    "\n",
    "    L = np.log(A+1e-12)\n",
    "    if log_base is not None:\n",
    "        L = L / np.log(log_base)\n",
    "\n",
    "    M = A.T @ L  # shape (N, N), M[k, l] = sum_i a_{i,k} * log(a_{i,l})\n",
    "    total = -(np.sum(M) - np.trace(M))  # sum over k != l, then negate\n",
    "    return float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "02f518be",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.random.rand(10, 3)\n",
    "arr2 = np.random.rand(10, 3)\n",
    "arr3 = np.random.rand(10, 3)\n",
    "arr4 = np.random.rand(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a6f446e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.12351293, 0.29237231, 0.70961478],\n",
       "        [0.7398503 , 0.05672318, 0.63630757],\n",
       "        [0.11432249, 0.22154813, 0.886484  ],\n",
       "        [0.35177859, 0.40300334, 0.68322051]],\n",
       "\n",
       "       [[0.20570996, 0.04658574, 0.01420149],\n",
       "        [0.26276874, 0.81190614, 0.24089568],\n",
       "        [0.78056377, 0.95961187, 0.10431696],\n",
       "        [0.87814417, 0.95017658, 0.84739626]],\n",
       "\n",
       "       [[0.70383204, 0.25313825, 0.8408647 ],\n",
       "        [0.14020372, 0.80240872, 0.96665429],\n",
       "        [0.78052659, 0.63248383, 0.24348909],\n",
       "        [0.52170827, 0.83689521, 0.8256521 ]],\n",
       "\n",
       "       [[0.81953719, 0.38692825, 0.25188964],\n",
       "        [0.5310831 , 0.96873106, 0.06320516],\n",
       "        [0.94242732, 0.04088438, 0.31880355],\n",
       "        [0.08900948, 0.45553488, 0.09730984]],\n",
       "\n",
       "       [[0.47038348, 0.63170717, 0.48889846],\n",
       "        [0.99294304, 0.72238559, 0.3006918 ],\n",
       "        [0.74789541, 0.01997505, 0.09957158],\n",
       "        [0.27661194, 0.19025774, 0.26321753]],\n",
       "\n",
       "       [[0.95027208, 0.01553644, 0.98367798],\n",
       "        [0.42677776, 0.52853843, 0.99759844],\n",
       "        [0.25973543, 0.79719378, 0.74642473],\n",
       "        [0.6177143 , 0.37862789, 0.86735429]],\n",
       "\n",
       "       [[0.70956088, 0.35211436, 0.18736687],\n",
       "        [0.57475954, 0.89868425, 0.42702912],\n",
       "        [0.10827169, 0.72524678, 0.56982512],\n",
       "        [0.43703137, 0.65277947, 0.79802163]],\n",
       "\n",
       "       [[0.57001316, 0.18692666, 0.27614995],\n",
       "        [0.32126931, 0.54598665, 0.43692208],\n",
       "        [0.61683705, 0.53453431, 0.99491933],\n",
       "        [0.00255883, 0.97836639, 0.61237236]],\n",
       "\n",
       "       [[0.89131335, 0.28325187, 0.15078341],\n",
       "        [0.04570566, 0.70010817, 0.07725921],\n",
       "        [0.15761286, 0.23082152, 0.47175446],\n",
       "        [0.54304622, 0.79659486, 0.07055274]],\n",
       "\n",
       "       [[0.05837623, 0.63671711, 0.02454064],\n",
       "        [0.07831072, 0.8010688 , 0.80247043],\n",
       "        [0.16522331, 0.13895516, 0.11187012],\n",
       "        [0.0787895 , 0.39717504, 0.21986578]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2302afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = np.stack([arr1,arr2,arr3, arr4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "778f5e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.070740258358502"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24913d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1,a2,a3 9.868764878539828\n",
      "a1,a1,a1 4.3315685693241734\n",
      "a2,a2,a2 6.0\n",
      "a3,a3,a3 2.8139735615356876\n",
      "a1,a2,a2 6.643856189774725\n",
      "a3,a2,a2 7.473931188332412\n",
      "a2,a3,a3 6.411922375510974\n",
      "a1,a3,a3 10.557733566151086\n",
      "a2,a1,a1 6.087712379549449\n",
      "a3,a1,a1 11.06359856874725\n"
     ]
    }
   ],
   "source": [
    "a1 = np.array([[0.2],[0.8]])\n",
    "a2 = np.array([[0.5],[0.5]])\n",
    "a3 = np.array([[0.9],[0.1]])\n",
    "\n",
    "val = cross_entropy([a1, a2, a3], log_base=2) \n",
    "print('a1,a2,a3',val)\n",
    "\n",
    "val = cross_entropy([a1, a1, a1],  log_base=2) \n",
    "print('a1,a1,a1',val)\n",
    "\n",
    "val = cross_entropy([a2, a2, a2],  log_base=2) \n",
    "print('a2,a2,a2',val)\n",
    "\n",
    "val = cross_entropy([a3, a3, a3], log_base=2) \n",
    "print('a3,a3,a3',val)\n",
    "\n",
    "val = cross_entropy([a1, a2, a2],  log_base=2) \n",
    "print('a1,a2,a2',val)\n",
    "\n",
    "val = cross_entropy([a3, a2, a2],  log_base=2) \n",
    "print('a3,a2,a2',val)\n",
    "\n",
    "val = cross_entropy([a2, a3, a3],  log_base=2) \n",
    "print('a2,a3,a3',val)\n",
    "\n",
    "val = cross_entropy([a1, a3, a3],  log_base=2) \n",
    "print('a1,a3,a3',val)\n",
    "\n",
    "val = cross_entropy([a2, a1, a1],  log_base=2) \n",
    "print('a2,a1,a1',val)\n",
    "\n",
    "val = cross_entropy([a3, a1, a1], log_base=2) \n",
    "print('a3,a1,a1',val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1112dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy_array(committee : list[tuple[str,Model]],\n",
    "                      entropy_version : str = \"argmax\",\n",
    "                      num_classes : int = 10,\n",
    "                      sample : np.ndarray = None, \n",
    "                      preds : dict[str, np.ndarray] = None) -> tuple[np.ndarray, dict[str, np.ndarray]]:\n",
    "    '''\n",
    "    param committee: list of tuple (member, model)\n",
    "    param entropy_version: whether to use argmax, overall or cross_entropy version for entropy calculation\n",
    "    param num_classes: number of targets\n",
    "    param sample: numpy array of (n,) shape\n",
    "    param preds: dictionary of (raw) predictions from each model in the committee.\n",
    "    Either sample or preds must be provided. If both sample and preds are provided, preds will be used.\n",
    "    \n",
    "    return (numpy array of entropy of (n,) shape, dictionary of preds whose keys are the member names) \n",
    "    '''\n",
    "    if sample is None and preds is None:\n",
    "        raise ValueError(\"Either sample or preds must be provided\")\n",
    "    if entropy_version not in [\"argmax\", \"overall\", \"cross_entropy\"]:\n",
    "        raise ValueError(\"entropy_version must be either 'argmax', 'overall' or 'cross_entropy'\")\n",
    "\n",
    "    if preds is None:\n",
    "        preds = {}\n",
    "        for member, model in committee:\n",
    "            preds[member] = model.predict(sample,verbose = 0)\n",
    "            \n",
    "    if entropy_version == \"argmax\":\n",
    "        stacked = np.stack([np.argmax(member_pred, axis=-1) for member_pred in preds.values()], axis=1)\n",
    "        counts = np.apply_along_axis(lambda x: np.bincount(x, minlength=num_classes), axis=1, arr=stacked)\n",
    "        probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "        disagreements = entropy(probs, axis=1, base = num_classes)\n",
    "        ## if member prediction is of (*,) dimensional array, then error will occur at stacked and counting procedure as well.\n",
    "        ## To remedy this, we can put reshaping line of code before stacking.\n",
    "        \n",
    "    elif entropy_version == \"overall\": # overall\n",
    "        stacked = np.stack([member_pred for member_pred in preds.values()], axis=1)\n",
    "        probs = np.sum(stacked,axis=1)/len(committee)\n",
    "        disagreements = entropy(probs, axis=1, base = num_classes)\n",
    "    elif entropy_version == 'cross_entropy':\n",
    "        stacked = np.stack([member_pred for member_pred in preds.values()], axis=1)\n",
    "        n = len(stacked)\n",
    "        disagreements = np.ndarray(np.zeros((n,num_classes)))\n",
    "        for i in range(n):\n",
    "            disagreements[i] = cross_entropy(stacked[i,:], log_base=num_classes)\n",
    "\n",
    "    return (disagreements, preds) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0a3c87",
   "metadata": {},
   "source": [
    "## Plotting entropies to choose threshold\n",
    "\n",
    "We may define entropy threhold arbitrarily. Or, we can see the boxplots of entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab87ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_disagreements(disagreements: np.ndarray | dict[str, np.ndarray], title: str = \"Disagreements\"):\n",
    "    '''\n",
    "    disagreements is either a numpy array of entropies, or a dictionary of entropy whose keys are the name of sample.\n",
    "    '''\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.boxplot(disagreements if isinstance(disagreements, np.ndarray) else list(disagreements.values()))\n",
    "    plt.title(title)\n",
    "    if isinstance(disagreements, dict):\n",
    "        plt.xticks(ticks=range(1, len(disagreements) + 1), labels=list(disagreements.keys()))\n",
    "    else:\n",
    "        plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Disagreement\")\n",
    "    plt.show()\n",
    "    \n",
    "def get_statistics(disagreements: np.ndarray) -> dict[str, float]:\n",
    "    '''\n",
    "    disagreements is a numpy array of entropies.\n",
    "    return dictionary of average, median, std, 1Q, 3Q.\n",
    "    '''\n",
    "    return {\n",
    "        \"average\": float(np.mean(disagreements)),\n",
    "        \"median\": float(np.median(disagreements)),\n",
    "        \"std\": float(np.std(disagreements)),\n",
    "        \"1Q\": float(np.percentile(disagreements, 25)),\n",
    "        \"3Q\": float(np.percentile(disagreements, 75)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0efa4d",
   "metadata": {},
   "source": [
    "## Specialization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c49ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 5, 10, 15, 18 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 18:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 15:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 10:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 5:\n",
    "        lr *= 1e-1\n",
    "#     print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "def turn_specialist(model : Model, path : str,\n",
    "        x_tr: np.ndarray | None = None,\n",
    "        y_tr: np.ndarray | None = None,\n",
    "        x_v: np.ndarray | None = None,\n",
    "        y_v: np.ndarray | None = None,\n",
    "        num_classes : int = 10,\n",
    "        epochs: int = 21,\n",
    "        learning_rate : float = 1e-3,\n",
    "        batch_size: int = 128,\n",
    "        # save_each: bool = False,\n",
    "        # save_bests: int | None = None,\n",
    "        verbose: int = 1,\n",
    "        name : str = '',\n",
    "        add_last_dense : bool = True\n",
    "    ):\n",
    "        if add_last_dense:\n",
    "            # build specialist network\n",
    "            base = Model(inputs = model.inputs, outputs = model.layers[-2].output, name=f\"base{name}\")\n",
    "            x    = keras.Input(shape=base.input_shape[1:], name=f\"in{name}\")\n",
    "            y    = Dense(num_classes, name=f\"dense{name}\")(base(x)) \n",
    "            z    = layers.Softmax(name=f\"softmax{name}\")(y)\n",
    "            specialist = Model(inputs = x, outputs = z)\n",
    "            specialist.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                loss=\"categorical_crossentropy\",\n",
    "                metrics=[\"accuracy\"],\n",
    "            )\n",
    "        else:\n",
    "            # we don't add a new dense layer\n",
    "            # and fine tune it.\n",
    "            specialist = model\n",
    "            specialist.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                loss=\"categorical_crossentropy\",\n",
    "                metrics=[\"accuracy\"],\n",
    "            )\n",
    "\n",
    "\n",
    "        # callbacks\n",
    "        callbacks = [ModelCheckpoint(path, monitor=\"val_accuracy\",\n",
    "                                    save_best_only=True, verbose=verbose)]\n",
    "        \n",
    "        callbacks += [LearningRateScheduler(lr_schedule),\n",
    "                        ReduceLROnPlateau(factor=np.sqrt(0.1), patience=5, min_lr=5e-7)]\n",
    "\n",
    "        # fit \n",
    "        hist = specialist.fit(x_tr, y_tr, batch_size=batch_size,\n",
    "                        validation_data=(x_v, y_v),\n",
    "                        epochs=epochs, callbacks=callbacks, verbose=verbose)\n",
    "\n",
    "\n",
    "\n",
    "        # ---------- summary ----------\n",
    "        metric = \"val_accuracy\"\n",
    "        best = np.max(hist.history[metric])\n",
    "        first = hist.history[metric][0]\n",
    "        print(f\"best {metric} {best:.3f} (first {first:.3f})\")\n",
    "        return specialist, hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb5594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specialize_Committee(committee: dict[str, Model], \n",
    "                         training_data: tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n",
    "                         data_name: str = '',\n",
    "                         committee_name:str = '',\n",
    "                         num_classes : int = 10):\n",
    "    x_tr, y_tr, x_v, y_v = training_data\n",
    "    if y_tr.ndim == 1:\n",
    "        y_tr = to_categorical(y_tr, num_classes)\n",
    "    if y_v.ndim == 1:\n",
    "        y_v = to_categorical(y_v, num_classes)\n",
    "    for member, model in committee.items():\n",
    "        print(f\"Training specialist for {member}...\")\n",
    "        # baseline, which is specialize once with the original labels.\n",
    "        specialist, history = turn_specialist(model,\n",
    "                        path=f'./specialists/{committee_name}/{member}_{data_name}.keras',\n",
    "                        x_tr = x_tr,\n",
    "                        y_tr = y_tr,\n",
    "                        x_v = x_v,\n",
    "                        y_v = y_v,\n",
    "                        name = f'{committee_name}_{member}_{data_name}',\n",
    "                        verbose = 0\n",
    "                        )\n",
    "        # see the graph of training process\n",
    "        plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "        plt.title(f'Accuracy of {member}\\non {data_name}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # save the specialist model\n",
    "        specialist.save(f'./specialists/{committee_name}/{member}_{data_name}.keras')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c29704",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d6380",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load CIFAR10 dataset\n",
    "\n",
    "(x, y), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, \n",
    "                                              random_state=42) # random state has been always 42.\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_val = x_val.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "y_train_categorical_10 = to_categorical(y_train,10)\n",
    "y_val_categorical_10 = to_categorical(y_val,10)\n",
    "y_test_categorical_10 = to_categorical(y_test,10)\n",
    "\n",
    "## we load only two adversarial samples\n",
    "\n",
    "import re\n",
    "\n",
    "pattern = re.compile(r'_\\d+to\\d+_')\n",
    "\n",
    "folder = './adversarial_examples/gen_by_ResNet'\n",
    "all_files = os.listdir(folder)\n",
    "\n",
    "filtered_files = [\n",
    "    fname for fname in all_files\n",
    "    if fname.endswith('.npy') and not pattern.search(fname)]\n",
    "\n",
    "\n",
    "## load all the training and validation dataset\n",
    "\n",
    "adv_dataset = {}\n",
    "import re\n",
    "for f in filtered_files:\n",
    "    \n",
    "    base = f.replace('.keras.npy','').replace('.npy','')\n",
    "    parts = base.split('_')\n",
    "    '''\n",
    "    cwl2_x_tr_untargeted\n",
    "    cwl2_x_v_untargeted\n",
    "    cwl2_x_test_untargeted <- gen by ResNet56v1_0\n",
    "    cwl2_x_test_untargeted_gen_by_ResNet56v1_1\n",
    "    pgd_0.376_x_untarget\n",
    "    pgd_0.376_x_val_untarget\n",
    "    pgd_0.376_x_test_untarget_gen_by_n_9_v1_cifar10\n",
    "    '''\n",
    "    if base in ['cwl2_x_tr_untargeted', \n",
    "                'cwl2_x_v_untargeted',\n",
    "                 'cwl2_x_test_untargeted', \n",
    "                #  'cwl2_x_test_untargeted_gen_by_ResNet56v1_1',\n",
    "                 'pgd_0.376_x_untarget', \n",
    "                 'pgd_0.376_x_val_untarget',\n",
    "                 'pgd_0.376_x_test_untarget_gen_by_n_9_v1_cifar10']:\n",
    "        attack_type = parts[0]\n",
    "        if attack_type == 'cwl2':\n",
    "            if parts[2] == 'tr':\n",
    "                train_or_val_or_test = 'train'\n",
    "            elif parts[2] == 'test':\n",
    "                train_or_val_or_test = 'test'\n",
    "            else:\n",
    "                train_or_val_or_test = 'val'\n",
    "        else:\n",
    "            attack_type = 'PGD'\n",
    "            if parts[3] == 'val':\n",
    "                train_or_val_or_test = 'val'\n",
    "            elif parts[3] == 'test':\n",
    "                train_or_val_or_test = 'test'\n",
    "            else:\n",
    "                train_or_val_or_test = 'train'\n",
    "    else:\n",
    "        continue\n",
    "    key = (attack_type, train_or_val_or_test)\n",
    "    adv_dataset[key] = np.load(os.path.join(folder, f))\n",
    "    \n",
    "\n",
    "adv_dataset[('VGG','test')] = np.load('./adversarial_examples/gen_by_VGG/pgd_0.376_x_test_untarget_by_vgg19.npy')\n",
    "print('-'*50, 'keys for adv samples', '-'*50)\n",
    "for k in adv_dataset.keys():\n",
    "    print(k)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e898b5a2",
   "metadata": {},
   "source": [
    "## Get raw predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f8928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions_B = {\n",
    "    'original': {'train':{}, 'val':{}, 'test':{}},\n",
    "    'cwl2': {'train':{}, 'val':{}, 'test':{}},\n",
    "    'PGD': {'train':{}, 'val':{}, 'test':{}},\n",
    "    'VGG': {'test':{}}\n",
    "    }\n",
    "\n",
    "\n",
    "for member, model in Judge_Committee_B:\n",
    "    raw_predictions_B['original']['train'][member] = model.predict(x_train, verbose=0)\n",
    "    raw_predictions_B['original']['val'][member] = model.predict(x_val, verbose=0)\n",
    "    raw_predictions_B['original']['test'][member] = model.predict(x_test, verbose=0)\n",
    "\n",
    "    for key, adv_samples in adv_dataset.items():\n",
    "        raw_predictions_B[key[0]][key[1]][member] = model.predict(adv_samples, verbose=0)\n",
    "        \n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "## dump raw predictions into pickle files\n",
    "with open('./data/raw_predictions_B.pkl', 'wb') as f:\n",
    "    pkl.dump(raw_predictions_B, f)\n",
    "    \n",
    "raw_predictions_SB = {\n",
    "    'original': {'train':{}, 'val':{}, 'test':{}},\n",
    "    'cwl2': {'train':{}, 'val':{}, 'test':{}},\n",
    "    'PGD': {'train':{}, 'val':{}, 'test':{}},\n",
    "    'VGG': {'test':{}}\n",
    "    }\n",
    "\n",
    "\n",
    "for member, model in Judge_Committee_SB:\n",
    "    raw_predictions_SB['original']['train'][member] = model.predict(x_train, verbose=0)\n",
    "    raw_predictions_SB['original']['val'][member] = model.predict(x_val, verbose=0)\n",
    "    raw_predictions_SB['original']['test'][member] = model.predict(x_test, verbose=0)\n",
    "\n",
    "    for key, adv_samples in adv_dataset.items():\n",
    "        raw_predictions_SB[key[0]][key[1]][member] = model.predict(adv_samples, verbose=0)\n",
    "        \n",
    "\n",
    "## dump raw predictions into pickle files\n",
    "with open('./data/raw_predictions_SB.pkl', 'wb') as f:\n",
    "    pkl.dump(raw_predictions_SB, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd070a",
   "metadata": {},
   "source": [
    "## get entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6eb90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load raw predictions_SB from pickle files\n",
    "# with open('./data/raw_predictions_SB.pkl', 'rb') as f:\n",
    "#     raw_predictions_SB = pkl.load(f)\n",
    "# ## Load raw predictions_B from pickle files\n",
    "# with open('./data/raw_predictions_B.pkl', 'rb') as f:\n",
    "#     raw_predictions_B = pkl.load(f)\n",
    "\n",
    "entropy_arrays_SB = {}\n",
    "for data_name in raw_predictions_SB.keys():\n",
    "    for data_type in raw_predictions_SB[data_name].keys():\n",
    "        entropy_array = get_entropy_array(Judge_Committee_SB, \n",
    "                          entropy_version = 'cross_entropy',\n",
    "                          preds = raw_predictions_SB[data_name][data_type])\n",
    "        entropy_arrays_SB[(data_name, data_type)] = entropy_array\n",
    "\n",
    "## dump it\n",
    "with open('./data/entropy_arrays_SB.pkl', 'wb') as f:\n",
    "    pkl.dump(entropy_arrays_SB, f)\n",
    "    \n",
    "entropy_arrays_B = {}\n",
    "for data_name in raw_predictions_B.keys():\n",
    "    for data_type in raw_predictions_SB[data_name].keys():\n",
    "        entropy_array = get_entropy_array(Judge_Committee_B, \n",
    "                          entropy_version = 'cross_entropy',\n",
    "                          preds = raw_predictions_B[data_name][data_type])\n",
    "        entropy_arrays_B[(data_name, data_type)] = entropy_array\n",
    "\n",
    "## dump it\n",
    "with open('./data/entropy_arrays_B.pkl', 'wb') as f:\n",
    "    pkl.dump(entropy_arrays_B, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10674ebe",
   "metadata": {},
   "source": [
    "## See statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7e28c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load entropy arrays from pickle files\n",
    "# with open('./data/entropy_arrays_SB.pkl', 'rb') as f:\n",
    "#     entropy_arrays_SB = pkl.load(f)\n",
    "# with open('./data/entropy_arrays_B.pkl', 'rb') as f:\n",
    "#     entropy_arrays_B = pkl.load(f)\n",
    "\n",
    "plot_disagreements(entropy_arrays_SB, title = \"Entropy Disagreements - SB\")\n",
    "for key, value in entropy_arrays_SB.items():\n",
    "    print(f\"{key}: {get_statistics(value)}\")\n",
    "plot_disagreements(entropy_arrays_B, title = \"Entropy Disagreements - B\")\n",
    "for key, value in entropy_arrays_B.items():\n",
    "    print(f\"{key}: {get_statistics(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in entropy_arrays_SB.items():\n",
    "    print(key)\n",
    "    \n",
    "    if key[1] == 'train':\n",
    "        truth = y_train.reshape(-1)\n",
    "    elif key[1] == 'val':\n",
    "        truth = y_val.reshape(-1)\n",
    "    else:\n",
    "        truth = y_test.reshape(-1)\n",
    "    stats = get_statistics(value)\n",
    "    alpha = stats['average']\n",
    "    print(alpha)\n",
    "    loc_0 = entropy_arrays_SB[key]<alpha\n",
    "    loc_1 = entropy_arrays_SB[key]>=alpha\n",
    "    X_0 = raw_predictions_SB[key[0]][key[1]][loc_0]\n",
    "    X_1 = raw_predictions_SB[key[0]][key[1]][loc_1]\n",
    "    \n",
    "    print(f\"X_0 shape: {X_0.shape}, X_1 shape: {X_1.shape}\")\n",
    "    for member, model in Judge_Committee_SB:\n",
    "\n",
    "        acc_0 = (np.argmax(X_0,axis=-1) == truth[loc_0])/len(X_0)\n",
    "        acc_1 = (np.argmax(X_1,axis=-1) == truth[loc_1])/len(X_1)\n",
    "    ## make table\n",
    "    table = pd.DataFrame({\n",
    "        \"Member\": [member],\n",
    "        \"Model\": [model],\n",
    "        f\"{key}_0\": [acc_0],\n",
    "        f\"{key}_1\": [acc_1],\n",
    "    })\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b185753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table[table.columns.str.startswith(\"Acc_\")]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
